{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House sales prices in King County\n",
    "\n",
    "A project on exploratory data analysis.\n",
    "\n",
    "Sebastian Thomas @ neue fische Bootcamp Data Science<br />\n",
    "(datascience at sebastianthomas dot de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Predictive analysis\n",
    "\n",
    "We fit some predictive models on our preprocessed data.\n",
    "\n",
    "## Imports\n",
    "\n",
    "### Modules, classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import of modules\n",
    "from importlib import import_module\n",
    "\n",
    "# object persistence\n",
    "import joblib\n",
    "\n",
    "# data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# machine learning\n",
    "from statsmodels.tools.tools import add_constant\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler, Normalizer, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, VotingRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, make_scorer\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# custom modules\n",
    "from modules.ds import root_mean_squared_error, median_absolute_error, mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers\n",
    "\n",
    "Some helping functions to evaluate our regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_evaluation(regressor, X_train, X_test, y_train, y_test):\n",
    "    print('R^2:   {:.3f}, {:.3f}'.format(r2_score(y_train, regressor.predict(X_train)),\n",
    "                                         r2_score(y_test, regressor.predict(X_test))))\n",
    "    print('RMSE:  {:.0f}, {:.0f}'.format(root_mean_squared_error(y_train, regressor.predict(X_train)),\n",
    "                                         root_mean_squared_error(y_test, regressor.predict(X_test))))\n",
    "    print('MAE:   {:.0f}, {:.0f}'.format(mean_absolute_error(y_train, regressor.predict(X_train)),\n",
    "                                         mean_absolute_error(y_test, regressor.predict(X_test))))\n",
    "    print('MedAE: {:.0f}, {:.0f}'.format(median_absolute_error(y_train, regressor.predict(X_train)),\n",
    "                                         median_absolute_error(y_test, regressor.predict(X_test))))\n",
    "    print('MAPE:  {:.3f}, {:.3f}'.format(mean_absolute_percentage_error(y_train, regressor.predict(X_train)),\n",
    "                                         mean_absolute_percentage_error(y_test, regressor.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "We import the preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>date</th>\n",
       "      <th>month</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_living log</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_above log</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>has_basement</th>\n",
       "      <th>room_size</th>\n",
       "      <th>...</th>\n",
       "      <th>long</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>zipcode cat</th>\n",
       "      <th>condition</th>\n",
       "      <th>condition bin</th>\n",
       "      <th>grade</th>\n",
       "      <th>grade bin</th>\n",
       "      <th>view</th>\n",
       "      <th>view bin</th>\n",
       "      <th>waterfront</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8724300010</th>\n",
       "      <td>548000.0</td>\n",
       "      <td>2014-09-09</td>\n",
       "      <td>2014-09</td>\n",
       "      <td>3420.0</td>\n",
       "      <td>8.137688</td>\n",
       "      <td>2330.0</td>\n",
       "      <td>7.754053</td>\n",
       "      <td>1090.0</td>\n",
       "      <td>True</td>\n",
       "      <td>414.545455</td>\n",
       "      <td>...</td>\n",
       "      <td>-121.982</td>\n",
       "      <td>98019</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3 to 4</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1423400005</th>\n",
       "      <td>249950.0</td>\n",
       "      <td>2014-08-15</td>\n",
       "      <td>2014-08</td>\n",
       "      <td>1370.0</td>\n",
       "      <td>7.223296</td>\n",
       "      <td>1370.0</td>\n",
       "      <td>7.223296</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>274.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-122.182</td>\n",
       "      <td>98058</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3 to 4</td>\n",
       "      <td>6</td>\n",
       "      <td>3 to 6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7202330280</th>\n",
       "      <td>401000.0</td>\n",
       "      <td>2014-09-22</td>\n",
       "      <td>2014-09</td>\n",
       "      <td>1350.0</td>\n",
       "      <td>7.208600</td>\n",
       "      <td>1350.0</td>\n",
       "      <td>7.208600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>216.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-122.036</td>\n",
       "      <td>98053</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3 to 4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1828000230</th>\n",
       "      <td>498000.0</td>\n",
       "      <td>2014-07-14</td>\n",
       "      <td>2014-07</td>\n",
       "      <td>1620.0</td>\n",
       "      <td>7.390799</td>\n",
       "      <td>1180.0</td>\n",
       "      <td>7.074117</td>\n",
       "      <td>440.0</td>\n",
       "      <td>True</td>\n",
       "      <td>270.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-122.128</td>\n",
       "      <td>98052</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3 to 4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3342100995</th>\n",
       "      <td>449000.0</td>\n",
       "      <td>2014-10-22</td>\n",
       "      <td>2014-10</td>\n",
       "      <td>1980.0</td>\n",
       "      <td>7.591357</td>\n",
       "      <td>1980.0</td>\n",
       "      <td>7.591357</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>264.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-122.207</td>\n",
       "      <td>98056</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3 to 4</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               price       date    month  sqft_living  sqft_living log  \\\n",
       "id                                                                       \n",
       "8724300010  548000.0 2014-09-09  2014-09       3420.0         8.137688   \n",
       "1423400005  249950.0 2014-08-15  2014-08       1370.0         7.223296   \n",
       "7202330280  401000.0 2014-09-22  2014-09       1350.0         7.208600   \n",
       "1828000230  498000.0 2014-07-14  2014-07       1620.0         7.390799   \n",
       "3342100995  449000.0 2014-10-22  2014-10       1980.0         7.591357   \n",
       "\n",
       "            sqft_above  sqft_above log  sqft_basement  has_basement  \\\n",
       "id                                                                    \n",
       "8724300010      2330.0        7.754053         1090.0          True   \n",
       "1423400005      1370.0        7.223296            0.0         False   \n",
       "7202330280      1350.0        7.208600            0.0         False   \n",
       "1828000230      1180.0        7.074117          440.0          True   \n",
       "3342100995      1980.0        7.591357            0.0         False   \n",
       "\n",
       "             room_size  ...     long  zipcode  zipcode cat  condition  \\\n",
       "id                      ...                                             \n",
       "8724300010  414.545455  ... -121.982    98019            2          3   \n",
       "1423400005  274.000000  ... -122.182    98058            1          4   \n",
       "7202330280  216.000000  ... -122.036    98053            4          3   \n",
       "1828000230  270.000000  ... -122.128    98052            4          3   \n",
       "3342100995  264.000000  ... -122.207    98056            1          3   \n",
       "\n",
       "            condition bin  grade  grade bin  view view bin  waterfront  \n",
       "id                                                                      \n",
       "8724300010         3 to 4     10         10     0        0       False  \n",
       "1423400005         3 to 4      6     3 to 6     0        0       False  \n",
       "7202330280         3 to 4      7          7     0        0       False  \n",
       "1828000230         3 to 4      7          7     0        0       False  \n",
       "3342100995         3 to 4      8          8     0        0       False  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "houses_train = pd.read_pickle('data/king_county_train_2_engineered.pickle')\n",
    "houses_train.sample(5, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = houses_train['price'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses_fit, houses_valid, y_fit, y_valid = train_test_split(houses_train, y_train, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rough analysis\n",
    "\n",
    "We start with a rough analysis.\n",
    "\n",
    "### Dummy approach\n",
    "\n",
    "As a baseline approach, we predict by the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fit = np.empty((y_fit.size, 1))\n",
    "X_valid = np.empty((y_valid.size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2:   -0.062, -0.065\n",
      "RMSE:  376836, 393670\n",
      "MAE:   222420, 222612\n",
      "MedAE: 150000, 148000\n",
      "MAPE:  0.423, 0.409\n"
     ]
    }
   ],
   "source": [
    "dummy_regressor = DummyRegressor(strategy='median')\n",
    "dummy_regressor.fit(X_fit, y_fit);\n",
    "print_evaluation(dummy_regressor, X_fit, X_valid, y_fit, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic approach with `'sqft_living log'`\n",
    "\n",
    "Next, we do some basic linear approaches. We start with just the feature `'sqft_living log'` (logarithm of living space area)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fit = houses_fit[['sqft_living log']]\n",
    "X_valid = houses_valid[['sqft_living log']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2:   0.375, 0.372\n",
      "RMSE:  289150, 302283\n",
      "MAE:   188190, 184524\n",
      "MedAE: 144977, 136775\n",
      "MAPE:  0.405, 0.386\n"
     ]
    }
   ],
   "source": [
    "# with sklearn\n",
    "linear_regressor = LinearRegression()\n",
    "linear_regressor.fit(X_fit, y_fit);\n",
    "print_evaluation(linear_regressor, X_fit, X_valid, y_fit, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th>  <td>   0.375</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.375</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   8665.</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 10 Jun 2020</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:35:13</td>     <th>  Log-Likelihood:    </th> <td>-2.0232e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 14458</td>      <th>  AIC:               </th>  <td>4.046e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 14456</td>      <th>  BIC:               </th>  <td>4.047e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>           <td>-3.449e+06</td> <td> 4.29e+04</td> <td>  -80.339</td> <td> 0.000</td> <td>-3.53e+06</td> <td>-3.36e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_living log</th> <td> 5.281e+05</td> <td> 5673.530</td> <td>   93.088</td> <td> 0.000</td> <td> 5.17e+05</td> <td> 5.39e+05</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>12921.107</td> <th>  Durbin-Watson:     </th>  <td>   1.987</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>1139262.900</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 3.928</td>   <th>  Prob(JB):          </th>  <td>    0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>45.772</td>   <th>  Cond. No.          </th>  <td>    137.</td>  \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.375\n",
       "Model:                            OLS   Adj. R-squared:                  0.375\n",
       "Method:                 Least Squares   F-statistic:                     8665.\n",
       "Date:                Wed, 10 Jun 2020   Prob (F-statistic):               0.00\n",
       "Time:                        23:35:13   Log-Likelihood:            -2.0232e+05\n",
       "No. Observations:               14458   AIC:                         4.046e+05\n",
       "Df Residuals:                   14456   BIC:                         4.047e+05\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===================================================================================\n",
       "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------\n",
       "const           -3.449e+06   4.29e+04    -80.339      0.000   -3.53e+06   -3.36e+06\n",
       "sqft_living log  5.281e+05   5673.530     93.088      0.000    5.17e+05    5.39e+05\n",
       "==============================================================================\n",
       "Omnibus:                    12921.107   Durbin-Watson:                   1.987\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          1139262.900\n",
       "Skew:                           3.928   Prob(JB):                         0.00\n",
       "Kurtosis:                      45.772   Cond. No.                         137.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with statsmodels\n",
    "linear_regressor = OLS(y_fit, add_constant(X_fit))\n",
    "results = linear_regressor.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic approach with `'sqft_living log'`, `'sqft_lot'`, `'lat'`, `'long'`\n",
    "\n",
    "We add the features `'sqft_lot'` (lot space), `'lat'` (lattitude) and `'long'` (longitude)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fit = houses_fit[['sqft_living log', 'sqft_lot', 'lat', 'long']]\n",
    "X_valid = houses_valid[['sqft_living log', 'sqft_lot', 'lat', 'long']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2:   0.469, 0.453\n",
      "RMSE:  266578, 282121\n",
      "MAE:   164509, 163007\n",
      "MedAE: 118236, 117162\n",
      "MAPE:  0.342, 0.331\n"
     ]
    }
   ],
   "source": [
    "linear_regressor = LinearRegression()\n",
    "linear_regressor.fit(X_fit, y_fit);\n",
    "print_evaluation(linear_regressor, X_fit, X_valid, y_fit, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th>  <td>   0.469</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.468</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   3186.</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 10 Jun 2020</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:35:13</td>     <th>  Log-Likelihood:    </th> <td>-2.0114e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 14458</td>      <th>  AIC:               </th>  <td>4.023e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 14453</td>      <th>  BIC:               </th>  <td>4.023e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>           <td>-7.431e+07</td> <td> 2.11e+06</td> <td>  -35.138</td> <td> 0.000</td> <td>-7.85e+07</td> <td>-7.02e+07</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_living log</th> <td> 5.381e+05</td> <td> 5445.604</td> <td>   98.813</td> <td> 0.000</td> <td> 5.27e+05</td> <td> 5.49e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>sqft_lot</th>        <td>    0.4251</td> <td>    0.053</td> <td>    8.062</td> <td> 0.000</td> <td>    0.322</td> <td>    0.528</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>lat</th>             <td> 7.216e+05</td> <td> 1.62e+04</td> <td>   44.595</td> <td> 0.000</td> <td>  6.9e+05</td> <td> 7.53e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>long</th>            <td>-2.983e+05</td> <td> 1.69e+04</td> <td>  -17.668</td> <td> 0.000</td> <td>-3.31e+05</td> <td>-2.65e+05</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>14371.403</td> <th>  Durbin-Watson:     </th>  <td>   2.004</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>1858373.110</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 4.570</td>   <th>  Prob(JB):          </th>  <td>    0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>57.784</td>   <th>  Cond. No.          </th>  <td>4.40e+07</td>  \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 4.4e+07. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.469\n",
       "Model:                            OLS   Adj. R-squared:                  0.468\n",
       "Method:                 Least Squares   F-statistic:                     3186.\n",
       "Date:                Wed, 10 Jun 2020   Prob (F-statistic):               0.00\n",
       "Time:                        23:35:13   Log-Likelihood:            -2.0114e+05\n",
       "No. Observations:               14458   AIC:                         4.023e+05\n",
       "Df Residuals:                   14453   BIC:                         4.023e+05\n",
       "Df Model:                           4                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===================================================================================\n",
       "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-----------------------------------------------------------------------------------\n",
       "const           -7.431e+07   2.11e+06    -35.138      0.000   -7.85e+07   -7.02e+07\n",
       "sqft_living log  5.381e+05   5445.604     98.813      0.000    5.27e+05    5.49e+05\n",
       "sqft_lot            0.4251      0.053      8.062      0.000       0.322       0.528\n",
       "lat              7.216e+05   1.62e+04     44.595      0.000     6.9e+05    7.53e+05\n",
       "long            -2.983e+05   1.69e+04    -17.668      0.000   -3.31e+05   -2.65e+05\n",
       "==============================================================================\n",
       "Omnibus:                    14371.403   Durbin-Watson:                   2.004\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          1858373.110\n",
       "Skew:                           4.570   Prob(JB):                         0.00\n",
       "Kurtosis:                      57.784   Cond. No.                     4.40e+07\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 4.4e+07. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_regressor = OLS(y_fit, add_constant(X_fit))\n",
    "results = linear_regressor.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinary approach with `LinearRegression`\n",
    "\n",
    "In the next approaches, we will work with more features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['price', 'date', 'month', 'sqft_living', 'sqft_living log',\n",
       "       'sqft_above', 'sqft_above log', 'sqft_basement', 'has_basement',\n",
       "       'room_size', 'room_size log', 'base_area', 'base_area log', 'sqft_lot',\n",
       "       'sqft_living15', 'sqft_living15 log', 'sqft_lot15', 'bedrooms',\n",
       "       'bedrooms bin', 'bathrooms', 'bathrooms bin', 'bathrooms_ratio',\n",
       "       'bathrooms_ratio bin', 'floors', 'floors bin', 'yr_built',\n",
       "       'decade_built', 'yr_built bin', 'yr_renovated', 'is_renovated', 'lat',\n",
       "       'long', 'zipcode', 'zipcode cat', 'condition', 'condition bin', 'grade',\n",
       "       'grade bin', 'view', 'view bin', 'waterfront'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "houses_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = ['sqft_living log', 'base_area log', 'sqft_lot', 'lat', 'long']\n",
    "categorical_features = ['bedrooms bin', 'bathrooms bin', 'bathrooms_ratio bin', 'floors bin', 'yr_built bin', \n",
    "                        'zipcode cat', 'condition bin', 'grade bin', 'view bin']\n",
    "boolean_features = ['has_basement', 'is_renovated', 'waterfront']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_transformer = ColumnTransformer([\n",
    "    ('numericals_transformer', StandardScaler(), numerical_features),\n",
    "    ('categoricals_transformer', OneHotEncoder(drop='first'), categorical_features),\n",
    "    ('booleans_transformer', 'passthrough', boolean_features)\n",
    "], n_jobs=-1)\n",
    "\n",
    "X_fit = column_transformer.fit_transform(houses_fit)\n",
    "X_valid = column_transformer.transform(houses_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2:   0.831, 0.768\n",
      "RMSE:  150453, 183818\n",
      "MAE:   91411, 94884\n",
      "MedAE: 60081, 57901\n",
      "MAPE:  0.177, 0.175\n"
     ]
    }
   ],
   "source": [
    "linear_regressor = LinearRegression()\n",
    "linear_regressor.fit(X_fit, y_fit)\n",
    "print_evaluation(linear_regressor, X_fit, X_valid, y_fit, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th>  <td>   0.831</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.830</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   1334.</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 10 Jun 2020</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:35:15</td>     <th>  Log-Likelihood:    </th> <td>-1.9287e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 14458</td>      <th>  AIC:               </th>  <td>3.859e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 14404</td>      <th>  BIC:               </th>  <td>3.863e+05</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    53</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td> 6.323e+05</td> <td> 7.89e+04</td> <td>    8.018</td> <td> 0.000</td> <td> 4.78e+05</td> <td> 7.87e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td> 7.405e+04</td> <td> 9091.486</td> <td>    8.145</td> <td> 0.000</td> <td> 5.62e+04</td> <td> 9.19e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td> 1.951e+04</td> <td> 8826.793</td> <td>    2.210</td> <td> 0.027</td> <td> 2208.616</td> <td> 3.68e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td> 8225.2209</td> <td> 1334.120</td> <td>    6.165</td> <td> 0.000</td> <td> 5610.173</td> <td> 1.08e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td> 2.539e+04</td> <td> 2130.881</td> <td>   11.913</td> <td> 0.000</td> <td> 2.12e+04</td> <td> 2.96e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>-2.387e+04</td> <td> 1703.360</td> <td>  -14.013</td> <td> 0.000</td> <td>-2.72e+04</td> <td>-2.05e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>-1.219e+04</td> <td> 1.63e+04</td> <td>   -0.748</td> <td> 0.454</td> <td>-4.41e+04</td> <td> 1.97e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>-1.035e+04</td> <td>  1.9e+04</td> <td>   -0.546</td> <td> 0.585</td> <td>-4.76e+04</td> <td> 2.68e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>-2.264e+04</td> <td> 2.04e+04</td> <td>   -1.110</td> <td> 0.267</td> <td>-6.26e+04</td> <td> 1.74e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>-3.044e+04</td> <td> 2.24e+04</td> <td>   -1.361</td> <td> 0.173</td> <td>-7.43e+04</td> <td> 1.34e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>-7.667e+04</td> <td> 2.57e+04</td> <td>   -2.988</td> <td> 0.003</td> <td>-1.27e+05</td> <td>-2.64e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td> 1.924e+04</td> <td> 7.57e+04</td> <td>    0.254</td> <td> 0.799</td> <td>-1.29e+05</td> <td> 1.68e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>   <td> -868.2025</td> <td> 7.66e+04</td> <td>   -0.011</td> <td> 0.991</td> <td>-1.51e+05</td> <td> 1.49e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>   <td> 1.941e+04</td> <td> 7.72e+04</td> <td>    0.252</td> <td> 0.801</td> <td>-1.32e+05</td> <td> 1.71e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th>   <td> 9.842e+04</td> <td> 7.79e+04</td> <td>    1.264</td> <td> 0.206</td> <td>-5.42e+04</td> <td> 2.51e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th>   <td> 1.953e+05</td> <td> 7.85e+04</td> <td>    2.489</td> <td> 0.013</td> <td> 4.15e+04</td> <td> 3.49e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th>   <td> 5.558e+05</td> <td> 8.15e+04</td> <td>    6.820</td> <td> 0.000</td> <td> 3.96e+05</td> <td> 7.16e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x17</th>   <td> 2.516e+06</td> <td> 1.24e+05</td> <td>   20.293</td> <td> 0.000</td> <td> 2.27e+06</td> <td> 2.76e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x18</th>   <td> 1.371e+04</td> <td> 8907.656</td> <td>    1.539</td> <td> 0.124</td> <td>-3748.091</td> <td> 3.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x19</th>   <td> 1.562e+04</td> <td> 1.13e+04</td> <td>    1.386</td> <td> 0.166</td> <td>-6472.610</td> <td> 3.77e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x20</th>   <td> 7612.1944</td> <td> 1.32e+04</td> <td>    0.578</td> <td> 0.563</td> <td>-1.82e+04</td> <td> 3.34e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x21</th>   <td> 3322.1294</td> <td> 1.88e+04</td> <td>    0.177</td> <td> 0.860</td> <td>-3.35e+04</td> <td> 4.02e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x22</th>   <td>-5.011e+04</td> <td> 5.21e+04</td> <td>   -0.961</td> <td> 0.337</td> <td>-1.52e+05</td> <td> 5.21e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x23</th>   <td> 2.456e+04</td> <td> 1.25e+04</td> <td>    1.968</td> <td> 0.049</td> <td>  102.411</td> <td>  4.9e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x24</th>   <td> 1.275e+05</td> <td> 2.36e+04</td> <td>    5.401</td> <td> 0.000</td> <td> 8.12e+04</td> <td> 1.74e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x25</th>   <td>  1.17e+04</td> <td> 2.24e+04</td> <td>    0.522</td> <td> 0.601</td> <td>-3.22e+04</td> <td> 5.56e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x26</th>   <td>-6.538e+04</td> <td> 4711.110</td> <td>  -13.877</td> <td> 0.000</td> <td>-7.46e+04</td> <td>-5.61e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x27</th>   <td>-9.697e+04</td> <td> 5644.136</td> <td>  -17.180</td> <td> 0.000</td> <td>-1.08e+05</td> <td>-8.59e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x28</th>   <td>-1.227e+05</td> <td> 7836.165</td> <td>  -15.655</td> <td> 0.000</td> <td>-1.38e+05</td> <td>-1.07e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x29</th>   <td>-1.148e+05</td> <td> 8468.126</td> <td>  -13.551</td> <td> 0.000</td> <td>-1.31e+05</td> <td>-9.82e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x30</th>   <td>-6.829e+04</td> <td> 1.03e+04</td> <td>   -6.650</td> <td> 0.000</td> <td>-8.84e+04</td> <td>-4.82e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x31</th>   <td> 5.401e+04</td> <td> 4614.921</td> <td>   11.704</td> <td> 0.000</td> <td>  4.5e+04</td> <td> 6.31e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x32</th>   <td> 1.124e+05</td> <td> 6464.667</td> <td>   17.390</td> <td> 0.000</td> <td> 9.97e+04</td> <td> 1.25e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33</th>   <td> 1.957e+05</td> <td> 5998.199</td> <td>   32.632</td> <td> 0.000</td> <td> 1.84e+05</td> <td> 2.07e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x34</th>   <td> 2.208e+05</td> <td> 7588.750</td> <td>   29.091</td> <td> 0.000</td> <td> 2.06e+05</td> <td> 2.36e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x35</th>   <td> 2.826e+05</td> <td> 6814.830</td> <td>   41.470</td> <td> 0.000</td> <td> 2.69e+05</td> <td> 2.96e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x36</th>   <td> 4.952e+05</td> <td> 9408.148</td> <td>   52.636</td> <td> 0.000</td> <td> 4.77e+05</td> <td> 5.14e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x37</th>   <td> 7.336e+05</td> <td> 1.19e+04</td> <td>   61.562</td> <td> 0.000</td> <td>  7.1e+05</td> <td> 7.57e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x38</th>   <td> 1.233e+06</td> <td> 2.72e+04</td> <td>   45.309</td> <td> 0.000</td> <td> 1.18e+06</td> <td> 1.29e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x39</th>   <td> 2.294e+04</td> <td> 1.35e+04</td> <td>    1.699</td> <td> 0.089</td> <td>-3529.363</td> <td> 4.94e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x40</th>   <td> 8.975e+04</td> <td> 1.41e+04</td> <td>    6.349</td> <td> 0.000</td> <td>  6.2e+04</td> <td> 1.17e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x41</th>   <td>  2.16e+05</td> <td> 1.12e+04</td> <td>   19.302</td> <td> 0.000</td> <td> 1.94e+05</td> <td> 2.38e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x42</th>   <td> 5.586e+05</td> <td> 2.14e+04</td> <td>   26.136</td> <td> 0.000</td> <td> 5.17e+05</td> <td>    6e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x43</th>   <td> 1.638e+06</td> <td> 5.01e+04</td> <td>   32.716</td> <td> 0.000</td> <td> 1.54e+06</td> <td> 1.74e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x44</th>   <td>-3.156e+05</td> <td> 9424.761</td> <td>  -33.485</td> <td> 0.000</td> <td>-3.34e+05</td> <td>-2.97e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x45</th>   <td>-2.966e+05</td> <td> 7597.782</td> <td>  -39.033</td> <td> 0.000</td> <td>-3.11e+05</td> <td>-2.82e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x46</th>   <td>-2.592e+05</td> <td> 6859.213</td> <td>  -37.784</td> <td> 0.000</td> <td>-2.73e+05</td> <td>-2.46e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x47</th>   <td>-1.575e+05</td> <td> 6770.315</td> <td>  -23.269</td> <td> 0.000</td> <td>-1.71e+05</td> <td>-1.44e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x48</th>   <td> 8.808e+04</td> <td> 5467.665</td> <td>   16.109</td> <td> 0.000</td> <td> 7.74e+04</td> <td> 9.88e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x49</th>   <td> 1.765e+05</td> <td> 8532.261</td> <td>   20.687</td> <td> 0.000</td> <td>  1.6e+05</td> <td> 1.93e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x50</th>   <td> 3.556e+05</td> <td> 1.33e+04</td> <td>   26.728</td> <td> 0.000</td> <td>  3.3e+05</td> <td> 3.82e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x51</th>   <td>-7244.5515</td> <td> 3296.817</td> <td>   -2.197</td> <td> 0.028</td> <td>-1.37e+04</td> <td> -782.366</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x52</th>   <td> 3.335e+04</td> <td> 6315.892</td> <td>    5.281</td> <td> 0.000</td> <td>  2.1e+04</td> <td> 4.57e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x53</th>   <td> 4.833e+05</td> <td> 1.93e+04</td> <td>   25.085</td> <td> 0.000</td> <td> 4.46e+05</td> <td> 5.21e+05</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>7983.271</td> <th>  Durbin-Watson:     </th>  <td>   2.013</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>433780.013</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 1.912</td>  <th>  Prob(JB):          </th>  <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>29.560</td>  <th>  Cond. No.          </th>  <td>    348.</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.831\n",
       "Model:                            OLS   Adj. R-squared:                  0.830\n",
       "Method:                 Least Squares   F-statistic:                     1334.\n",
       "Date:                Wed, 10 Jun 2020   Prob (F-statistic):               0.00\n",
       "Time:                        23:35:15   Log-Likelihood:            -1.9287e+05\n",
       "No. Observations:               14458   AIC:                         3.859e+05\n",
       "Df Residuals:                   14404   BIC:                         3.863e+05\n",
       "Df Model:                          53                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       6.323e+05   7.89e+04      8.018      0.000    4.78e+05    7.87e+05\n",
       "x1          7.405e+04   9091.486      8.145      0.000    5.62e+04    9.19e+04\n",
       "x2          1.951e+04   8826.793      2.210      0.027    2208.616    3.68e+04\n",
       "x3          8225.2209   1334.120      6.165      0.000    5610.173    1.08e+04\n",
       "x4          2.539e+04   2130.881     11.913      0.000    2.12e+04    2.96e+04\n",
       "x5         -2.387e+04   1703.360    -14.013      0.000   -2.72e+04   -2.05e+04\n",
       "x6         -1.219e+04   1.63e+04     -0.748      0.454   -4.41e+04    1.97e+04\n",
       "x7         -1.035e+04    1.9e+04     -0.546      0.585   -4.76e+04    2.68e+04\n",
       "x8         -2.264e+04   2.04e+04     -1.110      0.267   -6.26e+04    1.74e+04\n",
       "x9         -3.044e+04   2.24e+04     -1.361      0.173   -7.43e+04    1.34e+04\n",
       "x10        -7.667e+04   2.57e+04     -2.988      0.003   -1.27e+05   -2.64e+04\n",
       "x11         1.924e+04   7.57e+04      0.254      0.799   -1.29e+05    1.68e+05\n",
       "x12         -868.2025   7.66e+04     -0.011      0.991   -1.51e+05    1.49e+05\n",
       "x13         1.941e+04   7.72e+04      0.252      0.801   -1.32e+05    1.71e+05\n",
       "x14         9.842e+04   7.79e+04      1.264      0.206   -5.42e+04    2.51e+05\n",
       "x15         1.953e+05   7.85e+04      2.489      0.013    4.15e+04    3.49e+05\n",
       "x16         5.558e+05   8.15e+04      6.820      0.000    3.96e+05    7.16e+05\n",
       "x17         2.516e+06   1.24e+05     20.293      0.000    2.27e+06    2.76e+06\n",
       "x18         1.371e+04   8907.656      1.539      0.124   -3748.091    3.12e+04\n",
       "x19         1.562e+04   1.13e+04      1.386      0.166   -6472.610    3.77e+04\n",
       "x20         7612.1944   1.32e+04      0.578      0.563   -1.82e+04    3.34e+04\n",
       "x21         3322.1294   1.88e+04      0.177      0.860   -3.35e+04    4.02e+04\n",
       "x22        -5.011e+04   5.21e+04     -0.961      0.337   -1.52e+05    5.21e+04\n",
       "x23         2.456e+04   1.25e+04      1.968      0.049     102.411     4.9e+04\n",
       "x24         1.275e+05   2.36e+04      5.401      0.000    8.12e+04    1.74e+05\n",
       "x25          1.17e+04   2.24e+04      0.522      0.601   -3.22e+04    5.56e+04\n",
       "x26        -6.538e+04   4711.110    -13.877      0.000   -7.46e+04   -5.61e+04\n",
       "x27        -9.697e+04   5644.136    -17.180      0.000   -1.08e+05   -8.59e+04\n",
       "x28        -1.227e+05   7836.165    -15.655      0.000   -1.38e+05   -1.07e+05\n",
       "x29        -1.148e+05   8468.126    -13.551      0.000   -1.31e+05   -9.82e+04\n",
       "x30        -6.829e+04   1.03e+04     -6.650      0.000   -8.84e+04   -4.82e+04\n",
       "x31         5.401e+04   4614.921     11.704      0.000     4.5e+04    6.31e+04\n",
       "x32         1.124e+05   6464.667     17.390      0.000    9.97e+04    1.25e+05\n",
       "x33         1.957e+05   5998.199     32.632      0.000    1.84e+05    2.07e+05\n",
       "x34         2.208e+05   7588.750     29.091      0.000    2.06e+05    2.36e+05\n",
       "x35         2.826e+05   6814.830     41.470      0.000    2.69e+05    2.96e+05\n",
       "x36         4.952e+05   9408.148     52.636      0.000    4.77e+05    5.14e+05\n",
       "x37         7.336e+05   1.19e+04     61.562      0.000     7.1e+05    7.57e+05\n",
       "x38         1.233e+06   2.72e+04     45.309      0.000    1.18e+06    1.29e+06\n",
       "x39         2.294e+04   1.35e+04      1.699      0.089   -3529.363    4.94e+04\n",
       "x40         8.975e+04   1.41e+04      6.349      0.000     6.2e+04    1.17e+05\n",
       "x41          2.16e+05   1.12e+04     19.302      0.000    1.94e+05    2.38e+05\n",
       "x42         5.586e+05   2.14e+04     26.136      0.000    5.17e+05       6e+05\n",
       "x43         1.638e+06   5.01e+04     32.716      0.000    1.54e+06    1.74e+06\n",
       "x44        -3.156e+05   9424.761    -33.485      0.000   -3.34e+05   -2.97e+05\n",
       "x45        -2.966e+05   7597.782    -39.033      0.000   -3.11e+05   -2.82e+05\n",
       "x46        -2.592e+05   6859.213    -37.784      0.000   -2.73e+05   -2.46e+05\n",
       "x47        -1.575e+05   6770.315    -23.269      0.000   -1.71e+05   -1.44e+05\n",
       "x48         8.808e+04   5467.665     16.109      0.000    7.74e+04    9.88e+04\n",
       "x49         1.765e+05   8532.261     20.687      0.000     1.6e+05    1.93e+05\n",
       "x50         3.556e+05   1.33e+04     26.728      0.000     3.3e+05    3.82e+05\n",
       "x51        -7244.5515   3296.817     -2.197      0.028   -1.37e+04    -782.366\n",
       "x52         3.335e+04   6315.892      5.281      0.000     2.1e+04    4.57e+04\n",
       "x53         4.833e+05   1.93e+04     25.085      0.000    4.46e+05    5.21e+05\n",
       "==============================================================================\n",
       "Omnibus:                     7983.271   Durbin-Watson:                   2.013\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           433780.013\n",
       "Skew:                           1.912   Prob(JB):                         0.00\n",
       "Kurtosis:                      29.560   Cond. No.                         348.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_regressor = OLS(y_fit, add_constant(X_fit.toarray()))\n",
    "results = linear_regressor.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach with `PolynomialFeatures`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2:   0.927, 0.813\n",
      "RMSE:  98616, 164738\n",
      "MAE:   65639, 83548\n",
      "MedAE: 44544, 46349\n",
      "MAPE:  0.135, 0.149\n"
     ]
    }
   ],
   "source": [
    "polynomial_regressor = make_pipeline(PolynomialFeatures(), LinearRegression())\n",
    "polynomial_regressor.fit(X_fit, y_fit);\n",
    "print_evaluation(polynomial_regressor, X_fit, X_valid, y_fit, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logarithmic approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2:   0.867, 0.871\n",
      "RMSE:  133157, 136845\n",
      "MAE:   78836, 80706\n",
      "MedAE: 49363, 48743\n",
      "MAPE:  0.147, 0.144\n"
     ]
    }
   ],
   "source": [
    "linear_regressor = TransformedTargetRegressor(regressor=LinearRegression(), func=np.log, inverse_func=np.exp)\n",
    "linear_regressor.fit(X_fit, y_fit)\n",
    "print_evaluation(linear_regressor, X_fit, X_valid, y_fit, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.864</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.864</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1732.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 10 Jun 2020</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>23:35:31</td>     <th>  Log-Likelihood:    </th> <td>  3221.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 14458</td>      <th>  AIC:               </th> <td>  -6335.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 14404</td>      <th>  BIC:               </th> <td>  -5926.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    53</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>   12.6453</td> <td>    0.101</td> <td>  124.595</td> <td> 0.000</td> <td>   12.446</td> <td>   12.844</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.1368</td> <td>    0.012</td> <td>   11.691</td> <td> 0.000</td> <td>    0.114</td> <td>    0.160</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>    <td>    0.0528</td> <td>    0.011</td> <td>    4.643</td> <td> 0.000</td> <td>    0.030</td> <td>    0.075</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>    <td>    0.0300</td> <td>    0.002</td> <td>   17.484</td> <td> 0.000</td> <td>    0.027</td> <td>    0.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>    <td>    0.0583</td> <td>    0.003</td> <td>   21.246</td> <td> 0.000</td> <td>    0.053</td> <td>    0.064</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>    <td>   -0.0435</td> <td>    0.002</td> <td>  -19.847</td> <td> 0.000</td> <td>   -0.048</td> <td>   -0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>    <td>    0.0057</td> <td>    0.021</td> <td>    0.271</td> <td> 0.787</td> <td>   -0.035</td> <td>    0.047</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>    <td>    0.0182</td> <td>    0.024</td> <td>    0.743</td> <td> 0.457</td> <td>   -0.030</td> <td>    0.066</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>    <td>    0.0102</td> <td>    0.026</td> <td>    0.389</td> <td> 0.697</td> <td>   -0.041</td> <td>    0.062</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>    <td>   -0.0068</td> <td>    0.029</td> <td>   -0.236</td> <td> 0.814</td> <td>   -0.063</td> <td>    0.050</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th>   <td>   -0.0269</td> <td>    0.033</td> <td>   -0.815</td> <td> 0.415</td> <td>   -0.092</td> <td>    0.038</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th>   <td>    0.1774</td> <td>    0.097</td> <td>    1.821</td> <td> 0.069</td> <td>   -0.014</td> <td>    0.368</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th>   <td>    0.1400</td> <td>    0.099</td> <td>    1.420</td> <td> 0.155</td> <td>   -0.053</td> <td>    0.333</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th>   <td>    0.1594</td> <td>    0.099</td> <td>    1.606</td> <td> 0.108</td> <td>   -0.035</td> <td>    0.354</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th>   <td>    0.2077</td> <td>    0.100</td> <td>    2.072</td> <td> 0.038</td> <td>    0.011</td> <td>    0.404</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th>   <td>    0.2444</td> <td>    0.101</td> <td>    2.420</td> <td> 0.016</td> <td>    0.046</td> <td>    0.442</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th>   <td>    0.2842</td> <td>    0.105</td> <td>    2.709</td> <td> 0.007</td> <td>    0.079</td> <td>    0.490</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x17</th>   <td>    0.2352</td> <td>    0.160</td> <td>    1.474</td> <td> 0.141</td> <td>   -0.078</td> <td>    0.548</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x18</th>   <td>    0.0579</td> <td>    0.011</td> <td>    5.047</td> <td> 0.000</td> <td>    0.035</td> <td>    0.080</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x19</th>   <td>    0.0697</td> <td>    0.015</td> <td>    4.808</td> <td> 0.000</td> <td>    0.041</td> <td>    0.098</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x20</th>   <td>    0.0686</td> <td>    0.017</td> <td>    4.051</td> <td> 0.000</td> <td>    0.035</td> <td>    0.102</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x21</th>   <td>    0.0625</td> <td>    0.024</td> <td>    2.585</td> <td> 0.010</td> <td>    0.015</td> <td>    0.110</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x22</th>   <td>    0.0198</td> <td>    0.067</td> <td>    0.296</td> <td> 0.767</td> <td>   -0.112</td> <td>    0.151</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x23</th>   <td>    0.0685</td> <td>    0.016</td> <td>    4.268</td> <td> 0.000</td> <td>    0.037</td> <td>    0.100</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x24</th>   <td>    0.1240</td> <td>    0.030</td> <td>    4.081</td> <td> 0.000</td> <td>    0.064</td> <td>    0.184</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x25</th>   <td>    0.0575</td> <td>    0.029</td> <td>    1.995</td> <td> 0.046</td> <td>    0.001</td> <td>    0.114</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x26</th>   <td>   -0.1178</td> <td>    0.006</td> <td>  -19.435</td> <td> 0.000</td> <td>   -0.130</td> <td>   -0.106</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x27</th>   <td>   -0.1595</td> <td>    0.007</td> <td>  -21.958</td> <td> 0.000</td> <td>   -0.174</td> <td>   -0.145</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x28</th>   <td>   -0.1698</td> <td>    0.010</td> <td>  -16.839</td> <td> 0.000</td> <td>   -0.190</td> <td>   -0.150</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x29</th>   <td>   -0.1723</td> <td>    0.011</td> <td>  -15.805</td> <td> 0.000</td> <td>   -0.194</td> <td>   -0.151</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x30</th>   <td>   -0.0962</td> <td>    0.013</td> <td>   -7.282</td> <td> 0.000</td> <td>   -0.122</td> <td>   -0.070</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x31</th>   <td>    0.1815</td> <td>    0.006</td> <td>   30.563</td> <td> 0.000</td> <td>    0.170</td> <td>    0.193</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x32</th>   <td>    0.3240</td> <td>    0.008</td> <td>   38.941</td> <td> 0.000</td> <td>    0.308</td> <td>    0.340</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33</th>   <td>    0.4988</td> <td>    0.008</td> <td>   64.610</td> <td> 0.000</td> <td>    0.484</td> <td>    0.514</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x34</th>   <td>    0.5212</td> <td>    0.010</td> <td>   53.365</td> <td> 0.000</td> <td>    0.502</td> <td>    0.540</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x35</th>   <td>    0.6067</td> <td>    0.009</td> <td>   69.176</td> <td> 0.000</td> <td>    0.590</td> <td>    0.624</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x36</th>   <td>    0.7892</td> <td>    0.012</td> <td>   65.174</td> <td> 0.000</td> <td>    0.765</td> <td>    0.813</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x37</th>   <td>    1.0119</td> <td>    0.015</td> <td>   65.979</td> <td> 0.000</td> <td>    0.982</td> <td>    1.042</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x38</th>   <td>    1.1830</td> <td>    0.035</td> <td>   33.784</td> <td> 0.000</td> <td>    1.114</td> <td>    1.252</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x39</th>   <td>    0.1591</td> <td>    0.017</td> <td>    9.155</td> <td> 0.000</td> <td>    0.125</td> <td>    0.193</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x40</th>   <td>    0.2482</td> <td>    0.018</td> <td>   13.642</td> <td> 0.000</td> <td>    0.213</td> <td>    0.284</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x41</th>   <td>    0.1161</td> <td>    0.014</td> <td>    8.059</td> <td> 0.000</td> <td>    0.088</td> <td>    0.144</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x42</th>   <td>    0.2372</td> <td>    0.028</td> <td>    8.622</td> <td> 0.000</td> <td>    0.183</td> <td>    0.291</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x43</th>   <td>    0.5951</td> <td>    0.064</td> <td>    9.236</td> <td> 0.000</td> <td>    0.469</td> <td>    0.721</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x44</th>   <td>   -0.4484</td> <td>    0.012</td> <td>  -36.966</td> <td> 0.000</td> <td>   -0.472</td> <td>   -0.425</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x45</th>   <td>   -0.3382</td> <td>    0.010</td> <td>  -34.585</td> <td> 0.000</td> <td>   -0.357</td> <td>   -0.319</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x46</th>   <td>   -0.2429</td> <td>    0.009</td> <td>  -27.510</td> <td> 0.000</td> <td>   -0.260</td> <td>   -0.226</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x47</th>   <td>   -0.1103</td> <td>    0.009</td> <td>  -12.663</td> <td> 0.000</td> <td>   -0.127</td> <td>   -0.093</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x48</th>   <td>    0.1459</td> <td>    0.007</td> <td>   20.736</td> <td> 0.000</td> <td>    0.132</td> <td>    0.160</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x49</th>   <td>    0.2136</td> <td>    0.011</td> <td>   19.451</td> <td> 0.000</td> <td>    0.192</td> <td>    0.235</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x50</th>   <td>    0.3421</td> <td>    0.017</td> <td>   19.978</td> <td> 0.000</td> <td>    0.309</td> <td>    0.376</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x51</th>   <td>   -0.0172</td> <td>    0.004</td> <td>   -4.047</td> <td> 0.000</td> <td>   -0.025</td> <td>   -0.009</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x52</th>   <td>    0.0442</td> <td>    0.008</td> <td>    5.434</td> <td> 0.000</td> <td>    0.028</td> <td>    0.060</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x53</th>   <td>    0.4143</td> <td>    0.025</td> <td>   16.707</td> <td> 0.000</td> <td>    0.366</td> <td>    0.463</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>738.949</td> <th>  Durbin-Watson:     </th> <td>   2.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2725.995</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.099</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 5.118</td>  <th>  Cond. No.          </th> <td>    348.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.864\n",
       "Model:                            OLS   Adj. R-squared:                  0.864\n",
       "Method:                 Least Squares   F-statistic:                     1732.\n",
       "Date:                Wed, 10 Jun 2020   Prob (F-statistic):               0.00\n",
       "Time:                        23:35:31   Log-Likelihood:                 3221.6\n",
       "No. Observations:               14458   AIC:                            -6335.\n",
       "Df Residuals:                   14404   BIC:                            -5926.\n",
       "Df Model:                          53                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         12.6453      0.101    124.595      0.000      12.446      12.844\n",
       "x1             0.1368      0.012     11.691      0.000       0.114       0.160\n",
       "x2             0.0528      0.011      4.643      0.000       0.030       0.075\n",
       "x3             0.0300      0.002     17.484      0.000       0.027       0.033\n",
       "x4             0.0583      0.003     21.246      0.000       0.053       0.064\n",
       "x5            -0.0435      0.002    -19.847      0.000      -0.048      -0.039\n",
       "x6             0.0057      0.021      0.271      0.787      -0.035       0.047\n",
       "x7             0.0182      0.024      0.743      0.457      -0.030       0.066\n",
       "x8             0.0102      0.026      0.389      0.697      -0.041       0.062\n",
       "x9            -0.0068      0.029     -0.236      0.814      -0.063       0.050\n",
       "x10           -0.0269      0.033     -0.815      0.415      -0.092       0.038\n",
       "x11            0.1774      0.097      1.821      0.069      -0.014       0.368\n",
       "x12            0.1400      0.099      1.420      0.155      -0.053       0.333\n",
       "x13            0.1594      0.099      1.606      0.108      -0.035       0.354\n",
       "x14            0.2077      0.100      2.072      0.038       0.011       0.404\n",
       "x15            0.2444      0.101      2.420      0.016       0.046       0.442\n",
       "x16            0.2842      0.105      2.709      0.007       0.079       0.490\n",
       "x17            0.2352      0.160      1.474      0.141      -0.078       0.548\n",
       "x18            0.0579      0.011      5.047      0.000       0.035       0.080\n",
       "x19            0.0697      0.015      4.808      0.000       0.041       0.098\n",
       "x20            0.0686      0.017      4.051      0.000       0.035       0.102\n",
       "x21            0.0625      0.024      2.585      0.010       0.015       0.110\n",
       "x22            0.0198      0.067      0.296      0.767      -0.112       0.151\n",
       "x23            0.0685      0.016      4.268      0.000       0.037       0.100\n",
       "x24            0.1240      0.030      4.081      0.000       0.064       0.184\n",
       "x25            0.0575      0.029      1.995      0.046       0.001       0.114\n",
       "x26           -0.1178      0.006    -19.435      0.000      -0.130      -0.106\n",
       "x27           -0.1595      0.007    -21.958      0.000      -0.174      -0.145\n",
       "x28           -0.1698      0.010    -16.839      0.000      -0.190      -0.150\n",
       "x29           -0.1723      0.011    -15.805      0.000      -0.194      -0.151\n",
       "x30           -0.0962      0.013     -7.282      0.000      -0.122      -0.070\n",
       "x31            0.1815      0.006     30.563      0.000       0.170       0.193\n",
       "x32            0.3240      0.008     38.941      0.000       0.308       0.340\n",
       "x33            0.4988      0.008     64.610      0.000       0.484       0.514\n",
       "x34            0.5212      0.010     53.365      0.000       0.502       0.540\n",
       "x35            0.6067      0.009     69.176      0.000       0.590       0.624\n",
       "x36            0.7892      0.012     65.174      0.000       0.765       0.813\n",
       "x37            1.0119      0.015     65.979      0.000       0.982       1.042\n",
       "x38            1.1830      0.035     33.784      0.000       1.114       1.252\n",
       "x39            0.1591      0.017      9.155      0.000       0.125       0.193\n",
       "x40            0.2482      0.018     13.642      0.000       0.213       0.284\n",
       "x41            0.1161      0.014      8.059      0.000       0.088       0.144\n",
       "x42            0.2372      0.028      8.622      0.000       0.183       0.291\n",
       "x43            0.5951      0.064      9.236      0.000       0.469       0.721\n",
       "x44           -0.4484      0.012    -36.966      0.000      -0.472      -0.425\n",
       "x45           -0.3382      0.010    -34.585      0.000      -0.357      -0.319\n",
       "x46           -0.2429      0.009    -27.510      0.000      -0.260      -0.226\n",
       "x47           -0.1103      0.009    -12.663      0.000      -0.127      -0.093\n",
       "x48            0.1459      0.007     20.736      0.000       0.132       0.160\n",
       "x49            0.2136      0.011     19.451      0.000       0.192       0.235\n",
       "x50            0.3421      0.017     19.978      0.000       0.309       0.376\n",
       "x51           -0.0172      0.004     -4.047      0.000      -0.025      -0.009\n",
       "x52            0.0442      0.008      5.434      0.000       0.028       0.060\n",
       "x53            0.4143      0.025     16.707      0.000       0.366       0.463\n",
       "==============================================================================\n",
       "Omnibus:                      738.949   Durbin-Watson:                   2.003\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2725.995\n",
       "Skew:                          -0.099   Prob(JB):                         0.00\n",
       "Kurtosis:                       5.118   Cond. No.                         348.\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_regressor = OLS(np.log(y_fit), add_constant(X_fit.toarray()))\n",
    "results = linear_regressor.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logarithmic approach with `PolynomialFeatures`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2:   0.927, 0.628\n",
      "RMSE:  98518, 232583\n",
      "MAE:   62987, 83419\n",
      "MedAE: 41282, 42661\n",
      "MAPE:  0.124, 0.138\n"
     ]
    }
   ],
   "source": [
    "polynomial_regressor = TransformedTargetRegressor(regressor=make_pipeline(PolynomialFeatures(),\n",
    "                                                                          LinearRegression()),\n",
    "                                                  func=np.log, inverse_func=np.exp)\n",
    "polynomial_regressor.fit(X_fit, y_fit)\n",
    "print_evaluation(polynomial_regressor, X_fit, X_valid, y_fit, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logarithmic approach with `SVR`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2:   0.935, 0.835\n",
      "RMSE:  93281, 154721\n",
      "MAE:   56526, 71248\n",
      "MedAE: 37425, 39507\n",
      "MAPE:  0.110, 0.125\n"
     ]
    }
   ],
   "source": [
    "support_vector_regressor = TransformedTargetRegressor(regressor=SVR(), func=np.log, inverse_func=np.exp)\n",
    "support_vector_regressor.fit(X_fit, y_fit)\n",
    "print_evaluation(support_vector_regressor, X_fit, X_valid, y_fit, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logarithmic approach with `RandomForestRegressor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2:   0.977, 0.878\n",
      "RMSE:  55476, 133128\n",
      "MAE:   27246, 68982\n",
      "MedAE: 14602, 38785\n",
      "MAPE:  0.047, 0.123\n"
     ]
    }
   ],
   "source": [
    "random_forest_regressor = TransformedTargetRegressor(regressor=RandomForestRegressor(), func=np.log,\n",
    "                                                     inverse_func=np.exp)\n",
    "random_forest_regressor.fit(X_fit, y_fit)\n",
    "print_evaluation(random_forest_regressor, X_fit, X_valid, y_fit, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logarithmic approach with `XGBRegressor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2:   0.967, 0.881\n",
      "RMSE:  66188, 131726\n",
      "MAE:   42829, 64983\n",
      "MedAE: 27838, 35410\n",
      "MAPE:  0.084, 0.117\n"
     ]
    }
   ],
   "source": [
    "xgb_regressor = TransformedTargetRegressor(regressor=XGBRegressor(), func=np.log, inverse_func=np.exp)\n",
    "xgb_regressor.fit(X_fit, y_fit)\n",
    "print_evaluation(xgb_regressor, X_fit, X_valid, y_fit, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 components\n",
      "R^2:   0.848, 0.839\n",
      "RMSE:  142742, 153220\n",
      "MAE:   86861, 89670\n",
      "MedAE: 55861, 54205\n",
      "MAPE:  0.164, 0.159\n",
      "2 components\n",
      "R^2:   0.863, 0.867\n",
      "RMSE:  135448, 139272\n",
      "MAE:   80867, 82599\n",
      "MedAE: 49914, 48384\n",
      "MAPE:  0.151, 0.146\n",
      "3 components\n",
      "R^2:   0.862, 0.865\n",
      "RMSE:  135634, 140008\n",
      "MAE:   80561, 82016\n",
      "MedAE: 49403, 47475\n",
      "MAPE:  0.149, 0.144\n",
      "4 components\n",
      "R^2:   0.867, 0.869\n",
      "RMSE:  133434, 137974\n",
      "MAE:   78967, 81125\n",
      "MedAE: 49467, 48922\n",
      "MAPE:  0.147, 0.144\n",
      "5 components\n",
      "R^2:   0.867, 0.871\n",
      "RMSE:  133157, 136845\n",
      "MAE:   78836, 80706\n",
      "MedAE: 49364, 48743\n",
      "MAPE:  0.147, 0.144\n"
     ]
    }
   ],
   "source": [
    "for idx in range(1, len(numerical_features) + 1):\n",
    "    print('{} components'.format(idx))\n",
    "    linear_regressor = TransformedTargetRegressor(\n",
    "        make_pipeline(\n",
    "            ColumnTransformer([\n",
    "                ('numericals_transformer', make_pipeline(StandardScaler(), PCA(idx)), numerical_features),\n",
    "                ('categoricals_transformer', OneHotEncoder(drop='first'), categorical_features),\n",
    "                ('booleans_transformer', 'passthrough', boolean_features)\n",
    "            ], n_jobs=-1),\n",
    "            LinearRegression()),\n",
    "        func=np.log, inverse_func=np.exp)\n",
    "    linear_regressor.fit(houses_fit, y_fit);\n",
    "    print_evaluation(linear_regressor, houses_fit, houses_valid, y_fit, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 components\n",
      "R^2:   0.904, 0.733\n",
      "RMSE:  113125, 197011\n",
      "MAE:   73932, 90454\n",
      "MedAE: 49829, 53642\n",
      "MAPE:  0.146, 0.157\n",
      "2 components\n",
      "R^2:   0.917, 0.848\n",
      "RMSE:  105403, 148871\n",
      "MAE:   67636, 80677\n",
      "MedAE: 43706, 44602\n",
      "MAPE:  0.132, 0.142\n",
      "3 components\n",
      "R^2:   0.920, 0.753\n",
      "RMSE:  103575, 189563\n",
      "MAE:   65990, 83456\n",
      "MedAE: 42798, 43510\n",
      "MAPE:  0.129, 0.142\n",
      "4 components\n",
      "R^2:   0.926, 0.725\n",
      "RMSE:  99503, 199854\n",
      "MAE:   63359, 81787\n",
      "MedAE: 41183, 42159\n",
      "MAPE:  0.125, 0.138\n",
      "5 components\n",
      "R^2:   0.927, 0.630\n",
      "RMSE:  98521, 231854\n",
      "MAE:   62990, 83373\n",
      "MedAE: 41278, 42817\n",
      "MAPE:  0.124, 0.138\n"
     ]
    }
   ],
   "source": [
    "for idx in range(1, len(numerical_features) + 1):\n",
    "    print('{} components'.format(idx))\n",
    "    polynomial_regressor = TransformedTargetRegressor(\n",
    "        make_pipeline(\n",
    "            ColumnTransformer([\n",
    "                ('numericals_transformer', make_pipeline(StandardScaler(), PCA(idx)), numerical_features),\n",
    "                ('categoricals_transformer', OneHotEncoder(drop='first'), categorical_features),\n",
    "                ('booleans_transformer', 'passthrough', boolean_features)\n",
    "            ], n_jobs=-1),\n",
    "            PolynomialFeatures(),\n",
    "            LinearRegression()),\n",
    "        func=np.log, inverse_func=np.exp)\n",
    "    polynomial_regressor.fit(houses_fit, y_fit);\n",
    "    print_evaluation(polynomial_regressor, houses_fit, houses_valid, y_fit, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 components\n",
      "R^2:   0.908, 0.778\n",
      "RMSE:  110966, 179626\n",
      "MAE:   68146, 85985\n",
      "MedAE: 45849, 50487\n",
      "MAPE:  0.134, 0.150\n",
      "2 components\n",
      "R^2:   0.926, 0.814\n",
      "RMSE:  99434, 164294\n",
      "MAE:   61086, 77385\n",
      "MedAE: 40793, 43185\n",
      "MAPE:  0.119, 0.135\n",
      "3 components\n",
      "R^2:   0.929, 0.826\n",
      "RMSE:  97742, 159166\n",
      "MAE:   58527, 73463\n",
      "MedAE: 38460, 40223\n",
      "MAPE:  0.113, 0.129\n",
      "4 components\n",
      "R^2:   0.932, 0.831\n",
      "RMSE:  95140, 156757\n",
      "MAE:   56837, 71319\n",
      "MedAE: 37418, 38967\n",
      "MAPE:  0.110, 0.125\n",
      "5 components\n",
      "R^2:   0.935, 0.835\n",
      "RMSE:  93281, 154721\n",
      "MAE:   56526, 71248\n",
      "MedAE: 37425, 39507\n",
      "MAPE:  0.110, 0.125\n"
     ]
    }
   ],
   "source": [
    "for idx in range(1, len(numerical_features) + 1):\n",
    "    print('{} components'.format(idx))\n",
    "    support_vector_regressor = TransformedTargetRegressor(\n",
    "        make_pipeline(\n",
    "            ColumnTransformer([\n",
    "                ('numericals_transformer', make_pipeline(StandardScaler(), PCA(idx)), numerical_features),\n",
    "                ('categoricals_transformer', OneHotEncoder(drop='first'), categorical_features),\n",
    "                ('booleans_transformer', 'passthrough', boolean_features)\n",
    "            ], n_jobs=-1),\n",
    "            SVR()),\n",
    "        func=np.log, inverse_func=np.exp)\n",
    "    support_vector_regressor.fit(houses_fit, y_fit);\n",
    "    print_evaluation(support_vector_regressor, houses_fit, houses_valid, y_fit, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 components\n",
      "R^2:   0.911, 0.785\n",
      "RMSE:  109368, 176813\n",
      "MAE:   67552, 84920\n",
      "MedAE: 45843, 48689\n",
      "MAPE:  0.133, 0.150\n",
      "2 components\n",
      "R^2:   0.928, 0.822\n",
      "RMSE:  98404, 160860\n",
      "MAE:   60677, 76534\n",
      "MedAE: 40474, 43034\n",
      "MAPE:  0.118, 0.134\n",
      "3 components\n",
      "R^2:   0.931, 0.834\n",
      "RMSE:  95951, 155298\n",
      "MAE:   58132, 73026\n",
      "MedAE: 38428, 40540\n",
      "MAPE:  0.112, 0.128\n",
      "4 components\n",
      "R^2:   0.935, 0.842\n",
      "RMSE:  93149, 151481\n",
      "MAE:   56321, 70680\n",
      "MedAE: 37062, 38570\n",
      "MAPE:  0.109, 0.124\n",
      "5 components\n",
      "R^2:   0.936, 0.844\n",
      "RMSE:  92351, 150845\n",
      "MAE:   56168, 70550\n",
      "MedAE: 37163, 38845\n",
      "MAPE:  0.109, 0.124\n"
     ]
    }
   ],
   "source": [
    "for idx in range(1, len(numerical_features) + 1):\n",
    "    print('{} components'.format(idx))\n",
    "    xgb_regressor = TransformedTargetRegressor(\n",
    "        make_pipeline(\n",
    "            ColumnTransformer([\n",
    "                ('numericals_transformer', make_pipeline(StandardScaler(), PCA(idx)), numerical_features),\n",
    "                ('categoricals_transformer', OneHotEncoder(), categorical_features),\n",
    "                ('booleans_transformer', 'passthrough', boolean_features)\n",
    "            ], n_jobs=-1),\n",
    "            SVR()),\n",
    "        func=np.log, inverse_func=np.exp)\n",
    "    xgb_regressor.fit(houses_fit, y_fit);\n",
    "    print_evaluation(xgb_regressor, houses_fit, houses_valid, y_fit, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Systematic analysis\n",
    "\n",
    "We conduct a systematic analysis by tuning the hyperparameters for several regressors using a randomized search (due to long computation time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_selection_linear = [\n",
    "    ('sklearn.linear_model', 'LinearRegression', 'linear_regressor',\n",
    "     {\n",
    "         'fit_intercept': [True, False],\n",
    "         'normalize':     [True, False]\n",
    "     }),\n",
    "    ('sklearn.linear_model', 'ElasticNet', 'elastic_net_regressor',\n",
    "     {\n",
    "         'alpha':         [0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0],\n",
    "         'l1_ratio':      [r*0.1 for r in range(0, 11)],\n",
    "         'fit_intercept': [True, False],\n",
    "         'normalize':     [True, False],\n",
    "         'max_iter':      [1000, 2000],\n",
    "         'random_state':  [0]\n",
    "     }),\n",
    "    ('sklearn.neighbors', 'KNeighborsRegressor', 'k_nearest_neighbors_regressor',\n",
    "     {\n",
    "         'n_neighbors': list(range(1, 21)),\n",
    "         'leaf_size':   list(range(15, 55)),\n",
    "         'p':           [1, 2]\n",
    "     }),\n",
    "    ('sklearn.svm', 'SVR', 'linear_support_vector_regressor',\n",
    "     {\n",
    "         'kernel':       ['linear'],\n",
    "         'C':            [10**r for r in range(-5, 1)],\n",
    "         'shrinking':    [True, False]\n",
    "     }),\n",
    "    ('sklearn.svm', 'SVR', 'poly_support_vector_regressor',\n",
    "     {\n",
    "         'kernel':       ['poly'],\n",
    "         'degree':       [2, 3],\n",
    "         'C':            [10**r for r in range(-5, 1)],\n",
    "         'gamma':        [10**r for r in range(-5, 0)] + ['scale', 'auto'],\n",
    "         'shrinking':    [True, False]\n",
    "     }),\n",
    "    ('sklearn.svm', 'SVR', 'sigmoid_support_vector_regressor',\n",
    "     {\n",
    "         'kernel':       ['sigmoid'],\n",
    "         'C':            [10**r for r in range(-5, 5)],\n",
    "         'gamma':        [10**r for r in range(-5, 4)] + ['scale', 'auto'],\n",
    "         'shrinking':    [True, False]\n",
    "     }),\n",
    "    ('sklearn.svm', 'SVR', 'rbf_support_vector_regressor',\n",
    "     {\n",
    "         'kernel':       ['rbf'],\n",
    "         'C':            [10**r for r in range(-5, 4)],\n",
    "         'gamma':        [10**r for r in range(-5, 4)] + ['scale', 'auto'],\n",
    "         'shrinking':    [True, False]\n",
    "     }),\n",
    "]\n",
    "\n",
    "regressor_selection_polynomial = [\n",
    "    ('sklearn.linear_model', 'LinearRegression', 'polynomial_regressor',\n",
    "     {\n",
    "         'fit_intercept': [True, False],\n",
    "         'normalize':     [True, False]\n",
    "     }),\n",
    "    ('sklearn.linear_model', 'ElasticNet', 'polynomial_elastic_net_regressor',\n",
    "     {\n",
    "         'alpha':         [0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0],\n",
    "         'l1_ratio':      [r*0.1 for r in range(0, 11)],\n",
    "         'fit_intercept': [True, False],\n",
    "         'normalize':     [True, False],\n",
    "         'max_iter':      [1000, 2000],\n",
    "         'random_state':  [0]\n",
    "     })\n",
    "]\n",
    "\n",
    "regressor_selection_tree = [\n",
    "    ('sklearn.tree', 'DecisionTreeRegressor', 'decision_tree_regressor',\n",
    "     # criterion='mae' is very slow:\n",
    "     # https://github.com/scikit-learn/scikit-learn/issues/9553#issuecomment-324484928\n",
    "     {\n",
    "         'splitter':          ['best', 'random'],\n",
    "         'max_depth':         list(range(5, 30)) + [None],\n",
    "         'min_samples_split': list(range(2, 11)),\n",
    "         'min_samples_leaf':  list(range(1, 11)),\n",
    "         'random_state':      [0]\n",
    "     }),\n",
    "    ('sklearn.ensemble', 'RandomForestRegressor', 'random_forest_regressor',\n",
    "     {\n",
    "         'n_estimators':      [100, 150, 200, 250],\n",
    "         'max_depth':         list(range(5, 30)) + [None],\n",
    "         'min_samples_split': list(range(2, 11)),\n",
    "         'min_samples_leaf':  list(range(1, 11)),\n",
    "         'random_state':      [0],\n",
    "         'n_jobs':            [-1]\n",
    "     }),\n",
    "    ('sklearn.ensemble', 'ExtraTreesRegressor', 'extra_trees_regressor',\n",
    "     {\n",
    "         'n_estimators':      [100, 150, 200, 250],\n",
    "         'max_depth':         list(range(5, 30)) + [None],\n",
    "         'min_samples_split': list(range(2, 11)),\n",
    "         'min_samples_leaf':  list(range(1, 11)),\n",
    "         'bootstrap':         [True, False],\n",
    "         'random_state':      [0],\n",
    "         'n_jobs':            [-1]\n",
    "     }),    \n",
    "    ('xgboost', 'XGBRegressor', 'xgb_regressor',\n",
    "     {\n",
    "         'n_estimators':      [100, 150, 200, 250],\n",
    "         'max_depth':         [5, 6, 7, 8, 9, 10],\n",
    "         'learning_rate':     [0.05, 0.10, 0.20, 0.30],\n",
    "         'min_child_weight':  [1, 3, 5],\n",
    "         'colsample_bytree':  [0.5, 0.6, 0.7, 0.8, 0.9, 1.],\n",
    "         'reg_alpha':         [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "         'reg_lambda':        [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "         'random_state':      [0],\n",
    "         'n_jobs':            [-1]\n",
    "    })\n",
    "]\n",
    "\n",
    "scorer = make_scorer(mean_absolute_percentage_error, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Module</th>\n",
       "      <th>Class</th>\n",
       "      <th>Name</th>\n",
       "      <th>BestParameters</th>\n",
       "      <th>BestScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>xgb_regressor</td>\n",
       "      <td>{'regressor__xgb_regressor__colsample_bytree':...</td>\n",
       "      <td>-0.116013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>xgb_regressor</td>\n",
       "      <td>{'regressor__xgb_regressor__colsample_bytree':...</td>\n",
       "      <td>-0.116363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>xgb_regressor</td>\n",
       "      <td>{'regressor__xgb_regressor__colsample_bytree':...</td>\n",
       "      <td>-0.116771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>xgb_regressor</td>\n",
       "      <td>{'regressor__xgb_regressor__reg_lambda': 0.4, ...</td>\n",
       "      <td>-0.117596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sklearn.ensemble</td>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>random_forest_regressor</td>\n",
       "      <td>{'regressor__random_forest_regressor__random_s...</td>\n",
       "      <td>-0.128045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sklearn.ensemble</td>\n",
       "      <td>ExtraTreesRegressor</td>\n",
       "      <td>extra_trees_regressor</td>\n",
       "      <td>{'regressor__extra_trees_regressor__random_sta...</td>\n",
       "      <td>-0.128983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sklearn.svm</td>\n",
       "      <td>SVR</td>\n",
       "      <td>rbf_support_vector_regressor</td>\n",
       "      <td>{'regressor__rbf_support_vector_regressor__shr...</td>\n",
       "      <td>-0.135221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sklearn.svm</td>\n",
       "      <td>SVR</td>\n",
       "      <td>poly_support_vector_regressor</td>\n",
       "      <td>{'regressor__poly_support_vector_regressor__sh...</td>\n",
       "      <td>-0.136324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sklearn.linear_model</td>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>polynomial_elastic_net_regressor</td>\n",
       "      <td>{'regressor__polynomialiser__interaction_only'...</td>\n",
       "      <td>-0.136395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sklearn.linear_model</td>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>polynomial_regressor</td>\n",
       "      <td>{'regressor__polynomialiser__interaction_only'...</td>\n",
       "      <td>-0.142985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sklearn.linear_model</td>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>linear_regressor</td>\n",
       "      <td>{'regressor__linear_regressor__normalize': Tru...</td>\n",
       "      <td>-0.147419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sklearn.svm</td>\n",
       "      <td>SVR</td>\n",
       "      <td>linear_support_vector_regressor</td>\n",
       "      <td>{'regressor__linear_support_vector_regressor__...</td>\n",
       "      <td>-0.147536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sklearn.linear_model</td>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>elastic_net_regressor</td>\n",
       "      <td>{'regressor__elastic_net_regressor__random_sta...</td>\n",
       "      <td>-0.149387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sklearn.svm</td>\n",
       "      <td>SVR</td>\n",
       "      <td>sigmoid_support_vector_regressor</td>\n",
       "      <td>{'regressor__sigmoid_support_vector_regressor_...</td>\n",
       "      <td>-0.153145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sklearn.neighbors</td>\n",
       "      <td>KNeighborsRegressor</td>\n",
       "      <td>k_nearest_neighbors_regressor</td>\n",
       "      <td>{'regressor__k_nearest_neighbors_regressor__p'...</td>\n",
       "      <td>-0.153735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sklearn.tree</td>\n",
       "      <td>DecisionTreeRegressor</td>\n",
       "      <td>decision_tree_regressor</td>\n",
       "      <td>{'regressor__decision_tree_regressor__splitter...</td>\n",
       "      <td>-0.157945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Module                  Class  \\\n",
       "15               xgboost           XGBRegressor   \n",
       "14               xgboost           XGBRegressor   \n",
       "13               xgboost           XGBRegressor   \n",
       "12               xgboost           XGBRegressor   \n",
       "10      sklearn.ensemble  RandomForestRegressor   \n",
       "11      sklearn.ensemble    ExtraTreesRegressor   \n",
       "6            sklearn.svm                    SVR   \n",
       "4            sklearn.svm                    SVR   \n",
       "8   sklearn.linear_model             ElasticNet   \n",
       "7   sklearn.linear_model       LinearRegression   \n",
       "0   sklearn.linear_model       LinearRegression   \n",
       "3            sklearn.svm                    SVR   \n",
       "1   sklearn.linear_model             ElasticNet   \n",
       "5            sklearn.svm                    SVR   \n",
       "2      sklearn.neighbors    KNeighborsRegressor   \n",
       "9           sklearn.tree  DecisionTreeRegressor   \n",
       "\n",
       "                                Name  \\\n",
       "15                     xgb_regressor   \n",
       "14                     xgb_regressor   \n",
       "13                     xgb_regressor   \n",
       "12                     xgb_regressor   \n",
       "10           random_forest_regressor   \n",
       "11             extra_trees_regressor   \n",
       "6       rbf_support_vector_regressor   \n",
       "4      poly_support_vector_regressor   \n",
       "8   polynomial_elastic_net_regressor   \n",
       "7               polynomial_regressor   \n",
       "0                   linear_regressor   \n",
       "3    linear_support_vector_regressor   \n",
       "1              elastic_net_regressor   \n",
       "5   sigmoid_support_vector_regressor   \n",
       "2      k_nearest_neighbors_regressor   \n",
       "9            decision_tree_regressor   \n",
       "\n",
       "                                       BestParameters  BestScore  \n",
       "15  {'regressor__xgb_regressor__colsample_bytree':...  -0.116013  \n",
       "14  {'regressor__xgb_regressor__colsample_bytree':...  -0.116363  \n",
       "13  {'regressor__xgb_regressor__colsample_bytree':...  -0.116771  \n",
       "12  {'regressor__xgb_regressor__reg_lambda': 0.4, ...  -0.117596  \n",
       "10  {'regressor__random_forest_regressor__random_s...  -0.128045  \n",
       "11  {'regressor__extra_trees_regressor__random_sta...  -0.128983  \n",
       "6   {'regressor__rbf_support_vector_regressor__shr...  -0.135221  \n",
       "4   {'regressor__poly_support_vector_regressor__sh...  -0.136324  \n",
       "8   {'regressor__polynomialiser__interaction_only'...  -0.136395  \n",
       "7   {'regressor__polynomialiser__interaction_only'...  -0.142985  \n",
       "0   {'regressor__linear_regressor__normalize': Tru...  -0.147419  \n",
       "3   {'regressor__linear_support_vector_regressor__...  -0.147536  \n",
       "1   {'regressor__elastic_net_regressor__random_sta...  -0.149387  \n",
       "5   {'regressor__sigmoid_support_vector_regressor_...  -0.153145  \n",
       "2   {'regressor__k_nearest_neighbors_regressor__p'...  -0.153735  \n",
       "9   {'regressor__decision_tree_regressor__splitter...  -0.157945  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    regressor_comparison = pd.read_pickle('results/regressor_comparison.pickle')\n",
    "except FileNotFoundError:\n",
    "    regressor_comparison = pd.DataFrame(columns=['Module', 'Class', 'Name', 'BestParameters', 'BestScore'])\n",
    "\n",
    "    for (module_name, class_name, regressor_name, parameters) in regressor_selection_linear:\n",
    "        print('Randomized search for {:<80}'.format(regressor_name))\n",
    "        \n",
    "        module_of_regressor = import_module(module_name)\n",
    "        class_of_regressor = getattr(module_of_regressor, class_name)\n",
    "        \n",
    "        regressor = TransformedTargetRegressor(Pipeline([\n",
    "            ('column_transformer', ColumnTransformer([\n",
    "                ('numericals_transformer', Pipeline([\n",
    "                    ('scaler', 'passthrough'),\n",
    "                    ('pca', 'passthrough')\n",
    "                ]), numerical_features),\n",
    "                ('categoricals_transformer', OneHotEncoder(drop='first'), categorical_features),\n",
    "                ('booleans_transformer', 'passthrough', boolean_features)\n",
    "            ], n_jobs=-1)),\n",
    "            (regressor_name, class_of_regressor())\n",
    "        ]), func=np.log, inverse_func=np.exp)\n",
    "        \n",
    "        parameters = {'regressor__' + regressor_name + '__' + param: parameters[param] for param in parameters.keys()}\n",
    "        parameters.update({\n",
    "            'regressor__column_transformer__numericals_transformer__scaler': [StandardScaler(), MaxAbsScaler(), Normalizer()],\n",
    "            'regressor__column_transformer__numericals_transformer__pca':    ['passthrough'] + [PCA(n_components, random_state=0) for n_components in range(1, len(numerical_features) + 1)]\n",
    "        })\n",
    "        \n",
    "        rs = RandomizedSearchCV(estimator=regressor, param_distributions=parameters, n_iter=100, \n",
    "                                scoring=scorer, cv=10, verbose=1, n_jobs=-1, random_state=0)\n",
    "        rs.fit(houses_train, y_train)\n",
    "\n",
    "        regressor_comparison = regressor_comparison.append({\n",
    "            'Module':         module_name,\n",
    "            'Class':          class_name,\n",
    "            'Name':           regressor_name,\n",
    "            'BestParameters': rs.best_params_,\n",
    "            'BestScore':      rs.best_score_\n",
    "        }, ignore_index=True)\n",
    "\n",
    "    for (module_name, class_name, regressor_name, parameters) in regressor_selection_polynomial:\n",
    "        print('Randomized search for {:<80}'.format(regressor_name))\n",
    "        \n",
    "        module_of_regressor = import_module(module_name)\n",
    "        class_of_regressor = getattr(module_of_regressor, class_name)\n",
    "        \n",
    "        regressor = TransformedTargetRegressor(Pipeline([\n",
    "            ('column_transformer', ColumnTransformer([\n",
    "                ('numericals_transformer', Pipeline([\n",
    "                    ('scaler', 'passthrough'),\n",
    "                    ('pca', 'passthrough')\n",
    "                ]), numerical_features),\n",
    "                ('categoricals_transformer', OneHotEncoder(drop='first'), categorical_features),\n",
    "                ('booleans_transformer', 'passthrough', boolean_features)\n",
    "            ], n_jobs=-1)),\n",
    "            ('polynomialiser', PolynomialFeatures()),\n",
    "            (regressor_name, class_of_regressor())\n",
    "        ]), func=np.log, inverse_func=np.exp)\n",
    "\n",
    "        parameters = {'regressor__' + regressor_name + '__' + param: parameters[param] for param in parameters.keys()}\n",
    "        parameters.update({\n",
    "            'regressor__column_transformer__numericals_transformer__scaler': [StandardScaler(), MaxAbsScaler(), Normalizer()],\n",
    "            'regressor__column_transformer__numericals_transformer__pca':    ['passthrough'] + [PCA(n_components, random_state=0) for n_components in range(1, len(numerical_features) + 1)],\n",
    "            'regressor__polynomialiser__interaction_only':                   [True, False]\n",
    "        })\n",
    "\n",
    "        rs = RandomizedSearchCV(estimator=regressor, param_distributions=parameters, n_iter=100,\n",
    "                                scoring=scorer, cv=10, verbose=1, n_jobs=-1, random_state=0)\n",
    "        rs.fit(houses_train, y_train)\n",
    "\n",
    "        regressor_comparison = regressor_comparison.append({\n",
    "            'Module':         module_name,\n",
    "            'Class':          class_name,\n",
    "            'Name':           regressor_name,\n",
    "            'BestParameters': rs.best_params_,\n",
    "            'BestScore':      rs.best_score_\n",
    "        }, ignore_index=True)\n",
    "\n",
    "    for (module_name, class_name, regressor_name, parameters) in regressor_selection_tree:\n",
    "        print('Randomized search for {:<80}'.format(regressor_name))\n",
    "        \n",
    "        module_of_regressor = import_module(module_name)\n",
    "        class_of_regressor = getattr(module_of_regressor, class_name)\n",
    "\n",
    "        regressor = TransformedTargetRegressor(Pipeline([\n",
    "            ('column_transformer', ColumnTransformer([\n",
    "                ('numericals_transformer', 'passthrough', numerical_features),\n",
    "                ('categoricals_transformer', OneHotEncoder(), categorical_features),\n",
    "                ('booleans_transformer', 'passthrough', boolean_features)\n",
    "            ], n_jobs=-1)),\n",
    "            (regressor_name, class_of_regressor())\n",
    "        ]), func=np.log, inverse_func=np.exp)\n",
    "\n",
    "        parameters = {'regressor__' + regressor_name + '__' + param: parameters[param] for param in parameters.keys()}\n",
    "        rs = RandomizedSearchCV(estimator=regressor, param_distributions=parameters, n_iter=100,\n",
    "                                scoring=scorer, cv=10, verbose=1, n_jobs=-1, random_state=0)\n",
    "        rs.fit(houses_train, y_train)\n",
    "\n",
    "        regressor_comparison = regressor_comparison.append({\n",
    "            'Module':         module_name,\n",
    "            'Class':          class_name,\n",
    "            'Name':           regressor_name,\n",
    "            'BestParameters': rs.best_params_,\n",
    "            'BestScore':      rs.best_score_\n",
    "        }, ignore_index=True)\n",
    "        \n",
    "    regressor_comparison.to_pickle(path='results/regressor_comparison.pickle')\n",
    "    regressor_comparison.to_csv(path_or_buf='results/regressor_comparison.csv', index=False)\n",
    "\n",
    "regressor_comparison.sort_values(by='BestScore', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We optimise the XGBRegressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'regressor__xgb_regressor__reg_lambda': 0.4,\n",
       " 'regressor__xgb_regressor__reg_alpha': 0.4,\n",
       " 'regressor__xgb_regressor__random_state': 0,\n",
       " 'regressor__xgb_regressor__n_jobs': -1,\n",
       " 'regressor__xgb_regressor__n_estimators': 250,\n",
       " 'regressor__xgb_regressor__min_child_weight': 5,\n",
       " 'regressor__xgb_regressor__max_depth': 10,\n",
       " 'regressor__xgb_regressor__learning_rate': 0.05,\n",
       " 'regressor__xgb_regressor__colsample_bytree': 0.8}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor_comparison.loc[12, 'BestParameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Module</th>\n",
       "      <th>Class</th>\n",
       "      <th>Name</th>\n",
       "      <th>BestParameters</th>\n",
       "      <th>BestScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>xgb_regressor</td>\n",
       "      <td>{'regressor__xgb_regressor__colsample_bytree':...</td>\n",
       "      <td>-0.116013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>xgb_regressor</td>\n",
       "      <td>{'regressor__xgb_regressor__colsample_bytree':...</td>\n",
       "      <td>-0.116363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>xgb_regressor</td>\n",
       "      <td>{'regressor__xgb_regressor__colsample_bytree':...</td>\n",
       "      <td>-0.116771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>xgb_regressor</td>\n",
       "      <td>{'regressor__xgb_regressor__reg_lambda': 0.4, ...</td>\n",
       "      <td>-0.117596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sklearn.ensemble</td>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>random_forest_regressor</td>\n",
       "      <td>{'regressor__random_forest_regressor__random_s...</td>\n",
       "      <td>-0.128045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sklearn.ensemble</td>\n",
       "      <td>ExtraTreesRegressor</td>\n",
       "      <td>extra_trees_regressor</td>\n",
       "      <td>{'regressor__extra_trees_regressor__random_sta...</td>\n",
       "      <td>-0.128983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sklearn.svm</td>\n",
       "      <td>SVR</td>\n",
       "      <td>rbf_support_vector_regressor</td>\n",
       "      <td>{'regressor__rbf_support_vector_regressor__shr...</td>\n",
       "      <td>-0.135221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sklearn.svm</td>\n",
       "      <td>SVR</td>\n",
       "      <td>poly_support_vector_regressor</td>\n",
       "      <td>{'regressor__poly_support_vector_regressor__sh...</td>\n",
       "      <td>-0.136324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sklearn.linear_model</td>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>polynomial_elastic_net_regressor</td>\n",
       "      <td>{'regressor__polynomialiser__interaction_only'...</td>\n",
       "      <td>-0.136395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sklearn.linear_model</td>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>polynomial_regressor</td>\n",
       "      <td>{'regressor__polynomialiser__interaction_only'...</td>\n",
       "      <td>-0.142985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sklearn.linear_model</td>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>linear_regressor</td>\n",
       "      <td>{'regressor__linear_regressor__normalize': Tru...</td>\n",
       "      <td>-0.147419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sklearn.svm</td>\n",
       "      <td>SVR</td>\n",
       "      <td>linear_support_vector_regressor</td>\n",
       "      <td>{'regressor__linear_support_vector_regressor__...</td>\n",
       "      <td>-0.147536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sklearn.linear_model</td>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>elastic_net_regressor</td>\n",
       "      <td>{'regressor__elastic_net_regressor__random_sta...</td>\n",
       "      <td>-0.149387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sklearn.svm</td>\n",
       "      <td>SVR</td>\n",
       "      <td>sigmoid_support_vector_regressor</td>\n",
       "      <td>{'regressor__sigmoid_support_vector_regressor_...</td>\n",
       "      <td>-0.153145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sklearn.neighbors</td>\n",
       "      <td>KNeighborsRegressor</td>\n",
       "      <td>k_nearest_neighbors_regressor</td>\n",
       "      <td>{'regressor__k_nearest_neighbors_regressor__p'...</td>\n",
       "      <td>-0.153735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sklearn.tree</td>\n",
       "      <td>DecisionTreeRegressor</td>\n",
       "      <td>decision_tree_regressor</td>\n",
       "      <td>{'regressor__decision_tree_regressor__splitter...</td>\n",
       "      <td>-0.157945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Module                  Class  \\\n",
       "15               xgboost           XGBRegressor   \n",
       "14               xgboost           XGBRegressor   \n",
       "13               xgboost           XGBRegressor   \n",
       "12               xgboost           XGBRegressor   \n",
       "10      sklearn.ensemble  RandomForestRegressor   \n",
       "11      sklearn.ensemble    ExtraTreesRegressor   \n",
       "6            sklearn.svm                    SVR   \n",
       "4            sklearn.svm                    SVR   \n",
       "8   sklearn.linear_model             ElasticNet   \n",
       "7   sklearn.linear_model       LinearRegression   \n",
       "0   sklearn.linear_model       LinearRegression   \n",
       "3            sklearn.svm                    SVR   \n",
       "1   sklearn.linear_model             ElasticNet   \n",
       "5            sklearn.svm                    SVR   \n",
       "2      sklearn.neighbors    KNeighborsRegressor   \n",
       "9           sklearn.tree  DecisionTreeRegressor   \n",
       "\n",
       "                                Name  \\\n",
       "15                     xgb_regressor   \n",
       "14                     xgb_regressor   \n",
       "13                     xgb_regressor   \n",
       "12                     xgb_regressor   \n",
       "10           random_forest_regressor   \n",
       "11             extra_trees_regressor   \n",
       "6       rbf_support_vector_regressor   \n",
       "4      poly_support_vector_regressor   \n",
       "8   polynomial_elastic_net_regressor   \n",
       "7               polynomial_regressor   \n",
       "0                   linear_regressor   \n",
       "3    linear_support_vector_regressor   \n",
       "1              elastic_net_regressor   \n",
       "5   sigmoid_support_vector_regressor   \n",
       "2      k_nearest_neighbors_regressor   \n",
       "9            decision_tree_regressor   \n",
       "\n",
       "                                       BestParameters  BestScore  \n",
       "15  {'regressor__xgb_regressor__colsample_bytree':...  -0.116013  \n",
       "14  {'regressor__xgb_regressor__colsample_bytree':...  -0.116363  \n",
       "13  {'regressor__xgb_regressor__colsample_bytree':...  -0.116771  \n",
       "12  {'regressor__xgb_regressor__reg_lambda': 0.4, ...  -0.117596  \n",
       "10  {'regressor__random_forest_regressor__random_s...  -0.128045  \n",
       "11  {'regressor__extra_trees_regressor__random_sta...  -0.128983  \n",
       "6   {'regressor__rbf_support_vector_regressor__shr...  -0.135221  \n",
       "4   {'regressor__poly_support_vector_regressor__sh...  -0.136324  \n",
       "8   {'regressor__polynomialiser__interaction_only'...  -0.136395  \n",
       "7   {'regressor__polynomialiser__interaction_only'...  -0.142985  \n",
       "0   {'regressor__linear_regressor__normalize': Tru...  -0.147419  \n",
       "3   {'regressor__linear_support_vector_regressor__...  -0.147536  \n",
       "1   {'regressor__elastic_net_regressor__random_sta...  -0.149387  \n",
       "5   {'regressor__sigmoid_support_vector_regressor_...  -0.153145  \n",
       "2   {'regressor__k_nearest_neighbors_regressor__p'...  -0.153735  \n",
       "9   {'regressor__decision_tree_regressor__splitter...  -0.157945  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if regressor_comparison.shape[0] < 14:\n",
    "    regressor = TransformedTargetRegressor(Pipeline([\n",
    "        ('column_transformer', ColumnTransformer([\n",
    "            ('numericals_transformer', 'passthrough', numerical_features),\n",
    "            ('categoricals_transformer', OneHotEncoder(), categorical_features),\n",
    "            ('booleans_transformer', 'passthrough', boolean_features)\n",
    "        ], n_jobs=-1)),\n",
    "        ('xgb_regressor', XGBRegressor())\n",
    "    ]), func=np.log, inverse_func=np.exp)\n",
    "\n",
    "    parameters = {\n",
    "         'n_estimators':      [200, 250, 300],\n",
    "         'max_depth':         [8, 10, 12],\n",
    "         'learning_rate':     [0.04, 0.05, 0.06],\n",
    "         'min_child_weight':  [4, 5, 6],\n",
    "         'colsample_bytree':  [0.7, 0.8, 0.9],\n",
    "         'reg_alpha':         [0.3, 0.4, 0.5],\n",
    "         'reg_lambda':        [0.3, 0.4, 0.5],\n",
    "         'random_state':      [0],\n",
    "         'n_jobs':            [-1]\n",
    "    }\n",
    "    parameters = {'regressor__xgb_regressor__' + param: parameters[param] for param in parameters.keys()}\n",
    "\n",
    "    gs = GridSearchCV(estimator=regressor, param_grid=parameters, scoring=scorer, cv=10, verbose=1, n_jobs=-1)\n",
    "    gs.fit(houses_train, y_train)\n",
    "\n",
    "    regressor_comparison = regressor_comparison.append({\n",
    "        'Module':         module_name,\n",
    "        'Class':          class_name,\n",
    "        'Name':           regressor_name,\n",
    "        'BestParameters': gs.best_params_,\n",
    "        'BestScore':      gs.best_score_\n",
    "    }, ignore_index=True)\n",
    "\n",
    "    regressor_comparison.to_pickle(path='results/regressor_comparison.pickle')\n",
    "    regressor_comparison.to_csv(path_or_buf='results/regressor_comparison.csv', index=False)\n",
    "\n",
    "regressor_comparison.sort_values(by='BestScore', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'regressor__xgb_regressor__colsample_bytree': 0.7,\n",
       " 'regressor__xgb_regressor__learning_rate': 0.04,\n",
       " 'regressor__xgb_regressor__max_depth': 12,\n",
       " 'regressor__xgb_regressor__min_child_weight': 6,\n",
       " 'regressor__xgb_regressor__n_estimators': 300,\n",
       " 'regressor__xgb_regressor__n_jobs': -1,\n",
       " 'regressor__xgb_regressor__random_state': 0,\n",
       " 'regressor__xgb_regressor__reg_alpha': 0.5,\n",
       " 'regressor__xgb_regressor__reg_lambda': 0.4}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor_comparison.loc[13, 'BestParameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Module</th>\n",
       "      <th>Class</th>\n",
       "      <th>Name</th>\n",
       "      <th>BestParameters</th>\n",
       "      <th>BestScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>xgb_regressor</td>\n",
       "      <td>{'regressor__xgb_regressor__colsample_bytree':...</td>\n",
       "      <td>-0.116013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>xgb_regressor</td>\n",
       "      <td>{'regressor__xgb_regressor__colsample_bytree':...</td>\n",
       "      <td>-0.116363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>xgb_regressor</td>\n",
       "      <td>{'regressor__xgb_regressor__colsample_bytree':...</td>\n",
       "      <td>-0.116771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>xgb_regressor</td>\n",
       "      <td>{'regressor__xgb_regressor__reg_lambda': 0.4, ...</td>\n",
       "      <td>-0.117596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sklearn.ensemble</td>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>random_forest_regressor</td>\n",
       "      <td>{'regressor__random_forest_regressor__random_s...</td>\n",
       "      <td>-0.128045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sklearn.ensemble</td>\n",
       "      <td>ExtraTreesRegressor</td>\n",
       "      <td>extra_trees_regressor</td>\n",
       "      <td>{'regressor__extra_trees_regressor__random_sta...</td>\n",
       "      <td>-0.128983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sklearn.svm</td>\n",
       "      <td>SVR</td>\n",
       "      <td>rbf_support_vector_regressor</td>\n",
       "      <td>{'regressor__rbf_support_vector_regressor__shr...</td>\n",
       "      <td>-0.135221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sklearn.svm</td>\n",
       "      <td>SVR</td>\n",
       "      <td>poly_support_vector_regressor</td>\n",
       "      <td>{'regressor__poly_support_vector_regressor__sh...</td>\n",
       "      <td>-0.136324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sklearn.linear_model</td>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>polynomial_elastic_net_regressor</td>\n",
       "      <td>{'regressor__polynomialiser__interaction_only'...</td>\n",
       "      <td>-0.136395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sklearn.linear_model</td>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>polynomial_regressor</td>\n",
       "      <td>{'regressor__polynomialiser__interaction_only'...</td>\n",
       "      <td>-0.142985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sklearn.linear_model</td>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>linear_regressor</td>\n",
       "      <td>{'regressor__linear_regressor__normalize': Tru...</td>\n",
       "      <td>-0.147419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sklearn.svm</td>\n",
       "      <td>SVR</td>\n",
       "      <td>linear_support_vector_regressor</td>\n",
       "      <td>{'regressor__linear_support_vector_regressor__...</td>\n",
       "      <td>-0.147536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sklearn.linear_model</td>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>elastic_net_regressor</td>\n",
       "      <td>{'regressor__elastic_net_regressor__random_sta...</td>\n",
       "      <td>-0.149387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sklearn.svm</td>\n",
       "      <td>SVR</td>\n",
       "      <td>sigmoid_support_vector_regressor</td>\n",
       "      <td>{'regressor__sigmoid_support_vector_regressor_...</td>\n",
       "      <td>-0.153145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sklearn.neighbors</td>\n",
       "      <td>KNeighborsRegressor</td>\n",
       "      <td>k_nearest_neighbors_regressor</td>\n",
       "      <td>{'regressor__k_nearest_neighbors_regressor__p'...</td>\n",
       "      <td>-0.153735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sklearn.tree</td>\n",
       "      <td>DecisionTreeRegressor</td>\n",
       "      <td>decision_tree_regressor</td>\n",
       "      <td>{'regressor__decision_tree_regressor__splitter...</td>\n",
       "      <td>-0.157945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Module                  Class  \\\n",
       "15               xgboost           XGBRegressor   \n",
       "14               xgboost           XGBRegressor   \n",
       "13               xgboost           XGBRegressor   \n",
       "12               xgboost           XGBRegressor   \n",
       "10      sklearn.ensemble  RandomForestRegressor   \n",
       "11      sklearn.ensemble    ExtraTreesRegressor   \n",
       "6            sklearn.svm                    SVR   \n",
       "4            sklearn.svm                    SVR   \n",
       "8   sklearn.linear_model             ElasticNet   \n",
       "7   sklearn.linear_model       LinearRegression   \n",
       "0   sklearn.linear_model       LinearRegression   \n",
       "3            sklearn.svm                    SVR   \n",
       "1   sklearn.linear_model             ElasticNet   \n",
       "5            sklearn.svm                    SVR   \n",
       "2      sklearn.neighbors    KNeighborsRegressor   \n",
       "9           sklearn.tree  DecisionTreeRegressor   \n",
       "\n",
       "                                Name  \\\n",
       "15                     xgb_regressor   \n",
       "14                     xgb_regressor   \n",
       "13                     xgb_regressor   \n",
       "12                     xgb_regressor   \n",
       "10           random_forest_regressor   \n",
       "11             extra_trees_regressor   \n",
       "6       rbf_support_vector_regressor   \n",
       "4      poly_support_vector_regressor   \n",
       "8   polynomial_elastic_net_regressor   \n",
       "7               polynomial_regressor   \n",
       "0                   linear_regressor   \n",
       "3    linear_support_vector_regressor   \n",
       "1              elastic_net_regressor   \n",
       "5   sigmoid_support_vector_regressor   \n",
       "2      k_nearest_neighbors_regressor   \n",
       "9            decision_tree_regressor   \n",
       "\n",
       "                                       BestParameters  BestScore  \n",
       "15  {'regressor__xgb_regressor__colsample_bytree':...  -0.116013  \n",
       "14  {'regressor__xgb_regressor__colsample_bytree':...  -0.116363  \n",
       "13  {'regressor__xgb_regressor__colsample_bytree':...  -0.116771  \n",
       "12  {'regressor__xgb_regressor__reg_lambda': 0.4, ...  -0.117596  \n",
       "10  {'regressor__random_forest_regressor__random_s...  -0.128045  \n",
       "11  {'regressor__extra_trees_regressor__random_sta...  -0.128983  \n",
       "6   {'regressor__rbf_support_vector_regressor__shr...  -0.135221  \n",
       "4   {'regressor__poly_support_vector_regressor__sh...  -0.136324  \n",
       "8   {'regressor__polynomialiser__interaction_only'...  -0.136395  \n",
       "7   {'regressor__polynomialiser__interaction_only'...  -0.142985  \n",
       "0   {'regressor__linear_regressor__normalize': Tru...  -0.147419  \n",
       "3   {'regressor__linear_support_vector_regressor__...  -0.147536  \n",
       "1   {'regressor__elastic_net_regressor__random_sta...  -0.149387  \n",
       "5   {'regressor__sigmoid_support_vector_regressor_...  -0.153145  \n",
       "2   {'regressor__k_nearest_neighbors_regressor__p'...  -0.153735  \n",
       "9   {'regressor__decision_tree_regressor__splitter...  -0.157945  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if regressor_comparison.shape[0] < 15:\n",
    "    regressor = TransformedTargetRegressor(Pipeline([\n",
    "        ('column_transformer', ColumnTransformer([\n",
    "            ('numericals_transformer', 'passthrough', numerical_features),\n",
    "            ('categoricals_transformer', OneHotEncoder(), categorical_features),\n",
    "            ('booleans_transformer', 'passthrough', boolean_features)\n",
    "        ], n_jobs=-1)),\n",
    "        ('xgb_regressor', XGBRegressor())\n",
    "    ]), func=np.log, inverse_func=np.exp)\n",
    "\n",
    "    parameters = {\n",
    "         'n_estimators':      [250, 300, 350],\n",
    "         'max_depth':         [11, 12, 13],\n",
    "         'learning_rate':     [0.03, 0.04, 0.05],\n",
    "         'min_child_weight':  [5, 6, 7],\n",
    "         'colsample_bytree':  [0.6, 0.7, 0.8],\n",
    "         'reg_alpha':         [0.4, 0.5, 0.6],\n",
    "         'reg_lambda':        [0.3, 0.4, 0.5],\n",
    "         'random_state':      [0],\n",
    "         'n_jobs':            [-1]\n",
    "    }\n",
    "    parameters = {'regressor__xgb_regressor__' + param: parameters[param] for param in parameters.keys()}\n",
    "\n",
    "    gs = GridSearchCV(estimator=regressor, param_grid=parameters, scoring=scorer, cv=10, verbose=1, n_jobs=-1)\n",
    "    gs.fit(houses_train, y_train)\n",
    "\n",
    "    regressor_comparison = regressor_comparison.append({\n",
    "        'Module':         module_name,\n",
    "        'Class':          class_name,\n",
    "        'Name':           regressor_name,\n",
    "        'BestParameters': gs.best_params_,\n",
    "        'BestScore':      gs.best_score_\n",
    "    }, ignore_index=True)\n",
    "\n",
    "    regressor_comparison.to_pickle(path='results/regressor_comparison.pickle')\n",
    "    regressor_comparison.to_csv(path_or_buf='results/regressor_comparison.csv', index=False)\n",
    "\n",
    "regressor_comparison.sort_values(by='BestScore', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'regressor__xgb_regressor__colsample_bytree': 0.6,\n",
       " 'regressor__xgb_regressor__learning_rate': 0.04,\n",
       " 'regressor__xgb_regressor__max_depth': 12,\n",
       " 'regressor__xgb_regressor__min_child_weight': 5,\n",
       " 'regressor__xgb_regressor__n_estimators': 350,\n",
       " 'regressor__xgb_regressor__n_jobs': -1,\n",
       " 'regressor__xgb_regressor__random_state': 0,\n",
       " 'regressor__xgb_regressor__reg_alpha': 0.6,\n",
       " 'regressor__xgb_regressor__reg_lambda': 0.3}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor_comparison.loc[14, 'BestParameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Module</th>\n",
       "      <th>Class</th>\n",
       "      <th>Name</th>\n",
       "      <th>BestParameters</th>\n",
       "      <th>BestScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>xgb_regressor</td>\n",
       "      <td>{'regressor__xgb_regressor__colsample_bytree':...</td>\n",
       "      <td>-0.116013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>xgb_regressor</td>\n",
       "      <td>{'regressor__xgb_regressor__colsample_bytree':...</td>\n",
       "      <td>-0.116363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>xgb_regressor</td>\n",
       "      <td>{'regressor__xgb_regressor__colsample_bytree':...</td>\n",
       "      <td>-0.116771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>xgboost</td>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>xgb_regressor</td>\n",
       "      <td>{'regressor__xgb_regressor__reg_lambda': 0.4, ...</td>\n",
       "      <td>-0.117596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>sklearn.ensemble</td>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>random_forest_regressor</td>\n",
       "      <td>{'regressor__random_forest_regressor__random_s...</td>\n",
       "      <td>-0.128045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sklearn.ensemble</td>\n",
       "      <td>ExtraTreesRegressor</td>\n",
       "      <td>extra_trees_regressor</td>\n",
       "      <td>{'regressor__extra_trees_regressor__random_sta...</td>\n",
       "      <td>-0.128983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sklearn.svm</td>\n",
       "      <td>SVR</td>\n",
       "      <td>rbf_support_vector_regressor</td>\n",
       "      <td>{'regressor__rbf_support_vector_regressor__shr...</td>\n",
       "      <td>-0.135221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sklearn.svm</td>\n",
       "      <td>SVR</td>\n",
       "      <td>poly_support_vector_regressor</td>\n",
       "      <td>{'regressor__poly_support_vector_regressor__sh...</td>\n",
       "      <td>-0.136324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sklearn.linear_model</td>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>polynomial_elastic_net_regressor</td>\n",
       "      <td>{'regressor__polynomialiser__interaction_only'...</td>\n",
       "      <td>-0.136395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sklearn.linear_model</td>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>polynomial_regressor</td>\n",
       "      <td>{'regressor__polynomialiser__interaction_only'...</td>\n",
       "      <td>-0.142985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sklearn.linear_model</td>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>linear_regressor</td>\n",
       "      <td>{'regressor__linear_regressor__normalize': Tru...</td>\n",
       "      <td>-0.147419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sklearn.svm</td>\n",
       "      <td>SVR</td>\n",
       "      <td>linear_support_vector_regressor</td>\n",
       "      <td>{'regressor__linear_support_vector_regressor__...</td>\n",
       "      <td>-0.147536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sklearn.linear_model</td>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>elastic_net_regressor</td>\n",
       "      <td>{'regressor__elastic_net_regressor__random_sta...</td>\n",
       "      <td>-0.149387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sklearn.svm</td>\n",
       "      <td>SVR</td>\n",
       "      <td>sigmoid_support_vector_regressor</td>\n",
       "      <td>{'regressor__sigmoid_support_vector_regressor_...</td>\n",
       "      <td>-0.153145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sklearn.neighbors</td>\n",
       "      <td>KNeighborsRegressor</td>\n",
       "      <td>k_nearest_neighbors_regressor</td>\n",
       "      <td>{'regressor__k_nearest_neighbors_regressor__p'...</td>\n",
       "      <td>-0.153735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sklearn.tree</td>\n",
       "      <td>DecisionTreeRegressor</td>\n",
       "      <td>decision_tree_regressor</td>\n",
       "      <td>{'regressor__decision_tree_regressor__splitter...</td>\n",
       "      <td>-0.157945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Module                  Class  \\\n",
       "15               xgboost           XGBRegressor   \n",
       "14               xgboost           XGBRegressor   \n",
       "13               xgboost           XGBRegressor   \n",
       "12               xgboost           XGBRegressor   \n",
       "10      sklearn.ensemble  RandomForestRegressor   \n",
       "11      sklearn.ensemble    ExtraTreesRegressor   \n",
       "6            sklearn.svm                    SVR   \n",
       "4            sklearn.svm                    SVR   \n",
       "8   sklearn.linear_model             ElasticNet   \n",
       "7   sklearn.linear_model       LinearRegression   \n",
       "0   sklearn.linear_model       LinearRegression   \n",
       "3            sklearn.svm                    SVR   \n",
       "1   sklearn.linear_model             ElasticNet   \n",
       "5            sklearn.svm                    SVR   \n",
       "2      sklearn.neighbors    KNeighborsRegressor   \n",
       "9           sklearn.tree  DecisionTreeRegressor   \n",
       "\n",
       "                                Name  \\\n",
       "15                     xgb_regressor   \n",
       "14                     xgb_regressor   \n",
       "13                     xgb_regressor   \n",
       "12                     xgb_regressor   \n",
       "10           random_forest_regressor   \n",
       "11             extra_trees_regressor   \n",
       "6       rbf_support_vector_regressor   \n",
       "4      poly_support_vector_regressor   \n",
       "8   polynomial_elastic_net_regressor   \n",
       "7               polynomial_regressor   \n",
       "0                   linear_regressor   \n",
       "3    linear_support_vector_regressor   \n",
       "1              elastic_net_regressor   \n",
       "5   sigmoid_support_vector_regressor   \n",
       "2      k_nearest_neighbors_regressor   \n",
       "9            decision_tree_regressor   \n",
       "\n",
       "                                       BestParameters  BestScore  \n",
       "15  {'regressor__xgb_regressor__colsample_bytree':...  -0.116013  \n",
       "14  {'regressor__xgb_regressor__colsample_bytree':...  -0.116363  \n",
       "13  {'regressor__xgb_regressor__colsample_bytree':...  -0.116771  \n",
       "12  {'regressor__xgb_regressor__reg_lambda': 0.4, ...  -0.117596  \n",
       "10  {'regressor__random_forest_regressor__random_s...  -0.128045  \n",
       "11  {'regressor__extra_trees_regressor__random_sta...  -0.128983  \n",
       "6   {'regressor__rbf_support_vector_regressor__shr...  -0.135221  \n",
       "4   {'regressor__poly_support_vector_regressor__sh...  -0.136324  \n",
       "8   {'regressor__polynomialiser__interaction_only'...  -0.136395  \n",
       "7   {'regressor__polynomialiser__interaction_only'...  -0.142985  \n",
       "0   {'regressor__linear_regressor__normalize': Tru...  -0.147419  \n",
       "3   {'regressor__linear_support_vector_regressor__...  -0.147536  \n",
       "1   {'regressor__elastic_net_regressor__random_sta...  -0.149387  \n",
       "5   {'regressor__sigmoid_support_vector_regressor_...  -0.153145  \n",
       "2   {'regressor__k_nearest_neighbors_regressor__p'...  -0.153735  \n",
       "9   {'regressor__decision_tree_regressor__splitter...  -0.157945  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if regressor_comparison.shape[0] < 16:\n",
    "    regressor = TransformedTargetRegressor(Pipeline([\n",
    "        ('column_transformer', ColumnTransformer([\n",
    "            ('numericals_transformer', 'passthrough', numerical_features),\n",
    "            ('categoricals_transformer', OneHotEncoder(), categorical_features),\n",
    "            ('booleans_transformer', 'passthrough', boolean_features)\n",
    "        ], n_jobs=-1)),\n",
    "        ('xgb_regressor', XGBRegressor())\n",
    "    ]), func=np.log, inverse_func=np.exp)\n",
    "\n",
    "    parameters = {\n",
    "         'n_estimators':      [300, 350, 400],\n",
    "         'max_depth':         [11, 12, 13],\n",
    "         'learning_rate':     [0.03, 0.04, 0.05],\n",
    "         'min_child_weight':  [4, 5, 6],\n",
    "         'colsample_bytree':  [0.5, 0.6, 0.7],\n",
    "         'reg_alpha':         [0.5, 0.6, 0.7],\n",
    "         'reg_lambda':        [0.2, 0.3, 0.4],\n",
    "         'random_state':      [0],\n",
    "         'n_jobs':            [-1]\n",
    "    }\n",
    "    parameters = {'regressor__xgb_regressor__' + param: parameters[param] for param in parameters.keys()}\n",
    "\n",
    "    gs = GridSearchCV(estimator=regressor, param_grid=parameters, scoring=scorer, cv=10, verbose=1, n_jobs=-1)\n",
    "    gs.fit(houses_train, y_train)\n",
    "\n",
    "    regressor_comparison = regressor_comparison.append({\n",
    "        'Module':         module_name,\n",
    "        'Class':          class_name,\n",
    "        'Name':           regressor_name,\n",
    "        'BestParameters': gs.best_params_,\n",
    "        'BestScore':      gs.best_score_\n",
    "    }, ignore_index=True)\n",
    "\n",
    "    regressor_comparison.to_pickle(path='results/regressor_comparison.pickle')\n",
    "    regressor_comparison.to_csv(path_or_buf='results/regressor_comparison.csv', index=False)\n",
    "\n",
    "regressor_comparison.sort_values(by='BestScore', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best performing regression model found is an XGBRegressor with the following parameters. On average, the regressor achieves a mean absolute percentage error of 11.6% on the validation sets of a 10-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'regressor__xgb_regressor__colsample_bytree': 0.5,\n",
       " 'regressor__xgb_regressor__learning_rate': 0.03,\n",
       " 'regressor__xgb_regressor__max_depth': 12,\n",
       " 'regressor__xgb_regressor__min_child_weight': 5,\n",
       " 'regressor__xgb_regressor__n_estimators': 400,\n",
       " 'regressor__xgb_regressor__n_jobs': -1,\n",
       " 'regressor__xgb_regressor__random_state': 0,\n",
       " 'regressor__xgb_regressor__reg_alpha': 0.5,\n",
       " 'regressor__xgb_regressor__reg_lambda': 0.4}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor_comparison.loc[15, 'BestParameters']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting ensemble\n",
    "\n",
    "We study the results of a voting regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_estimator(idx):\n",
    "    module_name = regressor_comparison.loc[idx, 'Module']\n",
    "    class_name = regressor_comparison.loc[idx, 'Class']\n",
    "    regressor_name = regressor_comparison.loc[idx, 'Name']\n",
    "    parameters = regressor_comparison.loc[idx, 'BestParameters']\n",
    "\n",
    "    module_of_regressor = import_module(module_name)\n",
    "    class_of_regressor = getattr(module_of_regressor, class_name)\n",
    "    if idx <= 6:\n",
    "        regressor = TransformedTargetRegressor(Pipeline([\n",
    "            ('column_transformer', ColumnTransformer([\n",
    "                ('numericals_transformer', Pipeline([\n",
    "                    ('scaler', 'passthrough'),\n",
    "                    ('pca', 'passthrough')\n",
    "                ]), numerical_features),\n",
    "                ('categoricals_transformer', OneHotEncoder(drop='first'), categorical_features),\n",
    "                ('booleans_transformer', 'passthrough', boolean_features)\n",
    "            ], n_jobs=-1)),\n",
    "            (regressor_name, class_of_regressor())\n",
    "        ]), func=np.log, inverse_func=np.exp)\n",
    "    elif idx <= 8:\n",
    "        regressor = TransformedTargetRegressor(Pipeline([\n",
    "            ('column_transformer', ColumnTransformer([\n",
    "                ('numericals_transformer', Pipeline([\n",
    "                    ('scaler', 'passthrough'),\n",
    "                    ('pca', 'passthrough')\n",
    "                ]), numerical_features),\n",
    "                ('categoricals_transformer', OneHotEncoder(drop='first'), categorical_features),\n",
    "                ('booleans_transformer', 'passthrough', boolean_features)\n",
    "            ], n_jobs=-1)),\n",
    "            ('polynomialiser', PolynomialFeatures()),\n",
    "            (regressor_name, class_of_regressor())\n",
    "        ]), func=np.log, inverse_func=np.exp)\n",
    "    else:\n",
    "        regressor = TransformedTargetRegressor(Pipeline([\n",
    "            ('column_transformer', ColumnTransformer([\n",
    "                ('numericals_transformer', 'passthrough', numerical_features),\n",
    "                ('categoricals_transformer', OneHotEncoder(), categorical_features),\n",
    "                ('booleans_transformer', 'passthrough', boolean_features)\n",
    "            ], n_jobs=-1)),\n",
    "            (regressor_name, class_of_regressor())\n",
    "        ]), func=np.log, inverse_func=np.exp)\n",
    "    regressor.set_params(**parameters)\n",
    "    \n",
    "    return regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean valid score: 0.127\n"
     ]
    }
   ],
   "source": [
    "voting_regressor = VotingRegressor([('reg_{}'.format(idx), load_estimator(idx))\n",
    "                                    for idx in list(range(12)) + [15]], n_jobs=-1)\n",
    "\n",
    "cv_results = cross_validate(voting_regressor, houses_train, y_train, scoring=scorer, cv=10, n_jobs=-1)\n",
    "print('Mean valid score: {:.3f}'.format(-cv_results['test_score'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The voting regressor does not seem to achieve better results.\n",
    "\n",
    "## Feature selection\n",
    "\n",
    "We use the feature importances of our best performing single regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_regressor = load_estimator(15)\n",
    "\n",
    "cv_results = cross_validate(xgb_regressor, houses_train, y_train, scoring=scorer, cv=10, return_estimator=True,\n",
    "                            n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.concatenate([\n",
    "    numerical_features,\n",
    "    cv_results['estimator'][0].regressor_['column_transformer'].transformers_[1][1]\\\n",
    "        .get_feature_names(categorical_features),\n",
    "    boolean_features\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sqft_living log</th>\n",
       "      <th>base_area log</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>bedrooms bin_1</th>\n",
       "      <th>bedrooms bin_2</th>\n",
       "      <th>bedrooms bin_3</th>\n",
       "      <th>bedrooms bin_4</th>\n",
       "      <th>bedrooms bin_5</th>\n",
       "      <th>...</th>\n",
       "      <th>grade bin_7</th>\n",
       "      <th>grade bin_8</th>\n",
       "      <th>grade bin_9</th>\n",
       "      <th>view bin_0</th>\n",
       "      <th>view bin_1 to 2</th>\n",
       "      <th>view bin_3</th>\n",
       "      <th>view bin_4</th>\n",
       "      <th>has_basement</th>\n",
       "      <th>is_renovated</th>\n",
       "      <th>waterfront</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.036157</td>\n",
       "      <td>0.006187</td>\n",
       "      <td>0.002267</td>\n",
       "      <td>0.019908</td>\n",
       "      <td>0.002513</td>\n",
       "      <td>0.002693</td>\n",
       "      <td>0.001076</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>0.000586</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022487</td>\n",
       "      <td>0.006256</td>\n",
       "      <td>0.013003</td>\n",
       "      <td>0.060652</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>0.001945</td>\n",
       "      <td>0.040625</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>0.002775</td>\n",
       "      <td>0.068775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.037694</td>\n",
       "      <td>0.006182</td>\n",
       "      <td>0.002085</td>\n",
       "      <td>0.020407</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.002339</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022598</td>\n",
       "      <td>0.007218</td>\n",
       "      <td>0.016322</td>\n",
       "      <td>0.058324</td>\n",
       "      <td>0.002018</td>\n",
       "      <td>0.001952</td>\n",
       "      <td>0.040263</td>\n",
       "      <td>0.001016</td>\n",
       "      <td>0.002407</td>\n",
       "      <td>0.058316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.036470</td>\n",
       "      <td>0.006174</td>\n",
       "      <td>0.002121</td>\n",
       "      <td>0.021346</td>\n",
       "      <td>0.002752</td>\n",
       "      <td>0.001956</td>\n",
       "      <td>0.002113</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.000739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024238</td>\n",
       "      <td>0.008399</td>\n",
       "      <td>0.013070</td>\n",
       "      <td>0.058928</td>\n",
       "      <td>0.002198</td>\n",
       "      <td>0.001781</td>\n",
       "      <td>0.040202</td>\n",
       "      <td>0.001008</td>\n",
       "      <td>0.002799</td>\n",
       "      <td>0.066886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.037683</td>\n",
       "      <td>0.006235</td>\n",
       "      <td>0.002084</td>\n",
       "      <td>0.020737</td>\n",
       "      <td>0.002687</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>0.000715</td>\n",
       "      <td>0.000598</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022546</td>\n",
       "      <td>0.006555</td>\n",
       "      <td>0.014036</td>\n",
       "      <td>0.057902</td>\n",
       "      <td>0.002548</td>\n",
       "      <td>0.001795</td>\n",
       "      <td>0.036853</td>\n",
       "      <td>0.000926</td>\n",
       "      <td>0.002771</td>\n",
       "      <td>0.067337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.037943</td>\n",
       "      <td>0.006109</td>\n",
       "      <td>0.002203</td>\n",
       "      <td>0.020872</td>\n",
       "      <td>0.002619</td>\n",
       "      <td>0.002132</td>\n",
       "      <td>0.001054</td>\n",
       "      <td>0.000618</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020230</td>\n",
       "      <td>0.007485</td>\n",
       "      <td>0.014605</td>\n",
       "      <td>0.057169</td>\n",
       "      <td>0.002286</td>\n",
       "      <td>0.002042</td>\n",
       "      <td>0.041905</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>0.002333</td>\n",
       "      <td>0.056202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.036812</td>\n",
       "      <td>0.006176</td>\n",
       "      <td>0.002328</td>\n",
       "      <td>0.020336</td>\n",
       "      <td>0.002765</td>\n",
       "      <td>0.002747</td>\n",
       "      <td>0.001143</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.000557</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020762</td>\n",
       "      <td>0.006552</td>\n",
       "      <td>0.016041</td>\n",
       "      <td>0.058184</td>\n",
       "      <td>0.002202</td>\n",
       "      <td>0.001794</td>\n",
       "      <td>0.040732</td>\n",
       "      <td>0.001009</td>\n",
       "      <td>0.002633</td>\n",
       "      <td>0.059218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.038081</td>\n",
       "      <td>0.006307</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.020807</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.002015</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.000778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023022</td>\n",
       "      <td>0.006758</td>\n",
       "      <td>0.015064</td>\n",
       "      <td>0.054873</td>\n",
       "      <td>0.002282</td>\n",
       "      <td>0.002140</td>\n",
       "      <td>0.038709</td>\n",
       "      <td>0.001047</td>\n",
       "      <td>0.002795</td>\n",
       "      <td>0.064894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.036758</td>\n",
       "      <td>0.005756</td>\n",
       "      <td>0.002349</td>\n",
       "      <td>0.020740</td>\n",
       "      <td>0.002703</td>\n",
       "      <td>0.002047</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018709</td>\n",
       "      <td>0.007051</td>\n",
       "      <td>0.014143</td>\n",
       "      <td>0.058643</td>\n",
       "      <td>0.002189</td>\n",
       "      <td>0.001756</td>\n",
       "      <td>0.043581</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>0.002641</td>\n",
       "      <td>0.060859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.036660</td>\n",
       "      <td>0.006175</td>\n",
       "      <td>0.002239</td>\n",
       "      <td>0.020107</td>\n",
       "      <td>0.002538</td>\n",
       "      <td>0.002507</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>0.000659</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.000771</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022611</td>\n",
       "      <td>0.006015</td>\n",
       "      <td>0.014564</td>\n",
       "      <td>0.061048</td>\n",
       "      <td>0.002559</td>\n",
       "      <td>0.001465</td>\n",
       "      <td>0.041579</td>\n",
       "      <td>0.000993</td>\n",
       "      <td>0.002685</td>\n",
       "      <td>0.064507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.037422</td>\n",
       "      <td>0.005963</td>\n",
       "      <td>0.002132</td>\n",
       "      <td>0.020527</td>\n",
       "      <td>0.002661</td>\n",
       "      <td>0.002485</td>\n",
       "      <td>0.000872</td>\n",
       "      <td>0.000580</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023153</td>\n",
       "      <td>0.006775</td>\n",
       "      <td>0.014104</td>\n",
       "      <td>0.059083</td>\n",
       "      <td>0.002260</td>\n",
       "      <td>0.001416</td>\n",
       "      <td>0.042923</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.002946</td>\n",
       "      <td>0.065736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   sqft_living log  base_area log  sqft_lot       lat      long  \\\n",
       "0         0.036157       0.006187  0.002267  0.019908  0.002513   \n",
       "1         0.037694       0.006182  0.002085  0.020407  0.002600   \n",
       "2         0.036470       0.006174  0.002121  0.021346  0.002752   \n",
       "3         0.037683       0.006235  0.002084  0.020737  0.002687   \n",
       "4         0.037943       0.006109  0.002203  0.020872  0.002619   \n",
       "5         0.036812       0.006176  0.002328  0.020336  0.002765   \n",
       "6         0.038081       0.006307  0.002174  0.020807  0.002800   \n",
       "7         0.036758       0.005756  0.002349  0.020740  0.002703   \n",
       "8         0.036660       0.006175  0.002239  0.020107  0.002538   \n",
       "9         0.037422       0.005963  0.002132  0.020527  0.002661   \n",
       "\n",
       "   bedrooms bin_1  bedrooms bin_2  bedrooms bin_3  bedrooms bin_4  \\\n",
       "0        0.002693        0.001076        0.000561        0.000586   \n",
       "1        0.002339        0.000968        0.000624        0.000588   \n",
       "2        0.001956        0.002113        0.000633        0.000603   \n",
       "3        0.001713        0.000855        0.000715        0.000598   \n",
       "4        0.002132        0.001054        0.000618        0.000561   \n",
       "5        0.002747        0.001143        0.000638        0.000557   \n",
       "6        0.002015        0.000782        0.000611        0.000584   \n",
       "7        0.002047        0.000957        0.000612        0.000520   \n",
       "8        0.002507        0.000856        0.000659        0.000584   \n",
       "9        0.002485        0.000872        0.000580        0.000596   \n",
       "\n",
       "   bedrooms bin_5  ...  grade bin_7  grade bin_8  grade bin_9  view bin_0  \\\n",
       "0        0.000697  ...     0.022487     0.006256     0.013003    0.060652   \n",
       "1        0.000719  ...     0.022598     0.007218     0.016322    0.058324   \n",
       "2        0.000739  ...     0.024238     0.008399     0.013070    0.058928   \n",
       "3        0.000691  ...     0.022546     0.006555     0.014036    0.057902   \n",
       "4        0.000689  ...     0.020230     0.007485     0.014605    0.057169   \n",
       "5        0.000831  ...     0.020762     0.006552     0.016041    0.058184   \n",
       "6        0.000778  ...     0.023022     0.006758     0.015064    0.054873   \n",
       "7        0.000716  ...     0.018709     0.007051     0.014143    0.058643   \n",
       "8        0.000771  ...     0.022611     0.006015     0.014564    0.061048   \n",
       "9        0.000745  ...     0.023153     0.006775     0.014104    0.059083   \n",
       "\n",
       "   view bin_1 to 2  view bin_3  view bin_4  has_basement  is_renovated  \\\n",
       "0         0.002193    0.001945    0.040625      0.000925      0.002775   \n",
       "1         0.002018    0.001952    0.040263      0.001016      0.002407   \n",
       "2         0.002198    0.001781    0.040202      0.001008      0.002799   \n",
       "3         0.002548    0.001795    0.036853      0.000926      0.002771   \n",
       "4         0.002286    0.002042    0.041905      0.001080      0.002333   \n",
       "5         0.002202    0.001794    0.040732      0.001009      0.002633   \n",
       "6         0.002282    0.002140    0.038709      0.001047      0.002795   \n",
       "7         0.002189    0.001756    0.043581      0.000999      0.002641   \n",
       "8         0.002559    0.001465    0.041579      0.000993      0.002685   \n",
       "9         0.002260    0.001416    0.042923      0.000945      0.002946   \n",
       "\n",
       "   waterfront  \n",
       "0    0.068775  \n",
       "1    0.058316  \n",
       "2    0.066886  \n",
       "3    0.067337  \n",
       "4    0.056202  \n",
       "5    0.059218  \n",
       "6    0.064894  \n",
       "7    0.060859  \n",
       "8    0.064507  \n",
       "9    0.065736  \n",
       "\n",
       "[10 rows x 62 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances = pd.DataFrame(columns=features)\n",
    "\n",
    "for regressor in cv_results['estimator']:\n",
    "    feature_importances = feature_importances\\\n",
    "                          .append(dict(zip(features, regressor.regressor_['xgb_regressor'].feature_importances_)),\n",
    "                                  ignore_index=True)\n",
    "    \n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "zipcode cat_0                         0.171817\n",
       "zipcode cat_7                         0.063300\n",
       "waterfront                            0.063273\n",
       "view bin_0                            0.058480\n",
       "floors bin_1.0                        0.055180\n",
       "                                        ...   \n",
       "bathrooms_ratio bin_0.615 to 0.795    0.000631\n",
       "bedrooms bin_3                        0.000625\n",
       "bedrooms bin_4                        0.000578\n",
       "bathrooms bin_0.5                     0.000000\n",
       "bathrooms bin_7.75 to 8               0.000000\n",
       "Length: 62, dtype: float64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances.mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>mean importance</th>\n",
       "      <th>cumsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>zipcode cat_0</td>\n",
       "      <td>0.171817</td>\n",
       "      <td>0.171817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>zipcode cat_7</td>\n",
       "      <td>0.063300</td>\n",
       "      <td>0.235116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>waterfront</td>\n",
       "      <td>0.063273</td>\n",
       "      <td>0.298389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>view bin_0</td>\n",
       "      <td>0.058480</td>\n",
       "      <td>0.356870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>floors bin_1.0</td>\n",
       "      <td>0.055180</td>\n",
       "      <td>0.412049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>bathrooms_ratio bin_0.615 to 0.795</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.998797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>bedrooms bin_3</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>0.999422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>bedrooms bin_4</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>bathrooms bin_0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bathrooms bin_7.75 to 8</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>62 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               feature  mean importance    cumsum\n",
       "35                       zipcode cat_0         0.171817  0.171817\n",
       "42                       zipcode cat_7         0.063300  0.235116\n",
       "61                          waterfront         0.063273  0.298389\n",
       "55                          view bin_0         0.058480  0.356870\n",
       "25                      floors bin_1.0         0.055180  0.412049\n",
       "..                                 ...              ...       ...\n",
       "21  bathrooms_ratio bin_0.615 to 0.795         0.000631  0.998797\n",
       "7                       bedrooms bin_3         0.000625  0.999422\n",
       "8                       bedrooms bin_4         0.000578  1.000000\n",
       "11                   bathrooms bin_0.5         0.000000  1.000000\n",
       "18             bathrooms bin_7.75 to 8         0.000000  1.000000\n",
       "\n",
       "[62 rows x 3 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_feature_importances = feature_importances.mean().reset_index()\n",
    "mean_feature_importances.columns = ['feature', 'mean importance']\n",
    "mean_feature_importances['cumsum'] = mean_feature_importances['mean importance'].sort_values(ascending=False)\\\n",
    "                                    .cumsum()\n",
    "\n",
    "mean_feature_importances.sort_values('mean importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = mean_feature_importances.sort_values('mean importance', ascending=False).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1 features\n",
      " 2 features\n",
      " 3 features\n",
      " 4 features\n",
      " 5 features\n",
      " 6 features\n",
      " 7 features\n",
      " 8 features\n",
      " 9 features\n",
      "10 features\n",
      "11 features\n",
      "12 features\n",
      "13 features\n",
      "14 features\n",
      "15 features\n",
      "16 features\n",
      "17 features\n",
      "18 features\n",
      "19 features\n",
      "20 features\n",
      "21 features\n",
      "22 features\n",
      "23 features\n",
      "24 features\n",
      "25 features\n",
      "26 features\n",
      "27 features\n",
      "28 features\n",
      "29 features\n",
      "30 features\n",
      "31 features\n",
      "32 features\n",
      "33 features\n",
      "34 features\n",
      "35 features\n",
      "36 features\n",
      "37 features\n",
      "38 features\n",
      "39 features\n",
      "40 features\n",
      "41 features\n",
      "42 features\n",
      "43 features\n",
      "44 features\n",
      "45 features\n",
      "46 features\n",
      "47 features\n",
      "48 features\n",
      "49 features\n",
      "50 features\n",
      "51 features\n",
      "52 features\n",
      "53 features\n",
      "54 features\n",
      "55 features\n",
      "56 features\n",
      "57 features\n",
      "58 features\n",
      "59 features\n",
      "60 features\n",
      "61 features\n",
      "62 features\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for idx in range(1, indices.size + 1):\n",
    "    print('{:2d} features'.format(idx))\n",
    "\n",
    "    regressor = TransformedTargetRegressor(make_pipeline(\n",
    "        xgb_regressor.regressor['column_transformer'],\n",
    "        ColumnTransformer([('selector', 'passthrough', indices[:idx])]),\n",
    "        xgb_regressor.regressor['xgb_regressor']\n",
    "    ), func=np.log, inverse_func=np.exp)\n",
    "\n",
    "    cv_results = cross_validate(regressor, houses_train, y_train, scoring=scorer, cv=10, n_jobs=-1)\n",
    "    scores.append(-cv_results['test_score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAEt8AAATtCAYAAAA9nvmTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAuIwAALiMBeKU/dgAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOzcaXCd130e8OfFQoIEd0IkRQkkQbGyLMCmSG0U6SR2bcV2UjtOGruNpdgMLU0SJ9O0ajOTppNlppmmyTTTJG2TaexQlK3ItpyqtlzPpGMrdhqRkiiK2gCJshaAq0SKBLgvAIG3H0TKDCNYhAncC+L+fjP4cM57zvt/7pd78ekpyrIMAAAAAAAAAAAAAAAAAAAAAAAA1IK6agcAAAAAAAAAAAAAAAAAAAAAAACASlG+BQAAAAAAAAAAAAAAAAAAAAAAQM1QvgUAAAAAAAAAAAAAAAAAAAAAAEDNUL4FAAAAAAAAAAAAAAAAAAAAAABAzVC+BQAAAAAAAAAAAAAAAAAAAAAAQM1QvgUAAAAAAAAAAAAAAAAAAAAAAEDNUL4FAAAAAAAAAAAAAAAAAAAAAABAzVC+BQAAAAAAAAAAAAAAAAAAAAAAQM1QvgUAAAAAAAAAAAAAAAAAAAAAAEDNUL4FAAAAAAAAAAAAAAAAAAAAAABAzVC+BQAAAAAAAAAAAAAAAAAAAAAAQM1QvgUAAAAAAAAAAAAAAAAAAAAAAEDNUL4FAAAAAAAAAAAAAAAAAAAAAABAzVC+BQAAAAAAAAAAAAAAAAAAAAAAQM1QvgUAAAAAAAAAAAAAAAAAAAAAAEDNUL4FAAAAAAAAAAAAAAAAAAAAAABAzVC+BQAAAAAAAAAAAAAAAAAAAAAAQM1QvgUAAAAAAAAAAAAAAAAAAAAAAEDNUL4FAAAAAAAAAAAAAAAAAAAAAABAzVC+BQAAAAAAAAAAAAAAAAAAAAAAQM1QvgUAAAAAAAAAAAAAAAAAAAAAAEDNUL4FAAAAAAAAAAAAAAAAAAAAAABAzVC+BQAAAAAAAAAAAAAAAAAAAAAAQM1QvgUAAAAAAAAAAAAAAAAAAAAAAEDNUL4FAAAAAAAAAAAAAAAAAAAAAABAzVC+BQAAAAAAAAAAAAAAAAAAAAAAQM1oqHYAJqaiKGYm+bFztnYm6a9SHAAAAAAAAAAAAAAAAAAAAAAAYHyZlKT1nPXflWV5qBKDlW8xVn4syderHQIAAAAAAAAAAAAAAAAAAAAAALgk/FSSBysxqK4SQwAAAAAAAAAAAAAAAAAAAAAAAGA8UL4FAAAAAAAAAAAAAAAAAAAAAABAzWiodgAmrJ3nLr72ta9l2bJl1coCAAAAAAAAAAAAAAAAAAAAAACMIy+99FI+9rGPnbu1c7izo035FmOl/9zFsmXL0t7eXq0sAAAAAAAAAAAAAAAAAAAAAADA+Nb/9kdGR12lBgEAAAAAAAAAAAAAAAAAAAAAAEC1Kd8CAAAAAAAAAAAAAAAAAAAAAACgZijfAgAAAAAAAAAAAAAAAAAAAAAAoGYo3wIAAAAAAAAAAAAAAAAAAAAAAKBmKN8CAAAAAAAAAAAAAAAAAAAAAACgZijfAgAAAAAAAAAAAAAAAAAAAAAAoGYo3wIAAAAAAAAAAAAAAAAAAAAAAKBmKN8CAAAAAAAAAAAAAAAAAAAAAACgZijfAgAAAAAAAAAAAAAAAAAAAAAAoGYo3wIAAAAAAAAAAAAAAAAAAAAAAKBmKN8CAAAAAAAAAAAAAAAAAAAAAACgZijfAgAAAAAAAAAAAAAAAAAAAAAAoGYo3wIAAAAAAAAAAAAAAAAAAAAAAKBmKN8CAAAAAAAAAAAAAAAAAAAAAACgZijfAgAAAAAAAAAAAAAAAAAAAAAAoGYo3wIAAAAAAAAAAAAAAAAAAAAAAKBmKN8CAAAAAAAAAAAAAAAAAAAAAACgZijfAgAAAAAAAAAAAAAAAAAAAAAAoGYo3wIAAAAAAAAAAAAAAAAAAAAAAKBmKN8CAAAAAAAAAAAAAAAAAAAAAACgZijfAgAAAAAAAAAAAAAAAAAAAAAAoGYo3wIAAAAAAAAAAAAAAAAAAAAAAKBmKN8CAAAAAAAAAAAAAAAAAAAAAACgZijfAgAAAAAAAAAAAAAAAAAAAAAAoGYo3wIAAAAAAAAAAAAAAAAAAAAAAKBmKN8CAAAAAAAAAAAAAAAAAAAAAACgZijfAgAAAAAAAAAAAAAAAAAAAAAAoGYo3wIAAAAAAAAAAAAAAAAAAAAAAKBmKN8CAAAAAAAAAAAAAAAAAAAAAACgZijfAgAAAAAAAAAAAAAAAAAAAAAAoGYo3wIAAAAAAAAAAAAAAAAAAAAAAKBmKN8CAAAAAAAAAAAAAAAAAAAAAACgZijfAgAAAAAAAAAAAAAAAAAAAAAAoGYo3wIAAAAAAAAAAAAAAAAAAAAAAKBmKN8CAAAAAAAAAAAAAAAAAAAAAACgZijfAgAAAAAAAAAAAAAAAAAAAAAAoGYo3wIAAAAAAAAAAAAAAAAAAAAAAKBmKN8CAAAAAAAAAAAAAAAAAAAAAACgZijfAgAAAAAAAAAAAAAAAAAAAAAAoGYo3wIAAAAAAAAAAAAAAAAAAAAAAKBmKN8CAAAAAAAAAAAAAAAAAAAAAACgZijfAgAAAAAAAAAAAAAAAAAAAAAAoGYo3wIAAAAAAAAAAAAAAAAAAAAAAKBmKN8CGMf2Hj6ZkwOD1Y4BAAAAAAAAAAAAAAAAAAAAADBhNFQ7AADD+62vdeaJ7X257eZFuX3V4syb0VTtSAAAAAAAAAAAAAAAAAAAAAAAl7S6agcA4K3tOHA833p+bw4c68+f/u1LWfMHf5u7vvJUOncfqnY0AAAAAAAAAAAAAAAAAAAAAIBLVkO1AwDw1jZs6klZfn89MFjmgSd354End+emtjlZt6Ytt147P/V1RfVCAgAAAAAAAAAAAAAAAAAAAABcYpRvAYxDR04O5P4tO4d9vrm7N5u7e3Pl7ClZu3pJPnFja2Y0NVYwIQAAAAAAAAAAAAAAAAAAAADApamu2gEA+Me+umVXjp46/bbndvWdyO998/nc8p8eyu8+2JWe/ccqkA4AAAAAAAAAAAAAAAAAAAAA4NKlfAtgHDp5ejDNk+ov+Pyx/sFs2NST9/3Rd3PHPVuy6eX9KctyDBMCAAAAAAAAAAAAAAAAAAAAAFyaGqodAIB/7LPvXZbbVy3O/Y/vzIZNPdnVd+KC7pVl8u3n9+bbz+/NNQumZ9172vLR5QvT1HjhRV4AAAAAAAAAAAAAAAAAAAAAABNZUZZltTMwARVF0Z6k8+y6s7Mz7e3tVUwEl67BoTLfem5v1m/szubu3hHfn9s8KbetWpzbVy3KvOlNY5AQAAAAAAAAAAAAAAAAAAAAAGBkurq60tHRce5WR1mWXZWYrXyLMaF8C8ZG5+5DWb+xO994ek8GBkf2/d1YX+Qjyxdm3Zq2dFwxc4wSAgAAAAAAAAAAAAAAAAAAAAC8PeVbTDjKt2Bs7Tt8Mvc+tiN/9ej2HDjWP+L7N7XNybo1bbn12vmpryvGICEAAAAAAAAAAAAAAAAAAAAAwPCqWb7VUIkhAIyueTOactetV+ez770qDz61J+s3dmfba0cu+P7m7t5s7u7NlbOnZO3qJfnEja2Z0dQ4hokBAAAAAAAAAAAAAAAAAAAAAMaHoizLamdgAiqKoj1J59l1Z2dn2tvbq5gIJrayLPPIyweyfmN3Htq2LyP9am+eVJ+P39CatauXZElL89iEBAAAAAAAAAAAAAAAAAAAAAA4o6urKx0dHedudZRl2VWJ2Q2VGALA2CqKIquXtWT1spZ07z+Wezb15P4tO3O8f/CC7h/rH8yGTT2555GevP+a+Vn3niW5ZencFEUxtsEBAAAAAAAAAAAAAAAAAAAAACqsKMuy2hmYgIqiaE/SeXbd2dmZ9vb2KiaC2nPoxEC+umVnNmzqya6+EyO+f82C6Vn3nrZ8dPnCNDXWj0FCAAAAAAAAAAAAAAAAAAAAAKBWdXV1paOj49ytjrIsuyoxW/kWY0L5Fowfg0NlvvXc3qzf2J3N3b0jvj+3eVJuW7U4t69alHnTm8YgIQAAAAAAAAAAAAAAAAAAAABQa5RvMeEo34LxqXP3oazf2J1vPL0nA4Mj+/5vrC/ykeULs25NWzqumDlGCQEAAAAAAAAAAAAAAAAAAACAWqB8iwlH+RaMb/sOn8y9j+3IXz26PQeO9Y/4/k1tc7JuTVtuvXZ+6uuKMUgIAAAAAAAAAAAAAAAAAAAAAExk1SzfaqjEEADGl3kzmnLXrVfns++9Kg8+tSfrN3Zn22tHLvj+5u7ebO7uzZWzp2Tt6iX5xI2tmdHUOIaJAQAAAAAAAAAAAAAAAAAAAABGR1GWZbUzMAEVRdGepPPsurOzM+3t7VVMBPwgZVnmkZcPZP3G7jy0bV9G+tPQPKk+H7+hNWtXL8mSluaxCQkAAAAAAAAAAAAAAAAAAAAATBhdXV3p6Og4d6ujLMuuSsxuqMQQAMa3oiiyellLVi9rSff+Y7lnU0/u37Izx/sHL+j+sf7BbNjUk3se6cn7r5mfde9ZkluWzk1RFGMbHAAAAAAAAAAAAAAAAAAAAABghIqyLKudgQmoKIr2JJ1n152dnWlvb69iImCkDp0YyFe37MzdG3uy++CJEd+/ZsH0rHtPWz66fGGaGuvHICEAAAAAAAAAAAAAAAAAAAAAcKnq6upKR0fHuVsdZVl2VWK28i3GhPItmDhODw7l28/vzfqHe7K5p3fE9+c2T8ptqxbn9lWLMm960xgkBAAAAAAAAAAAAAAAAAAAAAAuNcq3mHCUb8HE9OyuQ7l7Y3e+8cyeDAyO7Pejsb7IR5YvzLo1bem4YuYYJQQAAAAAAAAAAAAAAAAAAAAALgXKt5hwlG/BxLbv8Mnc++j23PvYjvQe6x/x/Zva5mTdmrbceu381NcVY5AQAAAAAAAAAAAAAAAAAAAAABjPqlm+1VCJIQBMLPNmNOWuH39HPvu+ZXnwqT1Zv7E72147csH3N3f3ZnN3b1rnTMmnb1mST9zYmhlNjWOYGAAAAAAAAAAAAAAAAAAAAADgDUVZltXOwARUFEV7ks6z687OzrS3t1cxETCWyrLMIy8fyPqN3Xlo276M9Kdl2uSGfPyGK7N29ZIsnts8NiEBAAAAAAAAAAAAAAAAAAAAgHGjq6srHR0d5251lGXZVYnZDZUYAsDEVhRFVi9ryeplLenefyz3bOrJ/Vt25nj/4AXdP3rqdO7e2JMNm3rygXfOz7o1bVm1dE6Kohjj5AAAAAAAAAAAAAAAAAAAAABArSnKsqx2Biagoijak3SeXXd2dqa9vb2KiYBKO3RiIF/dsjN3b+zJ7oMnRnz/nZfPyLo1S/KR5QvT1Fg/BgkBAAAAAAAAAAAAAAAAAAAAgGrp6upKR0fHuVsdZVl2VWK28i3GhPIt4KzTg0P59vN7s/7hnmzu6R3x/ZZpk/LJmxfn9lWLMm960xgkBAAAAAAAAAAAAAAAAAAAAAAqTfkWE47yLeCtPLvrUO7e2J1vPLMnA4Mj+/1prC/ykeULs25NWzqumDlGCQEAAAAAAAAAAAAAAAAAAACASlC+xYSjfAv4QfYdPpl7H92eex/bkd5j/SO+f1PbnKxb05Zbr52f+rpiDBICAAAAAAAAAAAAAAAAAAAAAGOpmuVbDZUYAgDnmjejKXf9+Dvy2fcty4NP7cn6jd3Z9tqRC76/ubs3m7t7s2BGU25eOicrF83OykWzc83l09NYXzeGyQEAAAAAAAAAAAAAAAAAAACAS53yLQCqpqmxPp+4sTUfv+HKPPLygazf2J2Htu1LWV7Y/dcOn8zXn9qTrz+1J0kypbE+77pyZq5fPPtMIdeszJ02eQw/AQAAAAAAAAAAAAAAAAAAAABwqVG+BUDVFUWR1ctasnpZS7r3H8s9m3py/5adOd4/OKL3nBgYzObu3mzu7n1zb/HcqW8Wca1YNDvXLJiehvq60f4IAAAAAAAAAAAAAAAAAAAAAMAloijLstoZmICKomhP0nl23dnZmfb29iomAi41h04M5KtbdubujT3ZffDEqL136qT6vPvKmbl+8eysXDQ7KxbNzpzmSaP2fgAAAAAAAAAAAAAAAAAAAADg7XV1daWjo+PcrY6yLLsqMbuhEkMAYKRmTmnMHT+yNGtXL8m3n9+b9Q/3ZHNP70W/93j/YB59pTePvvL9d7W1NGfFollZueiNQq53LJie+rriomcBAAAAAAAAAAAAAAAAAAAAAOOP8i0AxrWG+rp8qOPyfKjj8jy761Du3tidbzyzJwOD5ajN6N5/LN37j+WBrbuTJM2T6rO89UwZ1+JZWdE6O7ObJ43aPAAAAAAAAAAAAAAAAAAAAACgeoqyHL3yEjirKIr2JJ1n152dnWlvb69iImAi6TvWn8e6e/Pkjr48sb0vz+w+lP7TQ2M6c+llzW+UcZ0p5Pon86anvq4Y05kAAAAAAAAAAAAAAAAAAAAAMFF1dXWlo6Pj3K2Osiy7KjG7oRJDAGA0zW6elA91LMiHOhYkSfpPD+W5Vw9n6/a+bN3Rlyd3HMzugydGdeYrrx/LK68fy18/sStJMm1yQ65rnZWVi2ZlxeLZWdk6OzOnNo7qTAAAAAAAAAAAAAAAAAAAAABg9CnfAuCSN6mhLte1zsp1rbOyLm1Jkr2HT75ZxvXE9r507j6c/sGhUZt59NTpPPzS/jz80v4395bNm5aVi2Zl5aLZWbl4dpZdNi11dcWozQQAAAAAAAAAAAAAAAAAAAAALp7yLQAmpPkzmvLhd12eD7/r8iTJqdOD6dpzOFu39+XJHQezdUdfXj10clRnvrTvaF7adzT3b9mVJJne1JDrWr9fxnVd66zMnNI4qjMBAAAAAAAAAAAAAAAAAAAAgJFRvgVATZjcUP9GCdai2W/uvXroRLZuf6OIa+uOvnTtPpz+waFRm3nk5On8/Yv78/cv7k+SFEWy7LJpuX7x7DOFXLOytGVa6uqKUZsJAAAAAAAAAAAAAAAAAAAAAPxgyrcAqFmXz5ySn3z3lPzkuy9PkpwcGEzXnkP/oJBr7+FTozavLJMX9x3Ni/uO5suP70ySzGhqyIpF3y/juq51VqY3NY7aTAAAAAAAAAAAAAAAAAAAAADgH1K+BQBnNDXW5/rFc3L94jlJkrIss+fQyWzd3nemjOtgnttzKAOD5ajNPHzydP7ue6/n7773epKkKJKr503PysWzs3LRrKxcPDtLW5pTFMWozQQAAAAAAAAAAAAAAAAAAACAWqZ8CwCGURRFrpg1JVfMmpKPLF+YJDk5MJjO3YfyxDmFXK8fOTVqM8syeWHvkbyw90i+tHlHkmTW1MasaJ2VlYtmZ+Xi2VneOivTJvsJBwAAAAAAAAAAAAAAAAAAAIAfhuYOABiBpsb63LBkTm5YMidJUpZldvWdyNYdfXlyx8Fs3dGX5/YczumhctRmHjw+kO+88Hq+88LrSZK6Irl6/vSsXDz7jUKuRbPS1tKcoihGbSYAAAAAAAAAAAAAAAAAAAAATFTKtwDgIhRFkdY5U9M6Z2p+6rorkiQn+gfz7O5D2bqjL09s78uTO/qy/2j/qM0cKpNtrx3JtteO5L7HdiRJ5jRPyorWWbmxbU5+esUVmT+jadTmAQAAAAAAAAAAAAAAAAAAAMBEonwLAEbZlEn1ualtTm5qm5MkKcsyO3tPZOuOvjf/nn/1SAaHylGb2XusPw9t25eHtu3Ln33npfzFp27IqqVzR+39AAAAAAAAAAAAAAAAAAAAADBRKN8CgDFWFEUWzZ2aRXOn5mMrrkiSHO8/nWd2HXqjjGv7wWzd0ZfeY/2jMu/wydNZt+HxfPEzN+X6xXNG5Z0AAAAAAAAAAAAAAAAAAAAAMFEo3wKAKpg6qSGrls7NqqVzkyRlWWb7geNvlHGdKeTa9trhDJU/3PuP9w9m7frHc+8dN2d566xRTA4AAAAAAAAAAAAAAAAAAAAAlzblWwAwDhRFkSUtzVnS0pyfWXllkuTYqdN5etfBPLnjYLZuf6OUq+/4wAW/88ip0/nU+s25786b075w5lhFBwAAAAAAAAAAAAAAAAAAAIBLivItABinmic3ZPVVLVl9VUuSpCzL9Bw4nifOFHFt3d6X7+09kqFy+HccOjGQn//LzfnSnavyjgXTK5QcAAAAAAAAAAAAAAAAAAAAAMYv5VsAcIkoiiJtLc1pa2nOz15/ZZLk6KnTeXrnwXzxke35m67X3vJe77H+3Pb5x/KVX1yVqy6bVsnIAAAAAAAAAAAAAAAAAAAAADDu1FU7AADww5s2uSFrlrXkv39yRX7iXQuGPbf/6Kl88nOPZvuBYxVMBwAAAAAAAAAAAAAAAAAAAADjj/ItAJgAGurr8if/ckU+8M75w57Ze/hUPvm5x7Kr73gFkwEAAAAAAAAAAAAAAAAAAADA+KJ8CwAmiMb6uvyP21bkx66+bNgzuw+eyCc/91hePXSigskAAAAAAAAAAAAAAAAAAAAAYPxQvgUAE8jkhvr8z5+/PquvmjvsmR29x3Pb5x7LviMnK5gMAAAAAAAAAAAAAAAAAAAAAMYH5VsAMME0Ndbn85++ITctmTPsmVf2H8ttn3ssB46eqmAyAAAAAAAAAAAAAAAAAAAAAKg+5VsAMAFNndSQ9b9wY1YsmjXsmRf3Hc3tf7k5B4/3VzAZAAAAAAAAAAAAAAAAAAAAAFSX8i0AmKCmTW7Ihl+4KR1XzBj2zPOvHs6n1m/O4ZMDFUwGAAAAAAAAAAAAAAAAAAAAANWjfAsAJrCZUxrzxXU355oF04c988yuQ1m7fnOOnjpdwWQAAAAAAAAAAAAAAAAAAAAAUB3KtwBggpvdPCn33nFzls2bNuyZrTsOZt2Gx3O8XwEXAAAAAAAAAAAAAAAAAAAAABOb8i0AqAEt0ybnvjtuTltL87BnNnf35s4vbMnJgcEKJgMAAAAAAAAAAAAAAAAAAACAylK+BQA1Yt6Mptx3581pnTNl2DMbXzqQX773iZw6rYALAAAAAAAAAAAAAAAAAAAAgIlJ+RYA1JDLZ07JfXesysKZTcOe+c4Lr+dX73syA4NDFUwGAAAAAAAAAAAAAAAAAAAAAJWhfAsAakzrnKm5785VmTd98rBnvvXc3vzrLz+V0wq4AAAAAAAAAAAAAAAAAAAAAJhglG8BQA1a0tKc++5clZZpk4Y9881nX82v//UzGRwqK5gMAAAAAAAAAAAAAAAAAAAAAMaW8i0AqFHL5k3LX92xKrOnNg575n8/uTu/+cCzGVLABQAAAAAAAAAAAAAAAAAAAMAEoXwLAGrYOxZMzxc/c3NmNDUMe+YrW3bmtx/sTFkq4AIAAAAAAAAAAAAAAAAAAADg0qd8CwBqXMcVM/OFz9ycaZOHL+C699Ed+Y//53kFXAAAAAAAAAAAAAAAAAAAAABc8pRvAQC5rnVWNvzCjZk6qX7YM+s3ducP/+8LCrgAAAAAAAAAAAAAAAAAAAAAuKQp3wIAkiQ3LJmTz3/6hkxuGP7fgz//7sv5k4derGAqAAAAAAAAAAAAAAAAAAAAABhdyrcAgDetvqoln/vUDZlUP/y/CH/87RfzZ999qYKpAAAAAAAAAAAAAAAAAAAAAGD0NFQ7wIUoiqItyXVJFiaZluTVJNuTbCrLcqAKeeYkuSZJa5L5SZrPPDqUZG+SJ8uyfKXSuQBgNPzo1Zflz29fmV+694kMDJZveeYP/+aFTKqvyx0/srTC6QAAAAAAAAAAAAAAAAAAAADg4ozr8q2iKH42yV1JbhnmSG9RFF9J8ttlWe4fwxzTkvzqmRw3Jrn8Au7sSvKFJH9aluXeEcx6b5Lv/HBJkyTby7JcchH3ASDvf+f8/LefW5Ffue/JDA69dQHX733z+UxuqMvP37KksuEAAAAAAAAAAAAAAAAAAAAA4CLUVTvAWymKYlpRFF9K8tUMX7yVJHOS/HKSzqIoPjiGkRYk+f0kH80FFG+dcWWS30zyQlEUa8coFwCMmQ91XJ7/+i+uS10x/Jnf+npXvvL4jsqFAgAAAAAAAAAAAAAAAAAAAICL1FDtAOcriqI+yVeS/MR5j15P8mSSQ0muSrIiydk6kPlJvl4UxQfKsny4QlF7k7yY5LUkR5NMzhslXcuTTD/n3MwkdxdFMbcsyz+qUDYAGBUfXb4wA6eH8u/++umU5Vuf+Y0Hns2khrr89IorKxsOAAAAAAAAAAAAAAAAAAAAAH4I4658K8l/zj8s3hpIcleSvyjLsv/sZlEU1yb5fJJbzmxNTvK1oijeVZblq2OQa1+Sbyb5VpJNZVluf6tDRVE0Jvlokj/IGyVhZ/1hURQPl2X52Ajn/kmSPx7B+dMjfD8A/ED//Por0z84lH//wLNv+bwsk397/9NprK/LP3v3wgqnAwAAAAAAAAAAAAAAAAAAAICRGVflW0VRLE3ya+dtf7wsy6+ff7Ysy+eKonh/kofy/QKuuUl+J8kvjXK07iSXl2U59HYHy7IcSPK/iqL42yT/L0nHmUd1SX43yYdHOPtgWZY9I7wDAKPq525alP7TQ/mdB7ve8vlQmfzal59KY31dPti+oMLpAAAAAAAAAAAAAAAAAAAAAODC1VU7wHl+J0njOesNb1W8dVZZlieSrE3Sf872Z86UeI2asiwHL6R467w7ffnHRWIfKIpi+uglA4DK+fTqJfkPP/HOYZ8PDpX51fu25jvb9lUwFQAAAAAAAAAAAAAAAAAAAACMzLgp3yqKYkqSnz1v+w/e7l5Zlt9L8rVzthqSfHIUo12M7yY5cc66Icni6kQBgIt3548uza9/8B3DPh8YLPOL9z6Rh1/cX8FUAAAAAAAAAAAAAAAAAAAAAHDhxk35VpIPJpl6zvqRsiy3XeDdu89b/8zoRLo4ZVkOJTl43vb0amQBgNHyK+9bln/1T5cN+7z/9FDu+MLjefSVAxVMBQAAAAAAAAAAAAAAAAAAAAAXZjyVb33ovPV3R3D375OcPme9oiiK+Red6CIVRTE1yWXnbe+pRhYAGE3/5tar84s/unTY5ycHhq0gcDIAACAASURBVLJuw+N5YntvBVMBAAAAAAAAAAAAAAAAAAAAwNsbT+VbHeetH7nQi2VZHkvy7Hnb7Red6OL9XJKGc9bdZVlur1YYABgtRVHkNz58TdauXjLsmeP9g1m7/vE8vfNg5YIBAAAAAAAAAAAAAAAAAAAAwNsYT+Vb7zxv/dII77983vrai8hy0YqiWJPkv5y3ff76QryvKIoHiqJ4pSj+Pzt3GuZXXZ8N/P7NTPaEQEjClkBYDBD2JMhiXWotFX0QF0ANS5BF6vKotbW1ta3a0sdaL23doJZFWWWzKJaKl9ZdCiRhh4BAWBIghBDInkxm5jwvTG0c/39IwszJLJ/PdZ0X53y/5/zumTczr+6yqpSytpTyZCllXinlK6WUd5RShvRAZADYIqWUfPK4aZl1xO5Nd1au78hpF9+W+55aXmMyAAAAAAAAAAAAAAAAAAAAAGiuT5RvlVLGJRnX7fETW/iZ7vuv2PpEW66UMqyUMqmUclwp5fIkP0uy/SYr301y/lZ8+jVJ3pZkzySjkgxPsmuS6Uk+kOS6JAtKKR8opZSX8zMAwJYqpeTc4w/MCTMmNd1ZvnZDTr3otjy4eGWNyQAAAAAAAAAAAAAAAAAAAACgsT5RvpXfLqlKkjVVVa3ewm8s6XY/9mXkeUmllDtLKdX/XEnWJVmY5IYkJ+d/f7dVkq8mOaGqqqqX4kxK8pUk3y2ldP9dAkCvamkp+ew7Ds7xh+7adGfZ6vacfOGteeTZVTUmAwAAAAAAAAAAAAAAAAAAAIDf1batA2w0utv92q34Rvd3xmxllp7SnuSCJOdVVXX/Vry/IskPk/w0yX35dbnY2iQ7JJma5A+TvDPJ8E3eeXOSb5dSjqmqqv1lZP8tpZSJSSZs4Wt799T5APR9rS0lnz/xkLR3dOV79y5uuLN01frMuuCWXHPOUdljx1E1JwQAAAAAAAAAAAAAAAAAAACAX+ur5VvrtuIb3cu3un+zbkOTnJJkaCnls1VVPbKZ7y1O8p4kV1VV1ez3MCfJFaWUjye5OMmxm8xem+Qfk3x062I39P4kn+zB7wEwALW1tuSL7zosG66Ylx/OX9Jw55kV6zPrgltz9TlHZtIOI2tOCAAAAAAAAAAAAAAAAAAAAABJy7YO0ERV0zsvx5uS7LnJdXCSY5L8dZL5G3fGJjk7yd2llNM356NVVT1QVdU3XqR4a9PdxUnenOTabqMPlFL23JzzAKAnDW1ryVdPnp7XTp3QdOfJF9Zm1gW3ZvHyrenaBAAAAAAAAAAAAAAAAAAAAICXp6+Ub63qdj9iK77R/Z3u3+xRVVU9VVXVY5tc91RV9YOqqv6hqqppSd6b5H9aRUYmubiUclov5KiSnJ7k6U0eD01yZk+fBQCbY1hba7526owcvfeOTXeeWLYmsy64JUtWKuACAAAAAAAAAAAAAAAAAAAAoF7Kt3pJVVUXJHnnJo9KkvNKKbv1wllrknyp2+M39uAR5yU5cAuv43vwfAD6meFDWnPh7Jk5fMoOTXcWLF2dky+4Nc+tWl9jMgAAAAAAAAAAAAAAAAAAAAAGu75SvrW82/3IUsqoLfzGxG73L7yMPD2iqqobkly/yaNRSd7fS8fd1O3+oJ76cFVVS6qqum9LriSP9NT5APRPI4e25eLTD8+hk7dvuvPQklU55aLb8sKa9hqTAQAAAAAAAAAAAAAAAAAAADCY9YnyraqqnkvyfLfHu2/hZ/bodv/Q1ifqUd/sdv/GXjrnsW73Q0spY3vpLADYLGOGD8klZ7wyB+62XdOd+U+vyGkX35YV6zbUmAwAAAAAAAAAAAAAAAAAAACAwapPlG9tNL/b/T5b+P5eL/G9beXBbvdb+nNtrrUNno3opbMAYLONHTEkl51xRPbbeUzTnbsXLc/pF9+WVes7akwGAAAAAAAAAAAAAAAAAAAAwGDUl8q37u12f9TmvlhKGZXk4Jf43rayodv9sF46Z3yDZ8/10lkAsEV2GDU0l591RPaZOLrpzu1PvJAzvjEna9s7a0wGAAAAAAAAAAAAAAAAAAAAwGDTl8q3bup2/7otePfVSdo2ub+jqqpnXnainjGp231v5Tqi2/2zVVV1L/4CgG1m/OhhufKsIzJlx5FNd257dFnOvnRu1m1QwAUAAAAAAAAAAAAAAAAAAABA7+hL5VvfT7J2k/ujSin7bea7p3e7v75HEvWMY7rdP9RL58zqdv+TXjoHALbaxO2G58qzj8zkcSOa7vzi4aV53+Xzsr5DARcAAAAAAAAAAAAAAAAAAAAAPa/PlG9VVbUmyXXdHv/FS71XSpma5G2bPOpIcmUPRttqpZRdkry32+Pv9MI5r0vy9t4+BwB6wq7bj8iVZx2ZXccOb7rz4wefzQevvCMbOrtqTAYAAAAAAAAAAAAAAAAAAADAYNBnyrc2+lSSDZvcn15KeUuz5VLK8CRfTzJ0k8cXVVX1yIsdUkqpul2ve5HdUaWUj5ZSRmzWT/C/701IcmOS7TZ5vCzJN1/knWNKKYds4TlHJPlWkrLJ4weTXL0l3wGAOk0eNzJXnn1kJo4Z1nTnB/c/k49cdWc6FHABAAAAAAAAAAAAAAAAAAAA0IP6VPlWVVULknyx2+PrSikfLKVsWrCVUsr+Sf4rydGbPH4uyad7ONaQJJ9PsqCU8oVSylHds3TLtVMp5U+TzE9yWLfxx6qqWvoiZx2d5I5Syk2llNNLKRNf5JzJpZTPJfl5knGbjDYkeX9VVR0v8XMBwDY1ZfyoXHn2kRk/uumf1dx4z9P52HV3p7OrqjEZAAAAAAAAAAAAAAAAAAAAAANZ27YO0MDHkxyQ5NiN90OSfDnJ35RSbk+yMsleSaYnKZu8157kbVVVPd1LuXZO8icbr/ZSyv1Jnk7ywsYcY5NM3ZitNHj/E1VVXbwZ55Qkf7TxSinlySQPbjxn7SbnTG3wbmeSM6qq+tHm/1gAsO3sM3F0Lj/riLz7327J82s2NNy5/o4nM7S1JZ95+0FpaWn0JxYAAAAAAAAAAAAAAAAAAAAANl+fK9+qqqqzlHJSkguTvHOT0cQkb2zy2pIks6uq+nlv59toaJJDN14vZVGSD1VVdf1WnrXbxuulLEhyWlVVv9zKcwBgm9hv5+1y2ZlHZNYFt2TFuo6GO1fPXZihbS35u+MPSCkKuAAAAAAAAAAAAAAAAAAAAADYei3bOkAjVVWtqqrqXUlOTHLLi6wuS3J+kgOrqrqpl+KsSPKWJOcluT9J12a805Hk50nem2T/LSjeumHjOfck6dzMc25OMjvJNMVbAPRXB+42Npec8cqMHta8F/SyWx7PuTfOT1VVNSYDAAAAAAAAAAAAAAAAAAAAYKBp3nDRB1RVdV2S60opeyaZnmTXJKOSLE7yeJJfVlXVvhXfLVuw25XkuxuvlFLGJJmWZEqSnTfmSX5d0rU8yYNJ7q6qat1W5Lo9ye0bzxm+8Zw9kuySZEySIUlWJXk+yaNJ5lZVtWZLzwGAvuiw3XfI199zeGZffFvWtDfuoLzoF49maFtL/vyP9k0pm/3nHAAAAAAAAAAAAAAAAAAAAAB+o0+Xb/2Pqqoeza/Lpra5qqpWJrl149Wb56zLr4u4bu/NcwCgLzl8yrhcOHtm3vP1OVnf0dVw5/yfPJJhbS35yBum1pwOAAAAAAAAAAAAAAAAAAAAgIGgZVsHAADY1NF7j8+/nTYzQ1ub/5vyLz98KOf95OEaUwEAAAAAAAAAAAAAAAAAAAAwUCjfAgD6nNdOnZDzT5metpbSdOefbnowF/58QY2pAAAAAAAAAAAAAAAAAAAAABgIlG8BAH3SH+y/U74y67C0vkgB17k3zs9l//1YbZkAAAAAAAAAAAAAAAAAAAAA6P+UbwEAfdYbD9wl//zOQ/Mi/Vv5m+/cl6vnPFFfKAAAAAAAAAAAAAAAAAAAAAD6NeVbAECf9pZDds0/nXBIyosUcH383+/J9Xcsqi8UAAAAAAAAAAAAAAAAAAAAAP2W8i0AoM87Ycak/MNbD2o6r6rkT6+5K/9x91M1pgIAAAAAAAAAAAAAAAAAAACgP1K+BQD0C7OO2D2ffssBTeddVfLhq+7M9+9bXGMqAAAAAAAAAAAAAAAAAAAAAPob5VsAQL8x++gp+cSb9m867+yq8sErb8+PH1hSYyoAAAAAAAAAAAAAAAAAAAAA+hPlWwBAv3L2a/bKnx0ztel8Q2eVcy6fl188tLTGVAAAAAAAAAAAAAAAAAAAAAD0F8q3AIB+54Ovf0U+9Pp9ms7bO7py1qVzcuuC52pMBQAAAAAAAAAAAAAAAAAAAEB/oHwLAOiX/uQPp+ac1+zVdL5uQ1fO+MaczHv8+RpTAQAAAAAAAAAAAAAAAAAAANDXKd8CAPqlUko+fux+Of3oKU13Vrd35vSLb8vdi16oLxgAAAAAAAAAAAAAAAAAAAAAfZryLQCg3yql5JPHTcusI3ZvurNyfUdOvei23PfU8hqTAQAAAAAAAAAAAAAAAAAAANBXKd8CAPq1UkrOPf7AnDBjUtOd5Ws35D1fn5PFy9fVmAwAAAAAAAAAAAAAAAAAAACAvkj5FgDQ77W0lHz2HQfnLYfs2nRnycr1OevSOVnT3lFjMgAAAAAAAAAAAAAAAAAAAAD6GuVbAMCA0NpS8oWTDsmxB+7cdOfeJ1fkT6+5K11dVY3JAAAAAAAAAAAAAAAAAAAAAOhLlG8BAANGW2tLvviuw/KG/Sc23fnevYvzzz/8VY2pAAAAAAAAAAAAAAAAAAAAAOhLlG8BAAPK0LaWfGXW9EzfffumO1/+0cP5zp1P1pgKAAAAAAAAAAAAAAAAAAAAgL5C+RYAMOAMH9Kar506M7ttP6Lpzseuuzu3P/F8jakAAAAAAAAAAAAAAAAAAAAA6AuUbwEAA9KEMcNy4eyZGTm0teG8vaMr7710Xp58YW3NyQAAAAAAAAAAAAAAAAAAAADYlpRvAQAD1v67bJcvveuwlNJ4vnTV+px1ydysXt9RbzAAAAAAAAAAAAAAAAAAAAAAthnlWwDAgPaGaTvlL4/dr+l8/tMr8pGr70xXV1VjKgAAAAAAAAAAAAAAAAAAAAC2FeVbAMCAd/ar98qJMyY1nf/g/mfyT99/sMZEAAAAAAAAAAAAAAAAAAAAAGwryrcAgAGvlJJz33ZgXjllXNOdf/3pI7lu3qIaUwEAAAAAAAAAAAAAAAAAAACwLSjfAgAGhWFtrTn/lOmZPG5E052//Pe7M+exZTWmAgAAAAAAAAAAAAAAAAAAAKBuyrcAgEFjx9HDctHswzN6WFvD+YbOKudcNi8Ll62pORkAAAAAAAAAAAAAAAAAAAAAdVG+BQAMKlN3GpMvzzosLaXxfNnq9px1ydysXLeh3mAAAAAAAAAAAAAAAAAAAAAA1EL5FgAw6Pz+vhPziTdPazp/8JmV+fBVd6azq6oxFQAAAAAAAAAAAAAAAAAAAAB1UL4FAAxKZ7xqSt79yslN5z96YEn+8Xvza0wEAAAAAAAAAAAAAAAAAAAAQB2UbwEAg1IpJX93/IE5cq9xTXcu+PmjuXrOEzWmAgAAAAAAAAAAAAAAAAAAAKC3Kd8CAAatIa0tOf/kGZmy48imO5+4/t7csuC5GlMBAAAAAAAAAAAAAAAAAAAA0JuUbwEAg9oOo4bmwtmHZ8zwtobzjq4qf3z5vDz+3OqakwEAAAAAAAAAAAAAAAAAAADQG5RvAQCD3j4TR+e8k6entaU0nL+wZkPO+MacrFi3oeZkAAAAAAAAAAAAAAAAAAAAAPQ05VsAAEle/YoJ+eRx05rOH3l2dT545R3p6OyqMRUAAAAAAAAAAAAAAAAAAAAAPU35FgDARqcdNSWnHrlH0/nPfvVszr1xfo2JAAAAAAAAAAAAAAAAAAAAAOhpyrcAADbxt8dNy+/tM77p/Bs3P5bLb3m8xkQAAAAAAAAAAAAAAAAAAAAA9CTlWwAAmxjS2pKvzpqevcaParrzyRvuy80PL60xFQAAAAAAAAAAAAAAAAAAAAA9RfkWAEA3Y0cOyUWnH56xI4Y0nHd2VXnfFbdnwbOrak4GAAAAAAAAAAAAAAAAAAAAwMulfAsAoIE9x4/K+adMT1tLaThfvnZDzrpkbpav2VBzMgAAAAAAAAAAAAAAAAAAAABeDuVbAABNHL33+Hz6+AOazhcsXZ33XzkvGzq7akwFAAAAAAAAAAAAAAAAAAAAwMuhfAsA4EWcfMQeec+rpjSd//Lh5/KpG+5LVVX1hQIAAAAAAAAAAAAAAAAAAABgqynfAgB4CZ940/557dQJTedX3PpELv3vx2tMBAAAAAAAAAAAAAAAAAAAAMDWUr4FAPAS2lpb8uVZh2WfiaOb7nz6u/flZ796tsZUAAAAAAAAAAAAAAAAAAAAAGwN5VsAAJthu+FDctHsmdlh5JCG864q+cCVt+fhJStrTgYAAAAAAAAAAAAAAAAAAADAllC+BQCwmfbYcVT+9ZQZGdJaGs5XruvImZfMzfOr22tOBgAAAAAAAAAAAAAAAAAAAMDmUr4FALAFjthrx/zDWw9qOn/8uTX548vnpb2jq8ZUAAAAAAAAAAAAAAAAAAAAAGwu5VsAAFvopMMn572v2avp/NZHl+Vvv3NvqqqqMRUAAAAAAAAAAAAAAAAAAAAAm0P5FgDAVviLN+6XP9hvYtP5VXMW5qJfPFpjIgAAAAAAAAAAAAAAAAAAAAA2h/ItAICt0NpS8sV3H5Z9dxrTdOf//ef8/OiBZ2pMBQAAAAAAAAAAAAAAAAAAAMBLUb4FALCVRg9ry4WzZ2bHUUMbzruq5EPfvDMPLl5ZczIAAAAAAAAAAAAAAAAAAAAAmlG+BQDwMkweNzJfO3VGhrY2/rdq1fqOnHnJnDy3an3NyQAAAAAAAAAAAAAAAAAAAABoRPkWAMDLNHPKuHzm7Qc1nS96fm3++PJ5Wd/RWWMqAAAAAAAAAAAAAAAAAAAAABpRvgUA0APeMWNS3ve6vZvO5zz2fD5x/b2pqqrGVAAAAAAAAAAAAAAAAAAAAAB0p3wLAKCHfOyYfXPMtJ2azq+btyj/9rMFNSYCAAAAAAAAAAAAAAAAAAAAoDvlWwAAPaSlpeSf33lopu2yXdOdf7zpgfzg/mdqTAUAAAAAAAAAAAAAAAAAAADAppRvAQD0oFHD2nLh7JkZP3pYw3lVJR++6o7c/9SKmpMBAAAAAAAAAAAAAAAAAAAAkCjfAgDocbtuPyIXnDYjQ9sa/6u1pr0zZ10yJ0tWrqs5GQAAAAAAAAAAAAAAAAAAAADKtwAAesFhu++Qz51wcNP5U8vX5ZzL5mXdhs4aUwEAAAAAAAAAAAAAAAAAAACgfAsAoJccf+hu+dDr92k6v+OJF/Lxb92dqqpqTAUAAAAAAAAAAAAAAAAAAAAwuCnfAgDoRR95w9S86aCdm86/fedTOe8nj9SYCAAAAAAAAAAAAAAAAAAAAGBwU74FANCLWlpKPn/ioTlot7FNdz73/Qdz071P15gKAAAAAAAAAAAAAAAAAAAAYPBSvgUA0MtGDG3NBafNzE7bDWu68ydX35V7n1xeYyoAAAAAAAAAAAAAAAAAAACAwUn5FgBADXYeOzwXnDYzw4c0/vdr7YbOnHXJ3DyzYl3NyQAAAAAAAAAAAAAAAAAAAAAGF+VbAAA1OXjS9vn8iYc2nS9esS5nXzo3a9s7a0wFAAAAAAAAAAAAAAAAAAAAMLgo3wIAqNGbD94lH/3DqU3ndy9anj+77q5UVVVjKgAAAAAAAAAAAAAAAAAAAIDBQ/kWAEDN/u/r98lxh+zadH7j3U/ni//1UI2JAAAAAAAAAAAAAAAAAAAAAAYP5VsAADUrpeRzJxycQyZv33TnX374UL5711M1pgIAAAAAAAAAAAAAAAAAAAAYHJRvAQBsA8OHtOaCU2dkl7HDm+782bV35a6FL9SYCgAAAAAAAAAAAAAAAAAAAGDgU74FALCNTNxueC44bWZGDGltOF/f0ZWzL52bp5evrTkZAAAAAAAAAAAAAAAAAAAAwMClfAsAYBs6cLex+Zd3Hdp0vmTl+px96dysae+oMRUAAAAAAAAAAAAAAAAAAADAwKV8CwBgG/ujA3bOn79x36bze59ckY9efVe6uqoaUwEAAAAAAAAAAAAAAAAAAAAMTMq3AAD6gPe9du+8/bDdms5vum9xvvCDX9WYCAAAAAAAAAAAAAAAAAAAAGBgUr4FANAHlFLymXcclBl77NB05ys/fjjfvuPJGlMBAAAAAAAAAAAAAAAAAAAADDzKtwAA+ohhba352qkzstv2I5ru/Pm37s68x5+vMRUAAAAAAAAAAAAAAAAAAADAwKJ8CwCgDxk/elgunD0zo4a2Npy3d3TlnMvmZtHza2pOBgAAAAAAAAAAAAAAAAAAADAwKN8CAOhj9t9lu3zxXYellMbzpavac9Ylc7N6fUe9wQAAAAAAAAAAAAAAAAAAAAAGAOVbAAB90Bum7ZS/PHa/pvMHFq/Mh6+6M11dVY2pAAAAAAAAAAAAAAAAAAAAAPo/5VsAAH3U2a/eKyfNnNR0/sP5z+Sz33+gxkQAAAAAAAAAAAAAAAAAAAAA/Z/yLQCAPqqUknPfelBeuee4pjtf++mCXDt3YY2pAAAAAAAAAAAAAAAAAAAAAPo35VsAAH3Y0LaW/OspMzJ53IimO391/T2Z89iyGlMBAAAAAAAAAAAAAAAAAAAA9F/KtwAA+rhxo4bm4tmHZ8ywtobzDZ1VzrlsXhYuW1NzMgAAAAAAAAAAAAAAAAAAAID+R/kWAEA/8IqdxuRLsw5LS2k8X7a6PWdeMicr122oNxgAAAAAAAAAAAAAAAAAAABAP6N8CwCgn/j9fSfmr988ren8V8+syoe+eUc6u6oaUwEAAAAAAAAAAAAAAAAAAAD0L8q3AAD6kfe8akre/crdm85//OCz+cx/zq8xEQAAAAAAAAAAAAAAAAAAAED/onwLAKAfKaXk744/IEfttWPTnQt/8Wiuuu2JGlMBAAAAAAAAAAAAAAAAAAAA9B/KtwAA+pkhrS05/5TpmbLjyKY7f/3te/PfjzxXYyoAAAAAAAAAAAAAAAAAAACA/kH5FgBAP7T9yKG56PTDM2Z4W8N5R1eV910xL48tXV1zMgAAAAAAAAAAAAAAAAAAAIC+TfkWAEA/tfeE0Tnv5OlpbSkN5y+s2ZAzL5mT5Ws31JwMAAAAAAAAAAAAAAAAAAAAoO9SvgUA0I+9+hUT8qnjpjWdP/Ls6nzwytvT0dlVYyoAAAAAAAAAAAAAAAAAAACAvkv5FgBAP3fqUVNy2lF7NJ3//KGlOffG+TUmAgAAAAAAAAAAAAAAAAAAAOi7lG8BAAwAf/t/puX39hnfdP6Nmx/LZbc8XmMiAAAAAAAAAAAAAAAAAAAAgL5J+RYAwADQ1tqSr86anr0mjGq686kb7ssvHlpaYyoAAAAAAAAAAAAAAAAAAACAvkf5FgDAADF25JBcNPvwjB0xpOG8s6vK+6+YlwXPrqo5GQAAAAAAAAAAAAAAAAAAAEDfoXwLAGAA2XP8qJx/yvS0tZSG8xXrOnLmJXPzwpr2mpMBAAAAAAAAAAAAAAAAAAAA9A3KtwAABpij9x6fv3/rgU3njy5dnfdfcXs2dHbVmAoAAAAAAAAAAAAAAAAAAACgb1C+BQAwAL37lbvnjFft2XR+8yPP5VM33FdjIgAAAAAAAAAAAAAAAAAAAIC+QfkWAMAA9Vdv2i+v23dC0/kVtz6Rmx9eWmMiAAAAAAAAAAAAAAAAAAAAgG1P+RYAwADV1tqSL737sLxi4uimOxf/8rH6AgEAAAAAAAAAAAAAAAAAAAD0Acq3AAAGsO2GD8lFsw/PDiOHNJz/+MElWbJyXc2pAAAAAAAAAAAAAAAAAAAAALYd5VsAAAPc7juOzBfeeWjDWWdXlW/f8WTNiQAAAAAAAAAAAAAAAAAAAAC2HeVbAACDwOumTsiUHUc2nF0zd1Gqqqo5EQAAAAAAAAAAAAAAAAAAAMC2oXwLAGAQKKXkxJmTG84eXrIqdyx8oeZEAAAAAAAAAAAAAAAAAAAAANuG8i0AgEHiHdMnpaU0nl07d1G9YQAAAAAAAAAAAAAAAAAAAAC2EeVbAACDxM5jh+c1Uyc0nH33rqeytr2z5kQAAAAAAAAAAAAAAAAAAAAA9VO+BQAwiJw0c3LD56vWd+R79z5dcxoAAAAAAAAAAAAAAAAAAACA+infAgAYRP5g/4nZYeSQhrNr5i6sOQ0AAAAAAAAAAAAAAAAAAABA/ZRvAQAMIsPaWnP8obs1nN2yYFmeeG5NzYkAAAAAAAAAAAAAAAAAAAAA6qV8CwBgkDlp5uSms+vmLawxCQAAAAAAAAAAAAAAAAAAAED9lG8BAAwy03bdLgfutl3D2XXzFqWzq6o5EQAAAAAAAAAAAAAAAAAAAEB9lG8BAAxCJ86Y3PD5U8vX5eZHltacBgAAAAAAAAAAAAAAAAAAAKA+yrcAAAah4w/dNUNbG/8reM3cRTWnAQAAAAAAAAAAAAAAAAAAAKiP8i0AgEFo+5FDc8wBOzWcff++xXlhTXvNiQAAAAAAAAAAAAAAAAAAAADqoXwLAGCQOmnm5IbP2zu6csNdT9WcBgAAAAAAAAAAAAAAAAAAAKAeyrcAAAapV+0zPruMHd5wdu3cRTWnAQAAAAAAAAAAAAAAAAAAAKiH8i0AgEGqtaXkhBmTGs7ueXJ57n9qRc2JAAAAAAAAAAAAAAAAAAAAAHqf8i0AgEGsWflWklw7b2GNSQAAAAAAAAAAAAAAAAAAAADqoXwLAGAQ22PHUTliz3ENZ9++48m0d3TVnAgAAAAAAAAAAAAAAAAAAACgdynfAgAY5E6aObnh8+fXbMh/zX+m5jQAAAAAAAAAAAAAAAAAAAAAvUv5FgDA1W0a0wAAIABJREFUIHfsQTtn9LC2hrNr5i6sOQ0AAAAAAAAAAAAAAAAAAABA71K+BQAwyI0c2pbjDtml4eynv3o2z6xYV3MiAAAAAAAAAAAAAAAAAAAAgN6jfAsAgJwwY3LD511V8q3bF9WcBgAAAAAAAAAAAAAAAAAAAKD3KN8CACDTd98+e08Y1XB27dxFqaqq5kQAAAAAAAAAAAAAAAAAAAAAvUP5FgAAKaXkpJmTG84eXbo6cx9/vuZEAAAAAAAAAAAAAAAAAAAAAL1D+RYAAEmSt03fLa0tpeHs2rkLa04DAAAAAAAAAAAAAAAAAAAA0DuUbwEAkCSZOGZ4fn/fCQ1n/3H301m9vqPmRAAAAAAAAAAAAAAAAAAAAAA9T/kWAAC/ceLMyQ2fr2nvzI33PF1zGgAAAAAAAAAAAAAAAAAAAICep3wLAIDfeP1+EzN+9NCGs+vmLqo5DQAAAAAAAAAAAAAAAAAAAEDPU74FAMBvDGltyVsP3a3h7LbHlmXBs6tqTgQAAAAAAAAAAAAAAAAAAADQs5RvAQDwW06cObnp7Lp5i2pMAgAAAAAAAAAAAAAAAAAAANDzlG8BAPBb9t15TA6ZvH3D2bduX5TOrqrmRAAAAAAAAAAAAAAAAAAAAAA9R/kWAAC/46SZkxo+f2bF+vzsoWdrTgMAAAAAAAAAAAAAAAAAAADQc5RvAQDwO447ZNcMa2v8r+K1cxfWnAYAAAAAAAAAAAAAAAAAAACg5yjfAgDgd2w3fEiOPXDnhrMf3P9Mlq1urzkRAAAAAAAAAAAAAAAAAAAAQM9QvgUAQEMnzZzc8PmGzirfufPJmtMAAAAAAAAAAAAAAAAAAAAA9AzlWwAANHTkXjtm0g4jGs6unrMwVVXVnAgAAAAAAAAAAAAAAAAAAADg5VO+BQBAQy0tJSfMmNRw9sDilbnvqRU1JwIAAAAAAAAAAAAAAAAAAAB4+ZRvAQDQ1AkzJqWUxrNr5i6sNwwAAAAAAAAAAAAAAAAAAAD/n507jc7zrM8Eft2yvMRLNieOHUuBLJCQhGxWWEIp6UJD2RqayKUtQ5n2lDkwbWegnSnTaTvtzFCGTplOpwPdoXTacrADJKyl0NICYZWzkkDIRiLZiR3HiRPHcbzong8RB/XNI1uylUevpN/vHH247/99P8+l59P76QKmgfItAAAm1Hfc0rzo9BMaZ9fcsCV79h1oOREAAAAAAAAAAAAAAAAAAADAkVG+BQDAQQ0O9DXu73x8Xz5z69aW0wAAAAAAAAAAAAAAAAAAAAAcGeVbAAAc1GXnrM6KJb2Nsw1Dwy2nAQAAAAAAAAAAAAAAAAAAADgyyrcAADioJQsX5McuOLlx9sU7tmfLw4+3nAgAAAAAAAAAAAAAAAAAAADg8CnfAgDgkAbX9Tfu15p8aNNIy2kAAAAAAAAAAAAAAAAAAAAADp/yLQAADum8vmNy5kkrGmcbN41kdLS2nAgAAAAAAAAAAAAAAAAAAADg8CjfAgDgkEopGRzoa5zdu2N3vnr3jpYTAQAAAAAAAAAAAAAAAAAAABwe5VsAAEzKay5cm96e0jjbuGm45TQAAAAAAAAAAAAAAAAAAAAAh0f5FgAAk7Jy+eL80HNWNc4+efN9eXTPvpYTAQAAAAAAAAAAAAAAAAAAAEyd8i0AACZt/UB/4/6efaP5+E33tZwGAAAAAAAAAAAAAAAAAAAAYOqUbwEAMGkvefaJOXHF4sbZxqHhltMAAAAAAAAAAAAAAAAAAAAATJ3yLQAAJq13QU9+/KK1jbPr7n04d2x7tOVEAAAAAAAAAAAAAAAAAAAAAFOjfAsAgCkZXNc/4Wzj0EiLSQAAAAAAAAAAAAAAAAAAAACmTvkWAABTcsaq5Vn3jOMaZx+6bnP2HRhtOREAAAAAAAAAAAAAAAAAAADA5CnfAgBgytYP9DXub9/1RP75tgdaTgMAAAAAAAAAAAAAAAAAAAAwecq3AACYslecd3KOWrigcbZhaLjlNAAAAAAAAAAAAAAAAAAAAACTp3wLAIApW764Ny9/7prG2T9+a1u273qi5UQAAAAAAAAAAAAAAAAAAAAAk6N8CwCAw7J+oK9xf/9ozdXXb245DQAAAAAAAAAAAAAAAAAAAMDkKN8CAOCwPO/U4/OMlUsbZx/8+nBqrS0nAgAAAAAAAAAAAAAAAAAAADg05VsAAByWUkoG1/U1zm7ftis3juxsOREAAAAAAAAAAAAAAAAAAADAoSnfAgDgsF2xri+lNM82DA23GwYAAAAAAAAAAAAAAAAAAABgEpRvAQBw2NYcc1S+/1knNs4+dsOWPL73QMuJAAAAAAAAAAAAAAAAAAAAAA5O+RYAAEdkcKCvcf/RJ/bn07fc33IaAAAAAAAAAAAAAAAAAAAAgINTvgUAwBF56dkn5dilCxtnG4aGW04DAAAAAAAAAAAAAAAAAAAAcHDKtwAAOCKLexfk8gvWNs6+dOeDGd6xu+VEAAAAAAAAAAAAAAAAAAAAABNTvgUAwBG7cl3fhLOrNo20mAQAAAAAAAAAAAAAAAAAAADg4JRvAQBwxM5de0zOXnN04+yqTSMZHa0tJwIAAAAAAAAAAAAAAAAAAABopnwLAIBpsX6gr3F/88OP58t3PdhyGgAAAAAAAAAAAAAAAAAAAIBmyrcAAJgWP3bB2ixa0PzzcsPQcMtpAAAAAAAAAAAAAAAAAAAAAJop3wIAYFoct2xRXnr2SY2zT33j/uzcva/lRAAAAAAAAAAAAAAAAAAAAABPpXwLAIBpMzjQ17i/d/9oPnrTlpbTAAAAAAAAAAAAAAAAAAAAADyV8i0AAKbNi591YlYfvaRxdtXQcMtpAAAAAAAAAAAAAAAAAAAAAJ6qd6YDTEYp5dQkFyQ5OcnyJPcluSfJl2qt+2Ygz/FJzkrSn+SkJMvGRjuTbE1yfa31rml+57FJLkmyNskJSbYn2Zwnv8HD0/kuAIDDtaCn5Ip1a/Puz935lNmNIzvzrfsfyVmrj56BZAAAAAAAAAAAAAAAAAAAAABP6uryrVLKlUnemuSFExzZUUr5YJLfrLVufxpzLE/yC2M5Lk6yZhJ3RpL8VZL/U2vdegTvvjDJbyZ5eZJFDUeeKKV8Kslv11pvONz3AABMl8F1/Y3lW0mycWgkv/HKs1tOBAAAAAAAAAAAAAAAAAAAAPA9PTMdoEkpZXkp5QNJNmbi4q0kOT7Jm5J8o5Ry2dMYaXWSdyR5dSZRvDWmL8mvJbmtlPKGw3lpKeVtSb6a5PI0F28lyeKx+VdLKf/xcN4DADCdnnnCsjzv1OMbZ1dfvzl794+2nAgAAAAAAAAAAAAAAAAAAADge3pnOkCnUsqCJB9M8vKO0QNJrk+yM8npSS5MUsZmJyW5ppTyw7XWL7YUdUeS25Pcn2RXnizBWp3k/CQrxp07Jsn7Sikra63vmuzDSym/luTtHduPJ/l6kvuSnJzk4iRLxmaLkryzlFJrrf9z6v8OAMD0WT/Qn6/dveMp+w8+tjf/+K1tedm5q2cgFQAAAAAAAAAAAAAAAAAAAEDSM9MBGvyP/MvirX1JfjFJX631slrr+lrruiTnJvnyuHOLk1xdSlnzNOXaluR9SX4qyTNrrStrrS+otV5ea31drXWw1vriJCuTXJnkzo77v1tKef5kXlRKeWWS/96x/adJTqm1vqTW+tpa6/cnOSXJn3ece2cp5WVT/N8AAKbVy5+7OssWLWicbRwabjkNAAAAAAAAAAAAAAAAAAAAwPd0VflWKeW0JP+uY3uw1vp/a617x2/WWm9N8kP5lwVcK5P8l6ch2t1J1tRaf7bW+oFa6z0THay17qu1fijJxUm+MW7Uk+S3DvWiUsqCJL+XpIzb/v1a67+ptW7veNcDtdafT/K/xz8iybvGngMAMCOWLurNK887uXH2udu2Zdsje1pOBAAAAAAAAAAAAAAAAAAAAPCkrirfypPFWQvHrf+y1nrNRIdrrY8neUOS8cVcPzdW4jVtaq0Haq2jU7zzUJ5aJPbDpZQVh7j6+iRnjlvfluQ/HeLO28bOfdfZSX56MjkBAJ4u6y/ua9wfrcmHr9/cchoAAAAAAAAAAAAAAAAAAACAJ3VN+VYp5agkV3Zsv/NQ92qt305y9bit3iQ/NY3RjsQ/JXl83Lo3yTMOcef1Hevfr7U+cbALY/M/OMRzAABaddEpx+W0E5c1zjYMDafW2nIiAAAAAAAAAAAAAAAAAAAAgC4q30pyWZKl49ZfrrV+a5J339ex/vHpiXRkaq2jSR7u2F4x0flSysokLx63tTfJ307ydX+TZN+49UtKKcdP8i4AwLQrpWRwXX/j7K4HHst19z7UciIAAAAAAAAAAAAAAAAAAACA7irfelnH+p+mcPcLSfaPW19YSjnpiBMdoVLK0iQndmxvOciVlyZZMG69qdb66GTeVWt9JMl147Z6x54HADBjrrhobRb0lMbZxqGRltMAAAAAAAAAAAAAAAAAAAAAdFf51rkd6y9P9mKt9bEkN3dsn3PEiY7cT+bJEqzvurvWes9Bzh/2NxjzpY51N3wDAGAeW3X0klz67M4u0id97MYt2b13f+MMAAAAAAAAAAAAAAAAAAAA4OnSTeVbz+lY3zHF+3d2rM8+gixHrJTyoiS/17Hdue7UmXlWfwMAgCQZHOhr3H9s74F88ub7W04DAAAAAAAAAAAAAAAAAAAAzHddUb5VSjk+yfEd2/dO8TGd5591+ImmrpSyuJTSV0p5VSnlr5N8Psmx4458LMkfHeIxZ3SsZ9U3AABo8oNnnZTjly1qnG0YGm45DQAAAAAAAAAAAAAAAAAAADDfdUX5Vv5lSVWS7K61PjbFZ2zrWB9zBHkOqZRyQymlfvcvyZ4kw0k+muSn871vW5O8O8mVtdZ6iMd2fofO/+lQWv0GAACTsai3J6+5cG3j7Gt378h3tk/1Zx8AAAAAAAAAAAAAAAAAAADA4eud6QBjlnesHz+MZ3TeWXGYWabL3iR/luQ9tdZbJ3nnSL/D0/INSimrkpw4xWunT8e7AYC5YXCgL3/xxbsbZ1dtGsmvXHZmy4kAAAAAAAAAAAAAAAAAAACA+apby7f2HMYzOounOp/ZtkVJXpdkUSnlnbXWOydx50i/w9P1Dd6c5L9M07MAgHnorNVH57y+Y3LTyM6nzK7aNJK3vPTZWdBTZiAZAAAAAAAAAAAAAAAAAAAAMN/0zHSACdSW7hyJlyc5ddzfeUl+JMmvJ/nm2Jljkvx8kptKKW84jHdM9X9q+xsAAEza4EB/4/79j+zJF+/Y3nIaAAAAAAAAAAAAAAAAAAAAYL7qlvKtXR3row7jGZ13Op85rWqtW2qt3xn3d3Ot9TO11rfXWs9O8sYke8aOL03y3lLK6w/x2CP9Dq1+AwCAqXj1+SdncW/zz88NQ8MtpwEAAAAAAAAAAAAAAAAAAADmq96ZDjBm1pVvHUqt9c9KKVuTXDO2VZK8p5TyD7XWzRNc25XkuHHrbinfek+SjVO8c3q+978DAOSYoxbmsnNW56M3bnnK7DO3bM1Dj+3NccsWzUAyAAAAAAAAAAAAAAAAAAAAYD7pmekAY3Z2rJeWUpZN8RmrOtYPH0GeaVFr/WiSj4zbWpbkzQe50vkdTpziK5+Wb1Br3VZrvWUqf0nunI53AwBzy/qB/sb9vQdGc80NE/WTAgAAAAAAAAAAAAAAAAAAAEyfrijfqrU+mOShju1TpviYZ3Ssbz/8RNPqAx3rlx3kbGfmzv/pULr1GwAAJEkuOX1l1h57VONs46aRltMAAAAAAAAAAAAAAAAAAAAA81FXlG+N+WbH+owp3j/tEM+bKbd1rA/2f83VbwAAkCTp6Sm5Yl1f4+yWLY/kG5t3tpwIAAAAAAAAAAAAAAAAAAAAmG+6qXzrGx3rF072YillWZLzDvG8mbKvY734IGcP+xuMedEhngcAMOMGJyjfSpKrNo20mAQAAAAAAAAAAAAAAAAAAACYj7qpfOvvOtaXTuHui5P0jltfX2vdesSJpkdnu8TBcn0myYFx63WllBWTecnYuYvGbe0fex4AQFfpP35pLjl9ZePs6hs254n9BxpnAAAAAAAAAAAAAAAAAAAAANOhm8q3Pp3k8XHrF5ZSzprk3Td0rD8yLYmmx490rG+f6GCtdXuSL47bWpTkpyb5np9OsnDc+vO11h2TvAsA0Kr1A/2N+w/v3pfP3rqt5TQAAAAAAAAAAAAAAAAAAADAfNI15Vu11t1JrurY/tVD3SulPDvJa8Zt7U/yt9MY7bCVUtYkeWPH9jWHuPZXHeu3lFIWH+I9i5P8+47t9x86IQDAzLjsnNVZsbi3cbZhaLjlNAAAAAAAAAAAAAAAAAAAAMB80jXlW2N+K8m+ces3lFJePdHhUsqSJO9Lsmjc9l/UWu882EtKKbXj79KDnF1WSnlrKeWoSf0H37t3YpJPJDl63PaOJB84xNX3J7lt3PrMJL9ziDvvGDv3Xbcm+ZvJJQUAaN9RixbkVRec3Dj7/O0P5L6dj7ecCAAAAAAAAAAAAAAAAAAAAJgvuqp8q9Z6V5I/6Ni+qpTyC6WU8QVbKaU8J8k/JLlk3PaDSX57mmMtTPKuJHeVUv5XKeWFnVk6cp1USvnlJN9McmHH+D/UWrcf7GW11gNJfiVJHbf91lLKn5RSVna864RSyp8mecv4RyT55bHnAAB0rfUD/Y37tSYfvm5zy2kAAAAAAAAAAAAAAAAAAACA+aJ3pgM0eFuSc5L86Nh6YZI/TPIbpZTrkjya5LQkFyUp4+7tTfKaWut9T1Ou1Xmy5OotSfaWUm5Ncl+Sh8dyHJPk2WPZSsP9/1xrfe9kXlRr/Xgp5deTvH3c9huT/KtSyleT3J9kTZLnJTmq4/rbaq1/N+n/CgBghpzfd0yetWp5bt+26ymzDUPDefOlp6eUpp9VAAAAAAAAAAAAAAAAAAAAAIev68q3aq0HSinrk/x5kp8YN1qV5GUTXNuW5GdqrV94uvONWZTkgrG/QxlJ8ku11o9M5QW11t8ppdQkv50nC8iSJ4u2Lp3gyr4kv1Fr/d2pvAcAYKaUUrJ+oD9v/+Q3nzK758Hd+drdO/L801bOQDIAAAAAAAAAAAAAAAAAAABgLuuZ6QBNaq27aq2vTTKY5CsHObojyR8lObfW+ndPU5xHkrw6yXuS3JpkdBJ39if5QpI3JnnOVIu3vqvW+o4kz09yTZK9ExzbOzZ/Xq31nYfzHgCAmXL5hWvT21MaZxs3jbScBgAAAAAAAAAAAAAAAAAAAJgPemc6wMHUWq9KclUp5dQkFyU5OcmyJPcnuSfJtbXWiUqpDvbc5oaH5rOjST429pdSyookZyd5ZpLVY3mSJ0u6dia5LclNtdY9U801wfuvT3J5KeW4JJckWZtkZZIHk2xO8qVa60PT8S4AgLaduGJxfvCsVfn7W7c+ZfaJm+7Lb736nCxf3NU/WQEAAAAAAAAAAAAAAAAAAIBZZlY0GdRa705y90znSJJa66NJvjr21+Z7H0ryiTbfCQDQhsGB/sbyrcf3HcgnbtqSn7j4lBlIBQAAAAAAAAAAAAAAAAAAAMxVPTMdAACA+e3SM0/MCcsXN842Do20nAYAAAAAAAAAAAAAAAAAAACY65RvAQAwoxYu6MkVF61tnA3d81DufGBXy4kAAAAAAAAAAAAAAAAAAACAuUz5FgAAM25woG/C2cahkRaTAAAAAAAAAAAAAAAAAAAAAHOd8i0AAGbcGatW5MJTjm2cfei6kew/MNpyIgAAAAAAAAAAAAAAAAAAAGCuUr4FAEBXWD/Q37j/wKNP5PO3P9ByGgAAAAAAAAAAAAAAAAAAAGCuUr4FAEBXeOV5a7JkYfPP0w1fH2k5DQAAAAAAAAAAAAAAAAAAADBXKd8CAKArrFiyMC8/d03j7LPf3JoHdz3RciIAAAAAAAAAAAAAAAAAAABgLlK+BQBA1xgc6G/c3z9ac/UNW1pOAwAAAAAAAAAAAAAAAAAAAMxFyrcAAOgazz/1+Jxy/NLG2cah4dRaW04EAAAAAAAAAAAAAAAAAAAAzDXKtwAA6Bo9PSVXrutrnH3r/kdz8+adLScCAAAAAAAAAAAAAAAAAAAA5hrlWwAAdJUr1vWllObZhqHhdsMAAAAAAAAAAAAAAAAAAAAAc47yLQAAusraY4/K951xQuPsozdsyZ59B1pOBAAAAAAAAAAAAAAAAAAAAMwlyrcAAOg66wf6G/cf2bM/n77l/pbTAAAAAAAAAAAAAAAAAAAAAHOJ8i0AALrOS88+KUcv6W2cbRwaaTkNAAAAAAAAAAAAAAAAAAAAMJco3wIAoOssWbggl1+4tnF27Z3bM/LQ7pYTAQAAAAAAAAAAAAAAAAAAAHOF8i0AALrS+oH+xv1akw9t2txyGgAAAAAAAAAAAAAAAAAAAGCuUL4FAEBXOufko3PW6hWNs42bhjM6WltOBAAAAAAAAAAAAAAAAAAAAMwFyrcAAOhKpZSsH+hvnI089Hi+cteDLScCAAAAAAAAAAAAAAAAAAAA5gLlWwAAdK3LL1ybhQtK42zjppGW0wAAAAAAAAAAAAAAAAAAAABzgfItAAC61vHLFuWlZ5/UOPvkzfflkT37Wk4EAAAAAAAAAAAAAAAAAAAAzHbKtwAA6GqD6/ob95/YP5qP3bil5TQAAAAAAAAAAAAAAAAAAADAbKd8CwCArvbiZ52Qk45e3DjbODTSchoAAAAAAAAAAAAAAAAAAABgtlO+BQBAV+td0JMrLuprnN0w/HC+vfXRlhMBAAAAAAAAAAAAAAAAAAAAs5nyLQAAut6V65rLt5Jk49Bwi0kAAAAAAAAAAAAAAAAAAACA2U75FgAAXe+0E5fn4mce1zj7yPWbs+/AaMuJAAAAAAAAAAAAAAAAAAAAgNlK+RYAALPC4EB/4/72XXvzuW9tazkNAAAAAAAAAAAAAAAAAAAAMFsp3wIAYFZ4xXPXZOmiBY2zDUMjLacBAAAAAAAAAAAAAAAAAAAAZivlWwAAzArLFvfmFc9d0zj73G3bsu3RPS0nAgAAAAAAAAAAAAAAAAAAAGYj5VsAAMwa6y/ub9w/MFpz9fWbW04DAAAAAAAAAAAAAAAAAAAAzEbKtwAAmDUGnnFcTj1hWeNsw9BIaq0tJwIAAAAAAAAAAAAAAAAAAABmG+VbAADMGqWUXLmur3F2x7ZduX744ZYTAQAAAAAAAAAAAAAAAAAAALON8i0AAGaVKy7qS09pnm0cGm43DAAAAAAAAAAAAAAAAAAAADDrKN8CAGBWWX3Mkrzk2Sc2zj524315fO+BlhMBAAAAAAAAAAAAAAAAAAAAs4nyLQAAZp31A/2N+7ue2J9PfeO+ltMAAAAAAAAAAAAAAAAAAAAAs4nyLQAAZp0fes5JOW7pwsbZhqHhltMAAAAAAAAAAAAAAAAAAAAAs4nyLQAAZp1FvT25/MK1jbOv3LUj9z64u+VEAAAAAAAAAAAAAAAAAAAAwGyhfAsAgFlpcF3/hLOrNg23mAQAAAAAAAAAAAAAAAAAAACYTZRvAQAwK5198tE5d+3RjbOrNo3kwGhtOREAAAAAAAAAAAAAAAAAAAAwGyjfAgBg1lo/0N+4v2Xnnlx7x/aW0wAAAAAAAAAAAAAAAAAAAACzgfItAABmrVeff3IW9Tb/pN24aaTlNAAAAAAAAAAAAAAAAAAAAMBsoHwLAIBZ69ili3LZOasbZ5++5f48vHtvy4kAAAAAAAAAAAAAAAAAAACAbqd8CwCAWW1wXV/j/t79o/nojVtaTgMAAAAAAAAAAAAAAAAAAAB0O+VbAADMai8644ScfMySxtnGoZGW0wAAAAAAAAAAAAAAAAAAAADdTvkWAACz2oKekivX9TXObt68M7dueaTlRAAAAAAAAAAAAAAAAAAAAEA3U74FAMCsd+W6/glnGzcNt5gEAAAAAAAAAAAAAAAAAAAA6HbKtwAAmPVOWbk0Lzjt+MbZ1ddvzt79oy0nAgAAAAAAAAAAAAAAAAAAALqV8i0AAOaE9QP9jfsP7d6Xf/jm1pbTAAAAAAAAAAAAAAAAAAAAAN1K+RYAAHPCj567JssX9zbONgwNt5wGAAAAAAAAAAAAAAAAAAAA6FbKtwAAmBOOWrQgrzp/TePsn7/9QO7fuaflRAAAAAAAAAAAAAAAAAAAAEA3Ur4FAMCcMTjQ37g/WpMPXz/SchoAAAAAAAAAAAAAAAAAAACgGynfAgBgzriw/9icsWp542zj0EhqrS0nAgAAAAAAAAAAAAAAAAAAALqN8i0AAOaMUkrWD/Q1zu7e/liG7nmo5UQAAAAAAAAAAAAAAAAAAABAt1G+BQDAnHL5hWuzoKc0zjYODbecBgAAAAAAAAAAAAAAAAAAAOg2yrcAAJhTVq1Ykh84c1Xj7OM33ZfHntjfciIAAAAAAAAAAAAAAAAAAACgmyjfAgBgzlk/0Ne4v3vvgXzi5vtaTgMAAAAAAAAAAAAAAAAAAAB0E+VbAADMOT9w1qqcsHxR42zj0HDLaQAAAAAAAAAAAAAAAAAAAIBuonwLAIA5Z+GCnrzmwrWNs69/56Hc9cCulhMBAAAAAAAAAAAAAAAAAAAA3UL5FgAAc9LgQP+Es6s2jbSYBAAAAAAAAAAAAAAAAAAAAOgmyrcAAJiTnn3SilzQf2zj7EPXjeTAaG05EQAAAAAAAAAAAAAAAAAAANANlG8BADBnDQ70Ne5vfeSJfP72B1pOAwAAAAAAAAAAAAAAAAAAAHQD5VsAAMxZrzr/5Czubf7Ju3FouOU0AAAAAAAAAAAAAAAAAAAAQDdQvgUAwJx19JKFeflz1zTOPnPr1ux4bG/LiQAAAAAAAAAAAAAAAAAAAICZpnwLAIA5bXBdX+P+vgM1V1+/ueU0AAAAAAAAAAAAAAAAAAAAwExTvgUAwJzRswMVAAAgAElEQVT2gtNWpu+4oxpnG4aGU2ttOREAAAAAAAAAAAAAAAAAAAAwk5RvAQAwp/X0lAyu62+cfev+R3PLlkdaTgQAAAAAAAAAAAAAAAAAAADMJOVbAADMeVesW5tSmmcbhobbDQMAAAAAAAAAAAAAAAAAAADMKOVbAADMeX3HLc2LTj+hcXbNDVuyZ9+BlhMBAAAAAAAAAAAAAAAAAAAAM0X5FgAA88LgQF/j/s7H9+Uzt25tOQ0AAAAAAAAAAAAAAAAAAAAwU5RvAQAwL1x2zuocvaS3cbZhaLjlNAAAAAAAAAAAAAAAAAAAAMBMUb4FAMC8sGThgrz6gpMbZ1+8Y3s2P/x4y4kAAAAAAAAAAAAAAAAAAACAmaB8CwCAeWP9QH/jfq3JhzeNtJwGAAAAAAAAAAAAAAAAAAAAmAnKtwAAmDeeu/aYnLV6ReNs46aRjI7WlhMBAAAAAAAAAAAAAAAAAAAAbVO+BQDAvFFKyeBAf+Ps3h2789W7d7ScCAAAAAAAAAAAAAAAAAAAAGib8i0AAOaVyy84Ob09pXG2cdNwy2kAAAAAAAAAAAAAAAAAAACAtinfAgBgXlm5fHF++DknNc4+efN9eXTPvpYTAQAAAAAAAAAAAAAAAAAAAG1SvgUAwLyz/uK+xv09+0bz/i99p90wAAAAAAAAAAAAAAAAAAAAQKuUbwEAMO98/7NOzKoVixtnv/f33847PvXNjI7WllMBAAAAAAAAAAAAAAAAAAAAbVC+BQDAvNO7oCc/flHfhPM/+ee78qa/2ZTde/e3mAoAAAAAAAAAAAAAAAAAAABog/ItAADmpdde3J9FCyb+OfzpW7bmJ/7kK9n6yJ4WUwEAAAAAAAAAAAAAAAAAAABPN+VbAADMS888YVl+9UfPOuiZmzfvzOXvvja3bNnZUioAAAAAAAAAAAAAAAAAAADg6aZ8CwCAeevnvu/U/NcfOyc9ZeIz9+3ck8E//nI+e+vW9oIBAAAAAAAAAAAAAAAAAAAATxvlWwAAzGuvf+Ez8943XJzli3snPLN774H8/P8byl988e7UWltMBwAAAAAAAAAAAAAAAAAAAEw35VsAAMx7l565Kh960yVZe+xRE56pNflvH781v3HNN7L/wGiL6QAAAAAAAAAAAAAAAAAAAIDppHwLAACSnLl6RT7yby/J+f3HHvTcX3/l3vzrv/x6Htmzr6VkAAAAAAAAAAAAAAAAAAAAwHRSvgUAAGNWrViSD77xBXnFeWsOeu4Lt2/PFe/5UoZ37G4pGQAAAAAAAAAAAAAAAAAAADBdlG8BAMA4SxYuyB++9sL84g+ecdBzt2/blcvffW023fNQS8kAAAAAAAAAAAAAAAAAAACA6aB8CwAAOvT0lPzyj5yZdw2en4ULyoTnHnxsb37yz76Sj964pcV0AAAAAAAAAAAAAAAAAAAAwJFQvgUAABO4Yl1f/vrnnp9jly6c8Mze/aP5pQ9cnz/47O2ptbaYDgAAAAAAAAAAAAAAAAAAADgcyrcAAOAgnn/aylz95hfltBOWHfTc73/223nrhhvzxP4DLSUDAAAAAAAAAAAAAAAAAAAADofyLQAAOIRnnrAsH37zJXnhaSsPeu4j12/O6/78q9nx2N6WkgEAAAAAAAAAAAAAAAAAAABTpXwLAAAm4dili/L+n31e1g/0HfTc17/zUC5/97W5Y9uulpIBAAAAAAAAAAAAAAAAAAAAU6F8CwAAJmlRb0/eecV5+dWXnXXQc/fu2J3XvOfaXHvH9paSAQAAAAAAAAAAAAAAAAAAAJOlfAsAAKaglJI3XXp6/vh1F2XJwol/Tj+6Z39+5r1fywe+dm+L6QAAAAAAAADg/7Nzn3F+l3W+8D/XlEx6QoBAAilUgQy9JBDWvnYpShNBsCAKrHq75eyes8Vz367uHj2elRsFxF1dKYogRVdFBRslAQk1ASQCJhBSCEgaKVN+50HCOsxmhkwy85/J5P1+vf6vcF3/73Vdn+FR5kE+AAAAAAAAAAC8GuVbAACwFd7WPCHfPf/Y7DqqqcuZ1vYqf3PDw/ncjx5NW3tVw3QAAAAAAAAAAAAAAAAAAABAV5RvAQDAVjpkz7G5+cKZOXDC6G7nvvbrJ/Oxq+bkpQ2tNUoGAAAAAAAAAAAAAAAAAAAAdEX5FgAAbIOJY4fluo8dmzceML7buZ89sjSnXT4rS1asq1EyAAAAAAAAAAAAAAAAAAAAYHOUbwEAwDYa2dSQKz5wVD40c69u5+YuWpmTvnJn5i5aUaNkAAAAAAAAAAAAAAAAAAAAQGfKtwAAoBfU15X8/bsPyv934rTU15Uu55asXJfTLp+Vnz2ytIbpAAAAAAAAAAAAAAAAAAAAgJcp3wIAgF509rFT82/nHp1RTQ1dzry0oS0fvfLefP32J1NVVQ3TAQAAAAAAAAAAAAAAAAAAAMq3AACgl71u/13zvQuOyx5jh3U5U1XJZ3/4aP7HTXPT0tZew3QAAAAAAAAAAAAAAAAAAACwY1O+BQAAfWD/3Ublpgtn5vDJY7udu+buhfnQN3+TFWtbapQMAAAAAAAAAAAAAAAAAAAAdmzKtwAAoI/sOqop3z5vRt51yIRu526fvzzvvfSuLHz+pRolAwAAAAAAAAAAAAAAAAAAgB2X8i0AAOhDQxvrc/EZh+fP3rhvt3O/W7Y6J331zsxZ8EKNkgEAAAAAAAAAAAAAAAAAAMCOSfkWAAD0sbq6kj9/y2vypdMOzZD6rv8K/sKaDXnfFXfn5gcW1TAdAAAAAAAAAAAAAAAAAAAA7FiUbwEAQI2854g9c9VHpmen4Y1dzmxobc8nv/NA/uXWx1NVVQ3TAQAAAAAAAAAAAAAAAAAAwI5B+RYAANTQMXuNy40XzMzeu47odu5fbp2fT137QNa1tNUoGQAAAAAAAAAAAAAAAAAAAOwYlG8BAECNTd1lRG78+Mwct8/O3c7d/MCzef/X787zq9fXKBkAAAAAAAAAAAAAAAAAAAAMfsq3AACgH4wZ3ph//9AxOf2oSd3OzVnwh5z01Tszf+mqGiUDAAAAAAAAAAAAAAAAAACAwU35FgAA9JPG+rr803sPzt+8/YCU0vXc0y+szXsuvSt3zF9eu3AAAAAAAAAAAAAAAAAAAAAwSCnfAgCAflRKyfmv2yeXvv/IDG3s+q/nq9a15pxv3JNr7l5Yw3QAAAAAAAAAAAAAAAAAAAAw+CjfAgCAAeBtzbvnuvOPy/hRTV3OtLVX+e83PpzP/scjaWuvapgOAAAAAAAAAAAAAAAAAAAABg/lWwAAMEAcvOeY3HzRzBw4YXS3c1+/46mcf+WcrFnfWqNkAAAAAAAAAAAAAAAAAAAAMHgo3wIAgAFkwphhuf5jx+ZNB4zvdu7WR5fm1MtmZfGKtTVKBgAAAAAAAAAAAAAAAAAAAIOD8i0AABhgRjQ15GsfOCofPn6vbuceWbwyJ33lzsxdtKJGyQAAAAAAAAAAAAAAAAAAAGD7p3wLAAAGoPq6kr9710H57EnNqa8rXc4tXbk+p142Kz+dt6SG6QAAAAAAAAAAAAAAAAAAAGD7pXwLAAAGsLNmTMk3zj06o5oaupxZ29KW86+akyt+/WSqqqphOgAAAAAAAAAAAAAAAAAAANj+KN8CAIAB7rX775rvXXBc9txpWJczVZX8448ezX+/8eG0tLXXMB0AAAAAAAAAAAAAAAAAAABsX5RvAQDAdmD/3Ublpgtn5ojJY7ud+/Y9T+fcb9yTFS+11CgZAAAAAAAAAAAAAAAAAAAAbF+UbwEAwHZil5FNuea8GXn3oRO7nbvzd8/nPZfemQXPr6lRMgAAAAAAAAAAAAAAAAAAANh+KN8CAIDtyNDG+lx8xmH5xJv263buiefW5OSv3pV7f/9CjZIBAAAAAAAAAAAAAAAAAADA9kH5FgAAbGdKKfn0n+6f/3P6oRlS3/Vf6V9YsyFnXnF3brp/UQ3TAQAAAAAAAAAAAAAAAAAAwMCmfAsAALZTJx++Z64+b3rGjRjS5cyGtvZ86toH8qWfPZ6qqmqYDgAAAAAAAAAAAAAAAAAAAAYm5VsAALAdO3rquNx4wXHZZ9cR3c5dfNv8fPI7D2RdS1uNkgEAAAAAAAAAAAAAAAAAAMDApHwLAAC2c1N2HpEbPj4zx+2zc7dz33/w2Zx5xewsX72+RskAAAAAAAAAAAAAAAAAAABg4FG+BQAAg8CY4Y359w8dk/cdM6nbufsWvpiTvnJn5i9dVaNkAAAAAAAAAAAAAAAAAAAAMLAo3wIAgEGisb4unzv54PyPdxyYUrqee+YPa/Oer96VXz/+XO3CAQAAAAAAAAAAAAAAAAAAwAChfAsAAAaRUkrOe+3eueysIzOssb7LuVXrW/PBb/4mV81eUMN0AAAAAAAAAAAAAAAAAAAA0P+UbwEAwCD01mm757qPHZvdRjd1OdPWXuVvb5qb//cHj6StvaphOgAAAAAAAAAAAAAAAAAAAOg/yrcAAGCQat5jTG6+8PhMmzi627l/u/OpfPRb92bN+tYaJQMAAAAAAAAAAAAAAAAAAID+o3wLAAAGsd3HDM13zz82bz5wt27nbntsWU69bFYWr1hbo2QAAAAAAAAAAAAAAAAAAADQP5RvAQDAIDeiqSGXn31kPnL8Xt3OPbJ4ZU685M48/MyKGiUDAAAAAAAAAAAAAAAAAACA2lO+BQAAO4D6upK/fddB+ceTm1NfV7qcW7ZqfU67fFZumbukhukAAAAAAAAAAAAAAAAAAACgdpRvAQDADuT906fkmx88OqOGNnQ5s7alLR+/ek4u/9UTqaqqhukAAAAAAAAAAAAAAAAAAACg7ynfAgCAHcyf7Ldrbvj4cZk0bliXM1WVfP7Hj+Uvrnsoz764tobpAAAAAAAAAAAAAAAAAAAAoG8p3wIAgB3QfruNyo0XzMwRk8d2O/e9+57J8f/883z0W/fm9vnPpb29qlFCAAAAAAAAAAAAAAAAAAAA6BvKtwAAYAe1y8imXHPejJxw6MRu59qr5KePLM3Z/3pP3vSlX+Xrtz+ZFS+11CglAAAAAAAAAAAAAAAAAAAA9C7lWwAAsAMb2lifL59xWD715v22aP6p5Wvy2R8+mmM+d2v+8roH89AzL/ZxQgAAAAAAAAAAAAAAAAAAAOhdDf0dAAAA6F+llHzqzftn6s4j8lfXP5QNbe2vemZ9a3uum/NMrpvzTA7Zc0zOmjEl7z5kYoYNqa9BYgAAAAAAAAAAAAAAAAAAANh6df0dAAAAGBhOOnyPXHPe9Ow+emiPzj30zIr81fUPZcbnb8tn/+ORPLV8TR8lBAAAAAAAAAAAAAAAAAAAgG2nfAsAAPhPR00dl1v//HX5zLsPyj67jujR2RVrW/L1O57KG774y5z9r3fnJ/OWpLWtvY+SAgAAAAAAAAAAAAAAAAAAwNZp6O8AAADAwDKyqSHnztwr5xw3NbOffCFXzV6wsUirvdriO26fvzy3z1+eCWOG5n3HTM4ZR0/K+NFD+zA1AAAAAAAAAAAAAAAAAAAAbBnlWwAAwGaVUnLsPjvn2H12ztKV6/Kde57ONfcsyNKV67f4jsUr1uVLP3s8F982P29t3j1nz5iS6XuNSymlD5MDAAAAAAAAAAAAAAAAAABA10pVVf2dgUGolDItydyX13Pnzs20adP6MREAAL2hta09tz66LFfNXpA7frd8q+7Yb/zInDVjSk4+Yo+MHtrYywkBAAAAAAAAAAAAAAAAAADYHsybNy/Nzc0dt5qrqppXi7eVb9EnlG8BAAx+Tzy3OlfPXpjr5jydVetae3x++JD6nHT4Hjlr+pQcNHF0HyQEAAAAAAAAAAAAAAAAAABgoFK+xaCjfAsAYMexdkNbfvDgs/nW7N9n7qKVW3XHkVN2ytkzpuTtB++epob6Xk4IAAAAAAAAAAAAAAAAAADAQKN8i0FH+RYAwI6nqqo8+MyKXDlrQX7w0LPZ0Nre4zt2HjEkpx09KWceMzmTxg3vg5QAAAAAAAAAAAAAAAAAAAAMBMq3GHSUbwEA7Nj+sGZDrp/zTK66e0EWPP9Sj8+XkrzhNeNz9owpee3+u6a+rvRBSgAAAAAAAAAAAAAAAAAAAPpLf5ZvNdTiEQAAYMey04ghOe+1e+fDx++V23+3PFfOWpCfP7Y07VvY/VtVyc8fW5afP7Ysk8YNy/unT8lpR03KuBFD+jY4AAAAAAAAAAAAAAAAAAAAg16pqi381+/QA6WUaUnmvryeO3dupk2b1o+JAADob4teXJtv370w3/nNwixfvaHH54fU1+Wdh0zIWTOm5IjJY1NK6YOUAAAAAAAAAAAAAAAAAAAA1MK8efPS3Nzccau5qqp5tXhb+RZ9QvkWAABd2dDanlvmLclVsxfknqde2Ko7DpwwOmfPmJITD5uYEU0NvZwQAAAAAAAAAAAAAAAAAACAvqZ8i0FH+RYAAFvit0tW5arZC3LDfc9kzYa2Hp8f1dSQ9x65Z86aMTn7jh/VBwkBAAAAAAAAAAAAAAAAAADoC8q3GHSUbwEA0BOr17fmxvsX5erZC/LYklVbdceMvcfl7BlT85Zpu6Wxvq6XEwIAAAAAAAAAAAAAAAAAANCb+rN8q6EWjwAAAHRnZFNDzp4xJWdNn5x7F/whV85akB/PXZyWti0vC5795AuZ/eQLGT+qKWccMznvO2ZSJowZ1oepAQAAAAAAAAAAAAAAAAAA2B4p3wIAAAaMUkqOnjouR08dl+dWHZTv3vt0rrl7YRa9uHaL71i2an0uvm1+vvKL3+XNB47P2TOm5rh9dk5dXenD5AAAAAAAAAAAAAAAAAAAAGwvSlVV/Z2BQaiUMi3J3JfXc+fOzbRp0/oxEQAA26u29iq/eGxZrpy9IL+e/1y25leYvXcZkTOnT86pR07KmOGNvR8SAAAAAAAAAAAAAAAAAACAHpk3b16am5s7bjVXVTWvFm8r36JPKN8CAKAvLHh+Ta65e2GuvffpvPhSS4/PD22sywmHTszZM6bm4D3H9EFCAAAAAAAAAAAAAAAAAAAAtoTyLQYd5VsAAPSldS1t+eFDi3PV3Qty/8IXt+qOQyeNzVnTJ+fdh07M0Mb6Xk4IAAAAAAAAAAAAAAAAAABAd5RvMego3wIAoFbmLlqRq2YvyE0PLMq6lvYenx8zrDGnHbVn3j99SqbuMqIPEgIAAAAAAAAAAAAAAAAAANCZ8i0GHeVbAADU2oq1LfnenGdy1d0L8uRza7bqjj/Zb5ecPWNK3njA+DTU1/VyQgAAAAAAAAAAAAAAAAAAAF7Wn+VbDbV4BAAAoK+NGdaYDx2/Vz44c2pmPfF8rpy9ID99ZGna2re8cPj2+ctz+/zlmThmaM6cPjmnHT0p40cN7cPUAAAAAAAAAAAAAAAAAAAA1Fqpqi3/h+iwpUop05LMfXk9d+7cTJs2rR8TAQCwI1qyYl2+85uF+fY9C7N05foen2+oK3lb8+45e8aUHLPXuJRS+iAlAAAAAAAAAAAAAAAAAADAjmfevHlpbm7uuNVcVdW8WrytfIs+oXwLAICBpKWtPbc+sjRXzl6Qu554fqvu2H+3kTlrxpQct88uGTW0ISObGjJ8SL1CLgAAAAAAAAAAAAAAAAAAgK3Qn+VbDbV4BAAAoD811tfl7QdPyNsPnpDfLVudq+9ekOvnPJNV61q3+I7Hl67O39/8yt/T6koyoqkho5oaMqKpISM3lXK9XM41sqlx0179f/73qA5z//kZ2pDG+rre/rEBAAAAAAAAAAAAAAAAAADYDOVbAADADmXf8SPzD++elr9862vy/QeezZWzF2Tesyu36q72Klm1rrVHJV5daWqo+2Np12bKuUY2NXYo9dpY9tVx/uVSr2GN9SmlbHMeAAAAAAAAAAAAAAAAAACAwUr5FgAAsEMaPqQhZxwzOacfPSkPPP1irpy9IP/x0OJsaG3vlzzrW9uzfvWGLF+9YZvuqSvZWMzVscRraOPG9aa9//r9H9cjhmws9RrR1JDG+rpe+ukAAAAAAAAAAAAAAAAAAAAGDuVbAADADq2UksMn75TDJ++Uv33nQbnu3qdz9d0Ls/CFl/o72lZpr5JV61qzal1rsmLb7hraWJeRTY0Ztamka0RT/SvWL5d37Td+ZN54wPg0KOsCAAAAAAAAAAAAAAAAAAC2A8q3AAAANhk3YkjOf90+Oe9P9s6v5z+Xq2YvyG2PLUtV9Xey/rGupT3rWtZn+er1rzp74ITRufysIzN55+E1SAYAAAAAAAAAAAAAAAAAALD16vo7AAAAwEBTV1fy+teMz9fPOTq3/9UbcuEb9skeY4f1d6wB7dHFK3PiV+7I3U8+399RAAAAAAAAAAAAAAAAAAAAulWqqurvDAxCpZRpSea+vJ47d26mTZvWj4kAAGDbrVnfmtXrW7Nq3cY/V69rzer1La9cb3h5f+Ofq9b/cb1m/cb1htb2/v5R+kxjfck/nnRwTjt6Un9HAQAAAAAAAAAAAAAAAAAABrB58+alubm541ZzVVXzavF2Qy0e2VallL2SHJZkYpKRSRYnWZDkrqqqWvohz7AkByY5IMmumzKtTvJCNhZOPVxVVWutcwEAAH1rRFNDRjQ1ZLfR23bP+ta2rFnftqmcqyWr17VmzYbOpV5/XG++9GvjZ6BpaavyV997KPOXrcpfv/3A1NeV/o4EAAAAAAAAAAAAAAAAAADwCgO6fKuUckqSTyc5touRF0op1yb5+6qqlvdxliOSnJTkjUmOSdLYzfiaTbm+XFXVQz185/VJfrG1OZMsqKpq6jacBwAA+lhTQ32aGuozbsSQbbqnvb3KSy1tm8q4Wl5RzrXq5dKul4u8OpZ2dSjv2ljq1ZKWtqqXfrqNrrj9qTzx3Jp8+YzDMmpod78+AQAAAAAAAAAAAAAAAAAA1NaALN8qpYxMckWSM15ldFySjyd5TynlnKqqftIHWYYmmZdk7x4cG5HkQ0nOKaV8McnfVVXV0tvZAACAHVtdXcnIpoaMbGpIMnSb7lrf2vbKUq51Hcu5NlPata41Ty1fk98uXdXlnT9/bFnee+ld+ddzjs6kccO3KR8AAAAAAAAAAAAAAAAAAEBvGXDlW6WU+iTXJnlHp6+eS3J/khVJ9klyeJKy6bvdktxcSnlzVVV39HKkhmy+eKtK8tskC5MsTzIySXOn2fok/y3JfqWU06uqau3lbAAAAL2iqaE+TSPrs/PIpi0+09LWns98f16uvnthlzOPL12dE79yZy4/+8gcPXVcb0QFAAAAAAAAAAAAAAAAAADYJnX9HWAz/imvLN5qSfJnSfasquqtVVWdVlXVkdlYdDWrw1xTkptKKRP6MFtbkh8nOSPJ+KqqDtyU6f1VVZ1YVdU+SY5K8utO596T5DNb+eaXk+zVg8/xW/kOAABAjzTW1+WzJzXnf54wLXWl67kX1mzImVfMznX3Pl27cAAAAAAAAAAAAAAAAAAAAF0YUOVbpZS9k3yy0/apVVVdUlXVho6bVVU9kuRNeWUB185J/qEPoq1P8pUkU6uqekdVVddWVbV8c4NVVc1J8sYk3+701V+WUqZsxdsvVlX1+x58ntmKNwAAALZKKSXnHDc13/zgMRk1tKHLuZa2Kn95/UP5/I8eTVt7VcOEAAAAAAAAAAAAAAAAAAAArzSgyreysTirscP6m1VV3dzVcFVVa5Ocm6RjMdeHN5V49ZZ1SfatquqiLS22qqqqLcmHkzzdYXtIktN6MRcAAMCA8dr9d82NF8zMlJ2Hdzt3+a+fzPlX3pvV61trlAwAAAAAAAAAAAAAAAAAAOCVBkz5VillWJJTOm3/86udq6rq8SQ3ddhqSHJmb+Wqqqp1S0u3Op1bm+Qbnbbf0DupAAAABp59x4/MTRfMzIy9x3U7d+ujy3LKpXfl6RdeqlEyAAAAAAAAAAAAAAAAAACAPxow5VtJ3ppkeIf1rKqqHtvCs51Lrt7TO5G22f2d1hP7JQUAAECN7DRiSK788PS875hJ3c49tmRVTvrKnbn39y/UKBkAAAAAAAAAAAAAAAAAAMBGA6l8622d1r/swdnbk7R2WB9eStltmxNtu9ZO6yH9kgIAAKCGGuvr8rmTD87fv+ug1JWu555fsyFnXnF3vjfnmdqFAwAAAAAAAAAAAAAAAAAAdngDqXyrudN61pYerKpqTZKHO21P2+ZE227fTuvF/ZICAACgxkop+dDxe+Xfzj06o5oaupzb0NaeP7/uwfzTjx9Le3tVw4QAAAAAAAAAAAAAAAAAAMCOaiCVbx3Yaf27Hp5/otP6oG3I0ltO6bS+ZyvueEMp5YZSypOllNWllLWllEWllDmllEtKKe8tpTT2RlgAAIDe9vrXjM8NFxyXyeOGdzt32a+eyPlXzcma9a01SgYAAAAAAAAAAAAAAAAAAOyoBkT5VillXJJxnbYX9vCazvP7bX2ibVdKOTrJzE7bN27FVa9NcnKSvZKMSDI0ycQkRyS5MMn1SZ4spVxYSilbnxgAAKBv7LfbqNx04cxM36vzr32v9LNHlua9l96VRS+urVEyAAAAAAAAAAAAAAAAAABgRzQgyreSjO20fqmqqjU9vGNZp/WYbcizTUopjUku77R9e1VV9/TRk3smuSTJD0opnf9fAgAA9LtxI4bkyg9PzxlHT+p27rElq3LiJXdkzoI/1CgZAAAAAAAAAAAAAAAAAACwo2no7wCbjOy0XrsVd3Q+M2ors/SGLyQ5vMO6JcknenjHyiS3JvlVknnZWC62NslOSfZP8qdJTk8ytMOZdya5qZTylqqqNmxd9P+qlDI+ya49PLZPb70PAAAMDkMa6vL59xyc/XYblX/84SNprzY/t3z1hrzva7Pzz6ccnJMP37O2IQEAAAAAAAAAAAAAAAAAgEFvoJZvrduKOzqXb3W+syZKKR9K8slO25+pquqBLdKiMpEAACAASURBVLxiSZIPJvlOVVVd/X/4TZKrSyl/neTfkry9w3evS/JPST695alf1QVJ/qEX7wMAAHZQpZR8+Pi9svcuI/Jn374/q9e3bnZuQ1t7/p9rH8z8pavzF295TerqSo2TAgAAAAAAAAAAAAAAAAAAg1VdfwfoQlWjM72qlPK2JJd12v6PJJ/f0juqqnqsqqpvdlO81XF2SZJ3Jrmu01cXllL22tI3AQAAau0NB4zPDRccl0njhnU799VfPpGPXz0na7oo6QIAAAAAAAAAAAAAAAAAAOipgVK+tbrTuvt/fb15nc90vrNPlVJmJvleksYO23ckOb2qqj4rBtt097lJFnfYHpLkw331JgAAQG/Yf7dRufnC43PM1HHdzv1k3tKcetmsPPvi2holAwAAAAAAAAAAAAAAAAAABjPlW72glHJkkh8mGd5h+54k76yq6qW+fn/TGxd32n5bLz7x1STNPfyc2IvvAwAAg9S4EUNy1Uem59Qj9+x27pHFK3PCJXfmvoV/qFEyAAAAAAAAAAAAAAAAAABgsGro7wCbrOi0Hl5KGVFV1Zoe3DG+0/rFbcy0RUophyT5aZIxHbbvT/LWqqpW1iLDJrck+XyH9cG9dXFVVcuSLOvJmVJKbz0PAAAMckMa6vK/Tjkk++82Kp/78aOpqs3PLV+9Pmd8bXa+cMohOfGwPWobEgAAAAAAAAAAAAAAAAAAGDTq+jtAklRV9XySP3TantzDa6Z0Ws/f+kRbppRyUJJbk4zrsD03yVuqqqpJ+VcHv++0HlJKGbO5QQAAgIGmlJLzXrt3vv6BozJiSH2Xcxta2/PJ7zyQL/7kt2lv76KlCwAAAAAAAAAAAAAAAAAAoBsDonxrk0c7rfft4fm9X+W+XlVKeU2S25Ls2mH7sSRvrqpqeV++3YW1m9kbVvMUAAAA2+BNB+6WGy6YmT136v7XmUt+8btccPV9eWlDa42SAQAAAAAAAAAAAAAAAAAAg8VAKt+a22l97JYeLKWMSHLIq9zXa0op+yb5eZLdO2zPT/LGqqqW9tW7r2KXzew9X/MUAAAA2+g1u4/KzRfOzNFTd+p27pZ5S3LqZbOyeMXmuogBAAAAAAAAAAAAAAAAAAA2byCVb93Saf36Hpz9kyQNHdb391UJVillr2ws3prYYfvJbCzeWtwXb26h6Z3Wz1VV1dIvSQAAALbRziObctVHpueUI/fsdm7esytzwiV35oGnX6xRMgAAAAAAAAAAAAAAAAAAYHs3kMq3fpJkbYf1saWUA7bw7Lmd1jf2SqJOSimTs7F4a1KH7QXZWLz1TF+82QNndlr/sj9CAAAA9Jamhvp84ZRD8jdvPyCldD333Kr1Of3yWfn+g8/WLhwAAAAAAAAAAAAAAAAAALDdGjDlW1VVvZTk+k7b/+3VzpVS9k9ycoet1iTX9GK0l9+ZmOS2JFM7bC/KxuKtBb39Xk+UUl6f5D2dtm/uhygAAAC9qpSS81+3T644+6iMGFLf5dz61vZ84tv350s/ezzt7VUNEwIAAAAAAAAAAAAAAAAAANubAVO+tclnkrR0WJ9bSjmhq+FSytAk30gypMP2v1ZV9UR3j5RSqk6f17/K/PhsLN7at8P24iRvqKrqye7O9kQp5S2llEN7eGZ6ku8lKR22f5vk2t7KBQAA0N/efNBuuf7jx2WPscO6nbv4tvm56Nv3Ze2GtholAwAAAAAAAAAAAAAAAAAAtjcDqnxrU5HVlzttX19KuaiU0rFgK6WUA7OxEOu4DtvPJ/mfvZmplDI2yc+SHNBhe02SDydpKaVM7cnnVZ47Lsn9pZRbSinnbir96irXpFLKF5LcnmRch69aklxQVVXrVvy4AAAAA9aBE0bn5otm5sgpO3U796OHl+S0y2dlyYp1NUoGAAAAAAAAAAAAAAAAAABsTxr6O8Bm/HWSaUnevmndmOT/T/J3pZT7kqxKsneSI5KUDuc2JDm5qqrFvZznsCSHdNobkeRHW3lf2YLv37rpk1LKoiS/TfJikrVJxiTZf9Ons7YkH6qq6udbmQ0AAGBA22VkU645b3r+5oaHc8N9i7qce3jRipxwyR35+jlH5ZA9x9YwIQAAAAAAAAAAAAAAAAAAMNANuPKtqqraSimnJfl6ktM7fDU+ydu6OLYsyTlVVd3e1/n6wR6bPq/mySQfqKrqzj7OAwAA0K+aGurzv089NPuNH5X/9ZPHUlWbn1u2an1OvWxW/vdph+Zdh0ysbUgAAAAAAAAAAAAAAAAAAGDAquvvAJtTVdXqqqrOSHJqktndjL6Q5NIkzVVV3VKTcH3r+0m+muThJG1bMN+a5K4k5yQ5SPEWAACwoyil5OOv3yeXn3Vkhg+p73JufWt7Lrrm/vyfnz2eqquWLgAAAAAAAAAAAAAAAAAAYIdStod/fFxK2SvJEUkmJhmRZEmSBUnurKpqQ39m6yullKFJDkoyJcmEJKOSNCZZneQPSZ5Kcm9VVS/1W8hulFKmJZn78nru3LmZNm1aPyYCAAAGq0eeXZnzvnVvFr24ttu5dx4yIV885dAM66asCwAAAAAAAAAAAAAAAAAAqI158+alubm541ZzVVXzavF2Qy0e2VZVVT2VjWVTO4yqqtYluW/TBwAAgC4cNHF0brpwZs6/8t7ct/DFLud++NDiPP3CS7niA0dlt9FDa5gQAAAAAAAAAAAAAAAAAAAYSOr6OwAAAABsq11HNeWa82bk5MP36HbuoWdW5IRL7sjDz6yoUTIAAAAAAAAAAAAAAAAAAGCgUb4FAADAoDC0sT5fOu3Q/OVbX9Pt3NKV63Pq5Xflhw8trlEyAAAAAAAAAAAAAAAAAABgIFG+BQAAwKBRSsmFb9g3l511ZIY11nc5t66lPRdec18uvm1+qqqqYUIAAAAAAAAAAAAAAAAAAKC/Kd8CAABg0Hlb8+657mPHZsKYod3Ofelnj+cT33kg61raapQMAAAAAAAAAAAAAAAAAADob8q3AAAAGJSa9xiTmy+amcMmje127gcPPpvTL5+VZSvX1SgZAAAAAAAAAAAAAAAAAADQn5RvAQAAMGiNHzU03/nojJx42MRu5x58ZkVOuOTOzF20okbJAAAAAAAAAAAAAAAAAACA/qJ8CwAAgEFtaGN9/uX0w/IXb9m/27klK9fl1Mtm5Za5i2uUDAAAAAAAAAAAAAAAAAAA6A/KtwAAABj0Sim56I375dL3H5FhjfVdzq1tacvHrrovl/x8fqqqqmFCAAAAAAAAAAAAAAAAAACgVpRvAQAAsMN4+8ETct3Hjs3uo4d2O/fFnz6eT137QNa1tNUoGQAAAAAAAAAAAAAAAAAAUCvKtwAAANihNO8xJt+/aGYOnTS227mbH3g2Z3xtdpatWlejZAAAAAAAAAAAAAAAAAAAQC0o3wIAAGCHM3700Fz70Rl596ETu5174OkXc9Ild2besytqlAwAAAAAAAAAAAAAAAAAAOhryrcAAADYIQ1trM/FZxyWT//p/t3OPbtiXU65dFZumbukRskAAAAAAAAAAAAAAAAAAIC+pHwLAACAHVYpJZ9403756vuPyNDGrn9FXtvSlo9dNSdf+cXvUlVVDRMCAAAAAAAAAAAAAAAAAAC9TfkWAAAAO7x3HDwh151/XHYb3dTt3Bd+8tt8+rsPZl1LW42SAQAAAAAAAAAAAAAAAAAAvU35FgAAACQ5eM8x+f5Fx+eQPcd0O3fj/Yvyvitm57lV62uUDAAAAAAAAAAAAAAAAAAA6E3KtwAAAGCT3UYPzbUfPTbvPGRCt3P3L3wxJ15yRx55dmWNkgEAAAAAAAAAAAAAAAAAAL1F+RYAAAB0MGxIfS553+H51Jv363bu2RXrcspld+Wn85bUKBkAAAAAAAAAAAAAAAAAANAblG8BAABAJ6WUfOrN++eSMw9PU0PXvzq/tKEt5181J5f+8olUVVXDhAAAAAAAAAAAAAAAAAAAwNZSvgUAAABdeNchE/Pd84/N+FFNXc5UVfLPtzyWP7/uwaxvbathOgAAAAAAAAAAAAAAAAAAYGso3wIAAIBuHDppbL5/0fFp3mN0t3M33LcoZ15xd5avXl+jZAAAAAAAAAAAAAAAAAAAwNZQvgUAAACvYvcxQ3Pd+cflHQfv3u3cnAV/yImX3JlHF6+sUTIAAAAAAAAAAAAAAAAAAKCnGvo7AAAAAGwPhg2pzyXvOyL/Mn5+Lr5tfpdzi15cm1MuvStnHTslu45sytjhQ7LT8MaMHT4kY4c3ZqfhQzJmWGPq60oN0wMAAAAAAAAAAAAAAAAAAC9TvgUAAABbqK6u5NN/un/2HT8yf3Hdg9nQ2r7ZuTUb2nL5r57s8p5SktFDGzN2UynXTh1KuXYaPiQ7jdhU1rVpvXGuMSObGlKK0i4AAAAAAAAAAAAAAAAAANgWyrcAAACgh044dGImjxue8751b55btb7H56sqWbG2JSvWtmTB8y9t8bnG+pIxwzaWdf2X4q5Nf+60aX/s8D8WdzU11Pc4IwAAAAAAAAAAAAAAAAAADFbKtwAAAGArHDZpbL5/0cx85N/vzbxnV9bkzZa2KstXr8/y1T0r/Bo+pD5jh20q6xqxqZxr2B/LuV7+s2OZ1+hhjamvK330kwAAwP9l516D9Lrr+4B/z950t26WVpeVjfFdK+PLCggmYAeDRBwKBLRpJplkOsl02qSXdJpJQzPTXKbttJmmk2badCadzrRvQpquICaEEAkIARqoqVb4opUM+ALWSlpdretK2tvpC8kg1kdY1q7OPvvs5/Nm5/x+5/zP93lePjP7BQAAAAAAAAAAAAAAmDnKtwAAAOA6rV26IH3/8B351f/9dD67Z2im41zV8Mh4hkfGc/DUhWt+piiSm+a3Z/nC9iy9opRr2cL2LFvw/RKv5Zevly1sz/JFHVnU0ZqiUNoFAAAAAAAAAAAAAAAAAEDjUr4FAAAAU7Cwoy1/+DMP5fc//638579+fqbjTJuyTE6dH82p86PJ8eFrfq69tcjSBZPKuha25+bF8/LAhmV5152rsqCj9QYmBwAAAAAAAAAAAAAAAACAH075FgAAAExRS0uRX91ydx69e3X+4pmDOXjyfF4ZHs2p4dG8MjySk8OjGRmfmOmYtRgdL3Ps7MUcO3uxcj+/vSWP3LUqWzauyWP3rs6yhR01JwQAAAAAAAAAAAAAAAAAYK5TvgUAAADTpOfW5em5dflr5mVZZnhkPCfPj+aVc5fKuF4ZHsnJ86M5eW4krwyP5uTl61fLul4ZHsmp86Mpyxn4IDfQhdGJ7Bg4nB0Dh9PaUuTtt63Ilo2d2dK9JuuWLZjpeAAAAAAAAAAAAAAAAAAAzAHKtwAAAOAGK4oii+a1ZdG8tqx/AwVTExNlTl8YzSuvlnFd/vu9sq7hHyzrOnl5fm5k/AZ+mukzPlHmqy8cz1dfOJ7f/vTe3Ld+abZ2XyriunP14hRFMdMRAQAAAAAAAAAAAAAAAABoQsq3AAAAoEG1tBRZtrAjyxZ25LYsuubnLo6N59T50UulXOculXWdOj/yvRKvk+dGc/L890u8Xv07Ol7ewE/z+p49cCrPHjiV39v5rdx286Js2XipiOvBDcvS0qKICwAAAAAAAAAAAAAAAACA6aF8CwAAAJrMvLbWrF7SmtVL5l/zM2VZZnhk/FI51/Dl4q7hkSvKuV4t6nq1zOvS/tT50ZQ3oLPrpWPn8kdffjF/9OUXs2rJvLxvY2e2bOzMO25fmXltrdP/QgAAAAAAAAAAAAAAAAAA5gzlWwAAAECKosiieW1ZNK8tXcuv/bnxiTKnz4/m5Pnvl3VdKu76flnX8bMjefKlEzlxbuS6sh09czEff/LlfPzJl7NkXlsevWd1tmzszKN3r8qS+e3XdSYAAAAAAAAAAAAAAAAAAHOX8i0AAADgurW2FFm+qCPLF3Xktiy66n1j4xPp/+4r2TFwODv3DmXwlfPX9b4zF8fy6acP5tNPH0xHa0sevmNltnavyXvv7cyqJfOu92MAAAAAAAAAAAAAAAAAADCHFGVZznQGmlBRFN1J9rx6vWfPnnR3d89gIgAAABpFWZbZd+hMdgwMZefew9l36PSUzyyKpOeW5dnS3Zmt3Wty68qrF4EBAAAAAAAAAAAAAAAAADDzBgYGsmnTpitHm8qyHKjj3cq3uCGUbwEAAHCtXj4+nJ17h7Jz4HB2ffdEJqbhp4q7O5dka3dntnSvSfe6m1IUxdQPBQAAAAAAAAAAAAAAAABg2ijfouko3wIAAOB6HDt7MV/Ydzg7Bw7nK88fy8jYxJTPXL9sQd63sTNbujvztjetSFtryzQkBQAAAAAAAAAAAAAAAABgKpRv0XSUbwEAADBV5y6O5UvfOpqdA0P5wnNHcubC2JTPXLawPY/d05mt3Z15152rsqCjdRqSAgAAAAAAAAAAAAAAAADwRs1k+VZbHS8BAAAAeKMWzWvL4/etzeP3rc3I2ESefOl4dgwM5XN7D+fw6YvXdebJ4dF8YvdgPrF7MPPbW/LIXauyZeOaPHbv6ixb2DHNnwAAAAAAAAAAAAAAAAAAgEZUlGU50xloQkVRdCfZ8+r1nj170t3dPYOJAAAAaBYTE2WeOXAqOwaGsmNgKC8ePTflM1tbirz9thXZ2r0m79vYmXXLFkxDUgAAAAAAAAAAAAAAAAAArmZgYCCbNm26crSpLMuBOt6tfIsbQvkWAAAAdXn+yNns3DuUHQOH8/T+k9Ny5n3rl2Zrd2e2dK/JnasXpyiKaTkXAAAAAAAAAAAAAAAAAIBLlG/RdJRvAQAAMBOGTl3I5/YOZefew/naC8czNjH13z1uu3lRtmy8VMT14IZlaWlRxAUAAAAAAAAAAAAAAAAAMFXKt2g6yrcAAACYaaeGR/PFbx7JjoGhfOlbRzM8Mj7lM1ctmZf3bezMlo2defj2m9PR1jINSQEAAAAAAAAAAAAAAAAA5h7lWzQd5VsAAAA0kguj4/nb549lx8BQPr/vSE6cG5nymUvmteXRe1Zna3dnHrlrVZbMb5+GpAAAAAAAAAAAAAAAAAAAc8NMlm+11fESAAAAgJk0v701j93bmcfu7cz4RJld3zmRnXsPZ8fAUAZfOX9dZ565OJZPP30wn376YDpaW/LwHSuztXtN3ntvZ1YtmTfNnwAAAAAAAAAAAAAAAAAAgOlSlGU50xloQkVRdCfZ8+r1nj170t3dPYOJAAAA4LXKssy+Q2eyY2AoO/cezr5Dp6d8ZlEkPbcsz5buzmztXpNbVy6ahqQAAAAAAAAAAAAAAAAAAM1lYGAgmzZtunK0qSzLgTrerXyLG0L5FgAAALPR/hPD3yvi2vWdE5mYhp9N7u5ckq3dndnSvSbd625KURRTPxQAAAAAAAAAAAAAAAAAYJZTvkXTUb4FAADAbHfs7MX89b4j2TEwlK88fywjYxNTPnP9sgV538bObO1ek7e+aXnaWlumISkAAAAAAAAAAAAAAAAAwOyjfIumo3wLAACAZnLu4li+9K2j2TkwlC88dyRnLoxN+cxlC9vzE/etza88dmdW3zR/GlICAAAAAAAAAAAAAAAAAMweM1m+1VbHSwAAAABms0Xz2vL4fWvz+H1rMzI2kSdfOp6dA4ezc+9QDp++eF1nnhwezR8/+XJ2DAxlxz97d1YunjfNqQEAAAAAAAAAAAAAAAAAqNIy0wEAAAAAZpOOtpa8685V+dcf3pSvfeyxPPGP3plfevT23L5q0XWdd+zsSP7gC9+e5pQAAAAAAAAAAAAAAAAAAFxN20wHAAAAAJitWlqKPLBhWR7YsCy//v578vyRs9m5dyg7Bw7nqf0nr/mcJ75xIL/x+L2Z3956A9MCAAAAAAAAAAAAAAAAAJAo3wIAAACYNnesXpw7Vt+RX370jgydupDP7TucnQND+doLxzM2UV71udMXxrJz7+F88P51NaYFAAAAAAAAAAAAAAAAAJiblG8BAAAA3ABrls7Pz/3Irfm5H7k1p86P5ovPHcnOvUP5/N4jGRmfeM39fbv2K98CAAAAAAAAAAAAAAAAAKhBy0wHAAAAAGh2Sxe058MPrs9//dme/N23bqi85/88fywHT56vORkAAAAAAAAAAAAAAAAAwNyjfAsAAACgRr2buyrnZZl8cvdgzWkAAAAAAAAAAAAAAAAAAOYe5VsAAAAANbpv/dLc3bmkcre9fzBlWdacCAAAAAAAAAAAAAAAAABgblG+BQAAAFCjoijSu7mrcved48P5f995peZEAAAAAAAAAAAAAAAAAABzi/ItAAAAgJp9+MH1aWspKnd9u/bXnAYAAAAAAAAAAAAAAAAAYG5RvgUAAABQs5sXz8uP3bO6cveZZw/l3MWxmhMBAAAAAAAAAAAAAAAAAMwdyrcAAAAAZkBvT1flfHhkPH/57KGa0wAAAAAAAAAAAAAAAAAAzB3KtwAAAABmwI/dszo3L+6o3G3vH6w5DQAAAAAAAAAAAAAAAADA3KF8CwAAAGAGtLe25MMPrK/cPfnSibx8fLjmRAAAAAAAAAAAAAAAAAAAc4PyLQAAAIAZ0rt5w1V32/v315gEAAAAAAAAAAAAAAAAAGDuUL4FAAAAMEPuXrMkb+laWrn7xO4DmZgoa04EAAAAAAAAAAAAAAAAAND8lG8BAAAAzKBtPV2V8wMnz+erLxyvOQ0AAAAAAAAAAAAAAAAAQPNTvgUAAAAwgz54/7p0tFb/RNPXv7/mNAAAAAAAAAAAAAAAAAAAzU/5FgAAAMAMWrawI+/r7qzc/dWeoZw6P1pzIgAAAAAAAAAAAAAAAACA5qZ8CwAAAGCG9fZ0Vc4vjk3kL545WHMaAAAAAAAAAAAAAAAAAIDmpnwLAAAAYIa9685VWXPT/Mpd367BmtMAAAAAAAAAAAAAAAAAADQ35VsAAAAAM6y1pchHHlpfuXtq/8k8f+RMzYkAAAAAAAAAAAAAAAAAAJqX8i0AAACABrCtp+uqu75dgzUmAQAAAAAAAAAAAAAAAABobsq3AAAAABrAm1ctzuZbl1fuPvmNAxkbn6g5EQAAAAAAAAAAAAAAAABAc1K+BQAAANAgejd3Vc6PnrmYL3/7aM1pAAAAAAAAAAAAAAAAAACak/ItAAAAgAbxE29ZlwXtrZW7vl2DNacBAAAAAAAAAAAAAAAAAGhOyrcAAAAAGsTieW358U1rKnef33c4J86N1JwIAAAAAAAAAAAAAAAAAKD5KN8CAAAAaCDbNndVzkfHy3zqqQM1pwEAAAAAAAAAAAAAAAAAaD7KtwAAAAAayI/ctjJdyxdU7vp2DdacBgAAAAAAAAAAAAAAAACg+SjfAgAAAGggLS1FtvV0Ve72HjqdgYOnak4EAAAAAAAAAAAAAAAAANBclG8BAAAANJiPPlRdvpUkfbsGa0wCAAAAAAAAAAAAAAAAANB8lG8BAAAANJgNKxbm4dtXVu4+9dSBjIxN1JwIAAAAAAAAAAAAAAAAAKB5KN8CAAAAaEC9m7sq568Mj+YL+w7XnAYAAAAAAAAAAAAAAAAAoHko3wIAAABoQO/vXpsl89oqd339gzWnAQAAAAAAAAAAAAAAAABoHsq3AAAAABrQgo7WfOD+tZW7v/nmkRw5faHmRAAAAAAAAAAAAAAAAAAAzUH5FgAAAECD2tazoXI+USZ/9o0DNacBAAAAAAAAAAAAAAAAAGgOyrcAAAAAGtRDtyzLm1ctqtz19Q+mLMuaEwEAAAAAAAAAAAAAAAAAzH7KtwAAAAAaVFEU2dbTVbl7/sjZPLX/ZM2JAAAAAAAAAAAAAAAAAABmP+VbAAAAAA3sow91paWo3vX1D9YbBgAAAAAAAAAAAAAAAACgCSjfAgAAAGhgnTfNz7vvWlW5+/TTB3NhdLzmRAAAAAAAAAAAAAAAAAAAs5vyLQAAAIAG19uzoXJ+5sJYdgwM1ZwGAAAAAAAAAAAAAAAAAGB2U74FAAAA0ODeu3F1li1sr9z17RqsOQ0AAAAAAAAAAAAAAAAAwOymfAsAAACgwc1ra82H7l9XufvbF47lwMnzNScCAAAAAAAAAAAAAAAAAJi9lG8BAAAAzAK9mzdUzssy+UT/YM1pAAAAAAAAAAAAAAAAAABmL+VbAAAAALNA97qbcs+aJZW77f2DmZgoa04EAAAAAAAAAAAAAAAAADA7Kd8CAAAAmAWKokjv5g2Vu5dPDOfr3zlRcyIAAAAAAAAAAAAAAAAAgNlJ+RYAAADALPHhB9alraWo3G3vH6w5DQAAAAAAAAAAAAAAAADA7KR8CwAAAGCWWLl4Xt5zz+rK3V8+eyjnLo7VnAgAAAAAAAAAAAAAAAAAYPZRvgUAAAAwi/Ru3lA5Hx4Zz2eePVRzGgAAAAAAAAAAAAAAAACA2Uf5FgAAAMAs8ujdq3Lz4o7K3fZdgzWnAQAAAAAAAAAAAAAAAACYfZRvAQAAAMwi7a0t+ckH11fuvv6dE/nOsXM1JwIAAAAAAAAAAAAAAAAAmF2UbwEAAADMMr2bN1x1t71/sMYkAAAAAAAAAAAAAAAAAACzj/ItAAAAgFnmrs4lub9raeXuE7sHMz5R1pwIAAAAAAAAAAAAAAAAAGD2UL4FAAAAMAtt27yhcn7o1IX87fPHak4DAAAAAAAAAAAAAAAAADB7KN8CAAAAmIU++JZ16Wir/mmnr3+w5jQAAAAAAAAAAAAAAAAAALOH8i0AAACAWWjpwvZs7V5TudsxMJRTw6M1JwIAAAAAAAAAAAAAAAAAmB2UbwEAAADMUr09XZXzkbGJ/PkzB2tOAwAAAAAAAAAAAAAAAAAwOyjfAgAAAJil3nnHzVm7dH7lbnv/YM1pAAAAAAAAAAAAAAAAAABmB+VbAAAAALNUa0uRjzy0vnL39P6T+fbhMzUnAgAAAAAAAAAAAAAAAABofMq3AAAAAGaxbT0brrrr6x+sMQkAAAAAtApatgAAIABJREFUAAAAAAAAAAAAwOygfAsAAABgFrvt5kV565uWV+4+uftARscnak4EAAAAAAAAAAAAAAAAANDYlG8BAAAAzHK9PRsq58fOXsyXvnm05jQAAAAAAAAAAAAAAAAAAI1N+RYAAADALPf4W9ZmQXtr5a6vf3/NaQAAAAAAAAAAAAAAAAAAGpvyLQAAAIBZbvG8tjx+39rK3Rf2HcnxsxdrTgQAAAAAAAAAAAAAAAAA0LiUbwEAAAA0gd7NXZXzsYkyTzx1sOY0AAAAAAAAAAAAAAAAAACNS/kWAAAAQBN4+20rcsuKhZW7vl37U5ZlzYkAAAAAAAAAAAAAAAAAABqT8i0AAACAJlAURbb1dFXunhs6k4GDp2tOBAAAAAAAAAAAAAAAAADQmJRvAQAAADSJj/Z0pSiqd3279tcbBgAAAAAAAAAAAAAAAACgQSnfAgAAAGgS65ctyMO3r6zcferpg7k4Nl5zIgAAAAAAAAAAAAAAAACAxqN8CwAAAKCJ9PZsqJyfHB7NF/YdqTkNAAAAAAAAAAAAAAAAAEDjUb4FAAAA0ES2dq/Jknltlbu+XftrTgMAAAAAAAAAAAAAAAAA0HiUbwEAAAA0kQUdrfnA/esqd1/61tEcPn2h5kQAAAAAAAAAAAAAAAAAAI1F+RYAAABAk+nd3FU5nyiTT+4+UHMaAAAAAAAAAAAAAAAAAIDGonwLAAAAoMk8uGFZbl+1qHLX178/ZVnWnAgAAAAAAAAAAAAAAAAAoHEo3wIAAABoMkVRpHfzhsrdi0fPZffLJ2tOBAAAAAAAAAAAAAAAAADQOJRvAQAAADShjzy4Pq0tReVue//+mtMAAAAAAAAAAAAAAAAAADQO5VsAAAAATWj1TfPzyF2rKneffvpQzo+M15wIAAAAAAAAAAAAAAAAAKAxKN8CAAAAaFK9PV2V87MXx/JXA4dqTgMAAAAAAAAAAAAAAAAA0BiUbwEAAAA0qcfu7czyhe2Vu75dgzWnAQAAAAAAAAAAAAAAAABoDMq3AAAAAJpUR1tLPvTA+srdV184nv0nhmtOBAAAAAAAAAAAAAAAAAAw85RvAQAAADSxbT1dV919cveBGpMAAAAAAAAAAAAAAAAAADQG5VsAAAAATWzT+qW5d+1Nlbvtu/dnYqKsOREAAAAAAAAAAAAAAAAAwMxSvgUAAADQ5Hp7uirn+0+cz5Mvnag5DQAAAAAAAAAAAAAAAADAzFK+BQAAANDkPvzg+rS3FpW7vv79NacBAAAAAAAAAAAAAAAAAJhZyrcAAAAAmtyKRR157J7Oyt1nnx3K2YtjNScCAAAAAAAAAAAAAAAAAJg5yrcAAAAA5oDezV2V8/Oj4/nMMwdrTgMAAAAAAAAAAAAAAAAAMHOUbwEAAADMAY/ctSqrlsyr3PXtGqw5DQAAAAAAAAAAAAAAAADAzFG+BQAAADAHtLW25CMPrq/c7fruK3nx6NmaEwEAAAAAAAAAAAAAAAAAzAzlWwAAAABzxLaerqvutvcP1pgEAAAAAAAAAAAAAAAAAGDmKN8CAAAAmCPu7FyS+zcsq9x9cveBjE+UNScCAAAAAAAAAAAAAAAAAKif8i0AAACAOaS3p6tyPnT6Qr7y7aM1pwEAAAAAAAAAAAAAAAAAqJ/yLQAAAIA55O/cvy7z2qp/EurrH6w5DQAAAAAAAAAAAAAAAABA/ZRvAQAAAMwhSxe0Z2v3msrd5wYO59TwaM2JAAAAAAAAAAAAAAAAAADqpXwLAAAAYI7p3dxVOR8Zn8ifP32g5jQAAAAAAAAAAAAAAAAAAPVSvgUAAAAwxzx8+81Zt3R+5a6vf7DmNAAAAAAAAAAAAAAAAAAA9VK+BQAAADDHtLYU+WhPV+XumcFT+ebQmZoTAQAAAAAAAAAAAAAAAADUR/kWAAAAwBy07SrlW0nSt2t/jUkAAAAAAAAAAAAAAAAAAOqlfAsAAABgDrp15aK87bYVlbsnnjqQ0fGJmhMBAAAAAAAAAAAAAAAAANRD+RYAAADAHNXb01U5P3Z2JF987kjNaQAAAAAAAAAAAAAAAAAA6qF8CwAAAGCOevy+tVnY0Vq56+sfrDkNAAAAAAAAAAAAAAAAAEA9lG8BAAAAzFGL5rXl8fvWVu6++NyRHDt7seZEAAAAAAAAAAAAAAAAAAA3nvItAAAAgDmst6ercj42UeaJbxyoOQ0AAAAAAAAAAAAAAAAAwI2nfAsAAABgDnvbbSty68qFlbu+XYMpy7LmRAAAAAAAAAAAAAAAAAAAN5byLQAAAIA5rCiKbHuoq3L3zcNnsufA6ZoTAQAAAAAAAAAAAAAAAADcWMq3AAAAAOa4j/Z0pSiqd339++sNAwAAAAAAAAAAAAAAAABwgynfAgAAAJjj1i1bkB+94+bK3aeeOpgLo+M1JwIAAAAAAAAAAAAAAAAAuHGUbwEAAACQbT1dlfNT50fz+X2Ha04DAAAAAAAAAAAAAAAAAHDjKN8CAAAAIFu712TJ/LbKXd+uwZrTAAAAAAAAAAAAAAAAAADcOMq3AAAAAMj89tZ88P51lbuvfPtohk5dqDkRAAAAAAAAAAAAAAAAAMCNoXwLAAAAgCRJ7+YNlfOJMvnE7sGa0wAAAAAAAAAAAAAAAAAA3BjKtwAAAABIktzftTR3rF5cudveP5iyLGtOBAAAAAAAAAAAAAAAAAAw/ZRvAQAAAJAkKYoivT1dlbuXjp1L/3dfqTkRAAAAAAAAAAAAAAAAAMD0U74FAAAAwPf85EPr09pSVO76dg3WnAYAAAAAAAAAAAAAAAAAYPop3wIAAADge1YvmZ9H71pVufuLZw5meGSs5kQAAAAAAAAAAAAAAAAAANNL+RYAAAAAP6B3c1fl/NzIeP5qz1DNaQAAAAAAAAAAAAAAAAAAppfyLQAAAAB+wHvu6cyKRR2Vu75dgzWnAQAAAAAAAAAAAAAAAACYXsq3AAAAAPgBHW0t+dAD6yp3X3vxePafGK45EQAAAAAAAAAAAAAAAADA9FG+BQAAAMBr9PZsuOpue/9gjUkAAAAAAAAAAAAAAAAAAKaX8i0AAAAAXmPjupvSve6myt32/sFMTJQ1JwIAAAAAAAAAAAAAAAAAmB7KtwAAAACo1NvTVTk/cPJ8/u+Lx2tOAwAAAAAAAAAAAAAAAAAwPdpmOsC1KIritiQPJFmXZHGSQ0m+m+SrZVmOzkCeBUnuTXJPklWXM51NciLJniTPlmU5Ns3vXJbk4STrk9yc5FiSA7n0HZyczncBAAAAJMmHHliff/uX+zI6Xr5m19c/mIfvuHkGUgEAAAAAAAAAAAAAAAAATE1Dl28VRbEtyT9P8o6r3HKiKIo/TfKbZVkeu8FZHkry4STvSfK2JO0/5PZzl3P9QVmWz0zxvQ8m+c0kjyfpqLjlYlEUn03yO2VZPjWVdwEAAABcafmijrz33s58ds/Qa3af3XMov/Oh7tw0/4f9RAIAAAAAAAAAAAAAAAAA0HhaZjpAlaIoFhdF8SdJ+nL14q0kWZHkl5LsKYpi6w3KMr8oiheS9Cf5V0nemR9evJUki5L8QpLdRVH8+6Ioruu/UIui+FiSJ3Op9KuqeCtJ5l3eP1kUxb+4nvcAAAAAXE3v5q7K+YXRiXzmmUM1pwEAAAAAAAAAAAAAAAAAmLqGK98qiqI1yZ8m+elJq6NJduZSIdfuJOUVu84knyqK4kdvQKS2JG+umJdJnruc6eNJ/jzJi5PuaU3y60n+V1EUbW/kpUVR/EaSf5cfLPo6n+TLufT9fCXJhSt2HUl+tyiKX3sj7wEAAAD4Yd5956qsXjKvcte3a3/NaQAAAAAAAAAAAAAAAAAApq7hyreS/Pskj19xPZrknyTpKstya1mWP1WWZU+STUm+dsV985I8URTF2huYbTzJZ3OpGGx1WZb3Xs70s2VZfqgsy9uTbM6lgqwrfSTJb1/rS4qi+ECSfzNp/N+S3FKW5SNlWf50WZbvTnJLkv8+6b7fLYri/df+kQAAAACurq21JT/50PrK3e6XT+b5I2drTgQAAAAAAAAAAAAAAAAAMDUNVb5VFMWbk/zKpHFvWZb/pSzLkSuHZVnuTfJYfrCAa2WS37oB0S4m+cMkbyrL8vGyLP+0LMtjVTeWZdmf5D1J/mTS6teKorj19V5UFEVrkt9LUlwx/v2yLP/B5HeWZXm0LMu/n+Q/XXlEkv94+RwAAACAKevt2XDV3fb+wRqTAAAAAAAAAAAAAAAAAABMXUOVb+VScVb7Fdf/syzLT13t5rIszyf5e0muLOb6xcslXtPlQpI7yrL8x2VZXtN/k5ZlOZ7kF5Psv2LckeSnruHxn09y9xXX30zyL1/nmY9dvu9VG5P87DW8CwAAAOB13bF6cR68ZVnl7s++MZjxibLmRAAAAAAAAAAAAAAAAAAA169hyreKoliQZNuk8e++3nNlWX4ryRNXjNqS/Mx05SrLcuxaS7cmPXc+yf+YNP6xa3j05ydd/35Zlhdf510Xk/zB65wDAAAAcN16ezZUzg+fvpgvf/tozWkAAAAAAAAAAAAAAAAAAK5fw5RvJdmaZOEV118ry/K5a3x2csnVR6Yn0pR9Y9L1uh92c1EUK5O864rRSJKPX+O7/jjJ6BXXjxRFseIanwUAAAD4oT5w/9rMb6/+KWn7rjfcWw4AAAAAAAAAAAAAAAAAMGMaqXzr/ZOu/+YNPPuVJGNXXD9YFEXnlBNN3dik647Xuf99SVqvuO4vy/LMtbyoLMvTSXZfMWq7fB4AAADAlN00vz3v715Tufvc3sM5OTxScyIAAAAAAAAAAAAAAAAAgOvTSOVbmyZdf+1aHyzL8lySZyeNu6ecaOrumHR96HXuv+7v4LKvTrpuhO8AAAAAaBK9mzdUzkfGJ/Kppw7WnAYAAAAAAAAAAAAAAAAA4Po0UvnWvZOun3+Dz78w6XrjFLJMl22Trr/+OvdPztwM3wEAAADQJN7x5pVZv2xB5a6vf3/NaQAAAAAAAAAAAAAAAAAArk9DlG8VRbEiyYpJ45ff4DGT77/z+hNNXVEUb03yzknjP3udx+6YdD2rvwMAAACgubS0FPnoQ+srd3sOnM6+Q6drTgQAAAAAAAAAAAAAAAAA8MY1RPlWkmWTrofLsjz3Bs84Mul66RTyTElRFO1J/mjS+CtlWX79dR6d/D1M/kyvp2G+AwAAAKA5bevZcNVd367BGpMAAAAAAAAAAAAAAAAAAFyftpkOcNniSdfnr+OMyc8suc4s0+E/JHnwiuvRJP/0Gp6b6vdwQ76DoihWJ1n1Bh+7fTreDQAAADSWW1YuzNtvW5EnXzrxmt0TTx3Ix378nnS0NUrfOwAAAAAAAAAAAAAAAADAazVq+daF6zhjcvHU5DNrURTFLyT5lUnj3y7L8qlreHyq38ON+g5+OclvTdNZAAAAwCzXu3lDZfnWiXMj+evnjuT9m9b8f3buNUiv+jAP+HP2Ii26rVYSuu5KgGRzMQiEFNuAwVLiJCROghOjOHGSiTOJYxv6oellmsmkM22nSaadyXQ6jSG2J07jpI0dEdu0MbUTxwgMxuYmcTMXS0LSSkIXJK3QBUl7Of0gActydH91dlf7+83svJz/s+ecR/vxneEZhlYAAAAAAAAAAAAAAAAAAKenabgLnEBZ0z0NVRTFrUn+fMjxPyT5k7N85Jn+m4b9bwAAAABc+H72mtmZOK65Mrvnie6a2wAAAAAAAAAAAAAAAAAAnJmRMr51YMj1RWfxjKH3DH3meVUUxU1J/j5J66Djh5J8rCzL0x3FOte/w7D+DQAAAICxYcK4lnx48ZzK7P4Xd2XX/iM1NwIAAAAAAAAAAAAAAAAAOH0tw13guFE9vlUUxdIk30gyYdDxo0k+XJbloTN41IEkHYOuR8r41l1JVp3hPQuT3Nug9wMAAAAjzMplXfm7x7e847x/oMzX12zNJ2+5bBhaAQAAAAAAAAAAAAAAAACc2kgZ39o35HpCURQTy7I8eAbPmDnkuuccO52WoigWJ/nHJO2Djtck+emyLF87w8ftS9I16PriM7z/vPwNyrLcmWTnmdxTFEUjXg0AAACMUMsWdOTSGRPz8qvv/Ppm1RPd+Z2bL/X9AAAAAAAAAAAAAAAAAAAwIjUNd4EkKctyd5K9Q47nn+FjFgy5/tHZNzo9RVFcleTbSaYNOn42yU+VZXk2w1dDOw/9N51K7X8DAAAAYGwqiiK3L+2szF7acSBPbxm6tQ4AAAAAAAAAAAAAAAAAMDKMiPGt454fcr3oDO+/7BTPa6iiKC5P8s9JLh50/EKSD5Vl+epZPnZU/Q0AAACAse2Xrp+XoqjOVj3RXW8ZAAAAAAAAAAAAAAAAAIDTNJLGt54dcn3D6d5YFMXEJItP8byGKYpiUZLvJJk96PhHSX68LMsd5/Dos/4bHHfTKZ4HAAAA0DBz2i/KBxbNqMz+z9ptOdzbX3MjAAAAAAAAAAAAAAAAAIBTG0njW98ccr38DO69OUnLoOs15ziCdUJFUVyaY8Nbcwcdb8ix4a1XzvHx/5Rk8P+VurQoismn2WtykusHHfUdfx4AAADAebNyWVfl+WuH+/KPPzwvX88AAAAAAAAAAAAAAAAAAJyTkTS+9a0krw+6vqEoiitO895PDLn+WkMaDVEUxfwcG94a/H+Vbsqx4a0t5/r8sixfTfLQoKNxST5+mrf/WpLWQdcPlmW551w7AQAAAJzMT101K1PaWiqzVY9319wGAAAAAAAAAAAAAAAAAODURsz4VlmWh5LcM+T4353qvqIo3p3kFwcd9SX53w2s9sZ75ib55ySXDDremmPDW5sa+KovDbn+vaIoxp+i2/gk/3LI8V81sBMAAABApbbW5vzCdXMrs4fWvZptPa9XZgAAAAAAAAAAAAAAAAAAw2XEjG8d9x+S9A66/kRRFL9wol8uiqItyV8mGTfo+C/Kslx/spcURVEO+Vl+it+fmWPDW4sGHb+SZEVZlhtOdu9Z+KskLw66vjzJH5/inj85/ntv+GGS/9XgXgAAAACVVi7tqjwvy+SrT26puQ0AAAAAAAAAAAAAAAAAwMmNqPGt40NW/33I8T1FUfyLoigGD2ylKIorc2wQ68ZBx7uT/MdGdiqKYmqSf0pyxaDjg0l+O0lvURSXnMnPqd5XlmV/kn+TpBx0/K+KovhcURTTh3SbURTF55P83uBHJPnXx58DAAAAcN4t7mzPu2dNqszueWJLyrKszAAAAAAAAAAAAAAAAAAAhkPLcBeo8PtJ3pPkZ45ftyb5H0n+fVEUTybZn+SyJNcnKQbddzTJL5Zl+UqD+1yXZPGQs4lJ7jvL5xWn+oWyLP+hKIo/TPJHg45/N8lvFEXxgyTbk8xJ8t4kFw25/ffLsvzmWXYDAAAAOGNFUWTl0q780X3PvyPbuPtQHt+0Nz92ybRhaAYAAAAAAAAAAAAAAAAA8E5Nw11gqLIs+5P8cpKvDIlmJrk1ycokS/P2EaudSW4ry/K7tZSsQVmWf5zkD5L0Djq+KMnyJL+S5IN5+/BWb44Nb/3XujoCAAAAvOEjS+alual6c3zV4901twEAAAAAAAAAAAAAAAAAOLERN76VJGVZHijL8ldybGjr+yf51T1J7k5ydVmW36ylXI3KsvyTJO9Lcm+Soyf4taPH8/eWZflf6uoGAAAAMNjFk8dnxeUzK7NvPP1KDh3tq7kRAAAAAAAAAAAAAAAAAEC1luEucDJlWd6T5J6iKC5Ncn2SuUkmJtmeZFOSh8uyPNEo1cmeW5zB765Octq/32hlWa5J8pGiKDqS3JhkXpLpSXYn2Zrke2VZ7h2ufgAAAABvWLmsM99+fsc7zg8e7c99z2zP7Us7h6EVAAAAAAAAAAAAAAAAAMDbjejxrTeUZflykpeHu8dwOj6w9Y3h7gEAAABwIisun5lpE8dlz8F3bqWverzb+BYAAAAAAAAAAAAAAAAAMCI0DXcBAAAAAC4M41qa8pHr5lVmP3h5TzbvPlRzIwAAAAAAAAAAAAAAAACAdzK+BQAAAEDDrFzWecLsnie6a2wCAAAAAAAAAAAAAAAAAFDN+BYAAAAADXPlnCm5et6Uyuzvn9yagYGy5kYAAAAAAAAAAAAAAAAAAG9nfAsAAACAhlq5tKvyfGvP6/ne+t01twEAAAAAAAAAAAAAAAAAeDvjWwAAAAA01G3Xzc245uqvnVY90V1zGwAAAAAAAAAAAAAAAACAtzO+BQAAAEBDTZ0wLj951azK7JvPbs++13trbgQAAAAAAAAAAAAAAAAA8BbjWwAAAAA03O3LOivPj/QN5B+e3lZzGwAAAAAAAAAAAAAAAACAtxjfAgAAAKDhbnnXxZk1ZXxlds8TW2puAwAAAAAAAAAAAAAAAADwFuNbAAAAADRcc1ORX7q+szJbs7kn63bur7kRAAAAAAAAAAAAAAAAAMAxxrcAAAAAOC9uX1o9vpUkq57YUmMTAAAAAAAAAAAAAAAAAIC3GN8CAAAA4LxYePGkXD9/amX21Se3pq9/oOZGAAAAAAAAAAAAAAAAAADGtwAAAAA4j1Yu66o837X/SB780a6a2wAAAAAAAAAAAAAAAAAAGN8CAAAA4Dz6ucVz0tZa/RXUqse31NwGAAAAAAAAAAAAAAAAAMD4FgAAAADn0eS21vzM1XMqs28/vyN7Dh6tuREAAAAAAAAAAAAAAAAAMNYZ3wIAAADgvFq5tLPyvLe/zL1rt9bcBgAAAAAAAAAAAAAAAAAY64xvAQAAAHBevf+y6ensuKgyW/X4lprbAAAAAAAAAAAAAAAAAABjnfEtAAAAAM6rpqYiH72+szL74Suv5blt+2puBAAAAAAAAAAAAAAAAACMZca3AAAAADjvbl9aPb6VJKse31JjEwAAAAAAAAAAAAAAAABgrDO+BQAAAMB51zVtQm64bHpldu/arTnaN1BzIwAAAAAAAAAAAAAAAABgrDK+BQAAAEAtVi7rrDzfe6g333lhR81tAAAAAAAAAAAAAAAAAICxyvgWAAAAALW49erZmTS+pTJb9fiWmtsAAAAAAAAAAAAAAAAAAGOV8S0AAAAAajFhXEs+fM2cymz1S7uyc//hmhsBAAAAAAAAAAAAAAAAAGOR8S0AAAAAarNyWWflef9Ama89ubXmNgAAAAAAAAAAAAAAAADAWGR8CwAAAIDaLF3QkctmTKzMVj2xJWVZ1twIAAAAAAAAAAAAAAAAABhrjG8BAAAAUJuiKPLRpZ2V2bqdB7K2u6fmRgAAAAAAAAAAAAAAAADAWGN8CwAAAIBaffT6zjQV1dmqJ7bUWwYAAAAAAAAAAAAAAAAAGHOMbwEAAABQq9ntbbn5XRdXZv/3qW053NtfcyMAAAAAAAAAAAAAAAAAYCwxvgUAAABA7VYu66w833+4L996bnvNbQAAAAAAAAAAAAAAAACAscT4FgAAAAC1+9CVs9J+UWtlturxLTW3AQAAAAAAAAAAAAAAAADGEuNbAAAAANSurbU5t103tzJ7eP2r+cGG3entH6i5FQAAAAAAAAAAAAAAAAAwFrQMdwEAAAAAxqaVS7vypUc2veO8LJOPff77aWttyuJ5U7Nk/hs/HZk1pW0YmgIAAAAAAAAAAAAAAAAAFxLjWwAAAAAMi6vnTcnlsybnxR37K/PDvQN5dOOePLpxz5tnc9vbsmR+x5uDXO+Z25621ua6KgMAAAAAAAAAAAAAAAAAFwDjWwAAAAAMi6IosnJZZ/7zN54/7Xu27Tucbc+8km8880qSpLW5yFVzprw1yNXVka5pF6UoivNVGwAAAAAAAAAAAAAAAAAY5YxvAQAAADBsPrJkXv7bP72Ug0f7z+r+3v4yT23Zl6e27Mv//N6xs+kTxx0b4prfkeu6pmZxZ3smt7U2sDUAAAAAAAAAAAAAAAAAMJoZ3wIAAABg2MyYND7/6bar82/veSoDZWOeufvg0Xz7+Z359vM7kyRFkbx75uTjg1zHRrkWXTwpTU1FY14IAAAAAAAAAAAAAAAAAIwqxrcAAAAAGFYfXdqZ6xd05G8f3ZwnN+3NM1v35UjfQMOeX5bJizv258Ud+/Plx7qTJJPHt+TarqlvDnJd19WRaRPHNeydAAAAAAAAAAAAAAAAAMDIZXwLAAAAgGF36YyJ+YOfvTJJcrRvIC9sfy1rNvdkzea9WdPdk027DzX0ffuP9OWhda/moXWvvnl2yfQJWTK/49ggV1dHrpgzOa3NTQ19LwAAAAAAAAAAAAAAAAAw/IxvAQAAADCijGtpyuLOqVncOTW/eeMlSZLdB45kbXfPsUGu7r15qntfDhzpa+h7N+4+lI27D+Vra7YmSca3NGVxZ/uxQa6uqVkyvyOz29sa+k4AAAAAAAAAAAAAAAAAoH7GtwAAAAAY8aZPGp+fuHJWfuLKWUmS/oEy63cdyJrNe48Ncm3uyUs796csG/fOI30DeWzj3jy2ce+bZ3Pa27Jk/tRcd3yM65p57WlrbW7cSwEAAAAAAAAAAAAAAACA8874FgAAAACjTnNTkXfPmpx3z5qcj/3Y/CTJ/sO9eXrLvrcGubp7sufg0Ya+95V9h/PKM9tz3zPbkyQtTUWunDMlS+ZPPfbT1ZEF0yekKIqGvhcAAAAAAAAAAAAAAAAAaBzjWwAAAABcECa3teamRTNy06IZSZKyLLN5z6FjQ1yb92ZNd09+uO219A2UDXtn30CZZ7buyzNb9+VLj2xKknRMaM2S+R1Z0jU1S+Z3ZHFXe6a0tTbsnQAAAAAAAAAAAAAAAADAuTG+BQAAAMAFqSiKLJg+MQumT8xHlsxLkhzu7c+zW/dlzeaerO0+Nsq1bd/hhr5376HefOeFnfnOCzuP90jeNXNSlnR1ZMn8Y4Nci2ZOSnNT0dD3AgAAAAAAAAAAAAAAAACnx/gWAAAAAGNGW2tzll0yLcsumfZoHt9sAAAgAElEQVTm2fZ9h7O2e2/WbO7Jms09eXprTw73DjTsnWWZvLTjQF7acSBfebw7STJpfEuu7WrPkq6OXNc1NdfNn5oZk8Y37J0AAAAAAAAAAAAAAAAAwIkZ3wIAAABgTJvd3pZb2+fk1qvnJEl6+wfy4vb9WbP5+CBXd09efvVgQ9954EhfHl63Ow+v2/3m2fxpE7Jk/tQs6ZqaJfM7cuWcKRnX0tTQ9wIAAAAAAAAAAAAAAAAAxrcAAAAA4G1am5ty9bz2XD2vPb9xw7GzPQeP5qnunmODXN09Wbu5J/uP9DX0vZv3HMrmPYdy79ptSZJxLU25Zl57rpnXniltLRnf2pzxLU1pO9lna1PaWt76fCNraioa2hUAAAAAAAAAAAAAAAAARjPjWwAAAABwCtMmjsuKK2ZmxRUzkyQDA2U2vHogT27uyZrNx0a5XtqxPwNl4955tG8gT2zamyc27T3nZ41rbsr4lqZBA15NGd/SfPLP1ua0Dbpn8PXgz6rRr/HHn9Ns9AsAAAAAAAAAAAAAAACAEcj4FgAAAACcoaamIotmTs6imZPzy8u6kiQHjvTl6S1vjHEdG+TaffDoMDc95mj/QI72D2T/kb5a39vaXLxtjKtqpOsdn28OhJ149GtyW0uumdee1uamWv89AAAAAAAAAAAAAAAAAFwYjG8BAAAAQANMGt+SGxfOyI0LZyRJyrLMlr2v58nNe4+NcXX35Ifb9qW3vxzmpvXp7S/T29+X/Uca/+z2i1rzhx++MiuPj58BAAAAAAAAAAAAAAAAwOkyvgUAAAAA50FRFOmaNiFd0ybktuvmJUkO9/bnuW2vZc3mvVnT3ZO1m3uytef1YW46Ou17vTe//9VnsmD6xLz30mnDXQcAAAAAAAAAAAAAAACAUcT4FgAAAADUpK21OUsXdGTpgo43z3a8djhrNvdkbXdP1mzem6e37Mvrvf3D2HL06B8o86f/+GK+8qkbhrsKAAAAAAAAAAAAAAAAAKOI8S0AAAAAGEazprTl1qtn59arZydJ+voH8uKO/VmzuSdPdfdk5/4jOdLXn8O9AznSN5Ajvf050jeQw4M++wbKYf5XDJ8fvLwnj2/ck2WXTBvuKgAAAAAAAAAAAAAAAACMEsa3AAAAAGAEaWluynvmtuc9c9vz6+9fcFr39PUfH+YaMso19PON/64a8Dr558ge/bpr9fp88RPGtwAAAAAAAAAAAAAAAAA4Pca3AAAAAGCUa2luSktzUyaOr/e9pzv69ebn2Yx+9fXnSO+xs/2H+3K0f+AdPb7zws48t21f3jO3vd4/AAAAAAAAAAAAAAAAAACjkvEtAAAAAOCs1D369d0f7cpv/MWjldndq9fnzz5+fT1FAAAAAAAAAAAAAAAAABjVmoa7AAAAAADA6fjAohlZ3NlemX3jmVeyYdeBmhsBAAAAAAAAAAAAAAAAMBoZ3wIAAAAARoWiKHLH8kWVWVkmn3tgQ82NAAAAAAAAAAAAAAAAABiNjG8BAAAAAKPGT101K4tmTqrMvrpmS7b1vF5zIwAAAAAAAAAAAAAAAABGG+NbAAAAAMCo0dRU5I7lCyuz3v4yX/juhpobAQAAAAAAAAAAAAAAADDaGN8CAAAAAEaVn792bjo7LqrM/vbRzdl94EjNjQAAAAAAAAAAAAAAAAAYTYxvAQAAAACjSmtzUz71wYWV2eHegfzlwxvrLQQAAAAAAAAAAAAAAADAqGJ8CwAAAAAYdVYu7cyMSeMrs796ZGNeO9xbbyEAAAAAAAAAAAAAAAAARg3jWwAAAADAqNPW2pxP3nxpZbb/cF/+5vubam4EAAAAAAAAAAAAAAAAwGhhfAsAAAAAGJV+7f0LMqWtpTL7i+++nNeP9tfcCAAAAAAAAAAAAAAAAIDRwPgWAAAAADAqTRrfkk/cdGlltvvg0fzd4901NwIAAAAAAAAAAAAAAABgNDC+BQAAAACMWr914yWZMK65MvvcA+tztG+g5kYAAAAAAAAAAAAAAAAAjHTGtwAAAACAUatj4rh8/L3zK7Nt+w7n3rVba24EAAAAAAAAAAAAAAAAwEhnfAsAAAAAGNV+5+bLMq65+qvOux9Yn/6BsuZGAAAAAAAAAAAAAAAAAIxkxrcAAAAAgFFtdntbPrq0szLbsOtgvvXc9pobAQAAAAAAAAAAAAAAADCSGd8CAAAAAEa9T3/wsjQV1dln71+XsizrLQQAAAAAAAAAAAAAAADAiGV8CwAAAAAY9RZMn5ifv3ZuZfbcttfywEu7am4EAAAAAAAAAAAAAAAAwEhlfAsAAAAAuCB8ZvnCE2Z33b++xiYAAAAAAAAAAAAAAAAAjGTGtwAAAACAC8IVs6fkQ1fOqswe3bgnj768p+ZGAAAAAAAAAAAAAAAAAIxExrcAAAAAgAvGHSsWnjC7a/W6GpsAAAAAAAAAAAAAAAAAMFIZ3wIAAAAALhjXz+/IjQunV2arX9yVZ7fuq7kRAAAAAAAAAAAAAAAAACON8S0AAAAA4IJy54pFJ8zuXr2+xiYAAAAAAAAAAAAAAAAAjETGtwAAAACAC8qNC6fn2q6pldl9z76S9bsO1NwIAAAAAAAAAAAAAAAAgJHE+BYAAAAAcEEpiiJ3Ll9YmZVl8rkH1tfcCAAAAAAAAAAAAAAAAICRxPgWAAAAAHDB+dCVs/LuWZMqs68+uTVbe16vuREAAAAAAAAAAAAAAAAAI4XxLQAAAADggtPUVOSO5Ysqs76BMl94cEPNjQAAAAAAAAAAAAAAAAAYKYxvAQAAAAAXpJ9bPCdd0y6qzL782Oa8euBIzY0AAAAAAAAAAAAAAAAAGAmMbwEAAAAAF6SW5qZ86paFldnh3oH85cMv19wIAAAAAAAAAAAAAAAAgJHA+BYAAAAAcMG6fWlnLp48vjL70vc25bXDvTU3AgAAAAAAAAAAAAAAAGC4Gd8CAAAAAC5Yba3N+eTNl1Zm+4/05a8f2VRzIwAAAAAAAAAAAAAAAACGm/EtAAAAAOCC9vH3LUj7Ra2V2RcfejmvH+2vuREAAAAAAAAAAAAAAAAAw8n4FgAAAABwQZs0viWfuPGSymz3waP5ymOb6y0EAAAAAAAAAAAAAAAAwLAyvgUAAAAAXPA+ceMlmTCuuTL7/IMbcrRvoOZGAAAAAAAAAAAAAAAAAAwX41sAAAAAwAWvY+K4/Nr75ldm2/YdztfXbq25EQAAAAAAAAAAAAAAAADDxfgWAAAAADAm/M7Nl2Vcc/VXon++en36B8qaGwEAAAAAAAAAAAAAAAAwHIxvAQAAAABjwqwpbbl9WWdltuHVg/nms9trbgQAAAAAAAAAAAAAAADAcDC+BQAAAACMGZ++ZWGaiurss/evS1mW9RYCAAAAAAAAAAAAAAAAoHbGtwAAAACAMWP+9An5hWvnVmY/fOW1rH5pV82NAAAAAAAAAAAAAAAAAKib8S0AAAAAYEz5zPJFJ8zuun9djU0AAAAAAAAAAAAAAAAAGA7GtwAAAACAMeXy2ZPzk1fNqswe27g3j768p+ZGAAAAAAAAAAAAAAAAANTJ+BYAAAAAMObcsXzhCbPP3r+uxiYAAAAAAAAAAAAAAAAA1M34FgAAAAAw5iyZ35GbFk2vzB54aVee3bqv5kYAAAAAAAAAAAAAAAAA1MX4FgAAAAAwJt25fNEJs7tWr6uxCQAAAAAAAAAAAAAAAAB1Mr4FAAAAAIxJNyycnuu6plZm/+/Z7Vm380DNjQAAAAAAAAAAAAAAAACog/EtAAAAAGBMKooid65YVJmVZfLnD6yvuREAAAAAAAAAAAAAAAAAdTC+BQAAAACMWT9xxcxcPmtyZfb1NVuzZe+hmhsBAAAAAAAAAAAAAAAAcL4Z3wIAAAAAxqympiJ3rFhYmfUNlPnCgxtqbgQAAAAAAAAAAAAAAADA+WZ8CwAAAAAY0z58zZzMnzahMvvyY93Ztf9IzY0AAAAAAAAAAAAAAAAAOJ+MbwEAAAAAY1pLc1M+/cGFldmRvoF88eGXa24EAAAAAAAAAAAAAAAAwPlkfAsAAAAAGPM+unReZk4eX5n99SObsu/13pobAQAAAAAAAAAAAAAAAHC+GN8CAAAAAMa88S3N+d1bLqvMDhzpy18/srHWPgAAAAAAAAAAAAAAAACcP8a3AAAAAACS/Op752fqhNbK7IsPb8yho301NwIAAAAAAAAAAAAAAADgfDC+BQAAAACQZOL4lvzWjZdWZnsOHs2XH+2uuREAAAAAAAAAAAAAAAAA54PxLQAAAACA437zxgWZOK65Mvv8gxtytG+g5kYAAAAAAAAAAAAAAAAANJrxLQAAAACA46ZOGJdff/+Cymz7a4fztTVbam4EAAAAAAAAAAAAAAAAQKMZ3wIAAAAAGOS3P3BpxrVUf3V69+r16R8oa24EAAAAAAAAAAAAAAAAQCMZ3wIAAAAAGGTmlLb88rLOymzj7kO575lXam4EAAAAAAAAAAAAAAAAQCMZ3wIAAAAAGOJTtyxMc1NRmX32/nUpy7LmRgAAAAAAAAAAAAAAAAA0ivEtAAAAAIAhuqZNyG3Xzq3MXti+P/e/uLPmRgAAAAAAAAAAAAAAAAA0ivEtAAAAAIAKn1m+8ITZZ+9fn7Isa2wDAAAAAAAAAAAAAAAAQKMY3wIAAAAAqPCuWZPz0++ZVZk9sWlvHn15T82NAAAAAAAAAAAAAAAAAGgE41sAAAAAACdwx/JFJ8w+u3p9jU0AAAAAAAAAAAAAAAAAaBTjWwAAAAAAJ3Bt19Tc/K4ZldmDL+3KM1v21dwIAAAAAAAAAAAAAAAAgHNlfAsAAAAA4CTuWL7ohNldq9fV2AQAAAAAAAAAAAAAAACARjC+BQAAAABwEu+/bFqunz+1Mvvmc9uzbuf+mhsBAAAAAAAAAAAAAAAAcC6MbwEAAAAAnERRFLlzxaLKrCyTu1dvqLkRAAAAAAAAAAAAAAAAAOfC+BYAAAAAwCn8+BUzc8XsyZXZ19duTfeeQzU3AgAAAAAAAAAAAAAAAOBsGd8CAAAAADiFoijymeULK7P+gTJf+O6GmhsBAAAAAAAAAAAAAAAAcLaMbwEAAAAAnIYPXzMnC6ZPqMy+/Fh3du4/XHMjAAAAAAAAAAAAAAAAAM6G8S0AAAAAgNPQ0tyUT39wYWV2tG8gX3xoY72FAAAAAAAAAAAAAAAAADgrxrcAAAAAAE7TL10/L7OmjK/M/ub7m7LvUG/NjQAAAAAAAAAAAAAAAAA4U8a3AAAAAABO0/iW5nzy5ssqswNH+vKlRzbW2gcAAAAAAAAAAAAAAACAM2d8CwAAAADgDPzqe+enY0JrZfbFh1/OoaN9NTcCAAAAAAAAAAAAAAAA4EwY3wIAAAAAOAMTx7fkt266tDLbe6g3f/tod82NAAAAAAAAAAAAAAAAADgTxrcAAAAAAM7Qb95wSSaOa67MvvDghhzp66+5EQAAAAAAAAAAAAAAAACny/gWAAAAAMAZap/Qml+/YUFltv21w/nak1trbgQAAAAAAAAAAAAAAADA6TK+BQAAAABwFn77A5dmXEv1V6x3P7A+ff0DNTcCAAAAAAAAAAAAAAAA4HQY3wIAAAAAOAszJ7flY8u6KrNNuw/lvme319wIAAAAAAAAAAAAAAAAgNNhfAsAAAAA4Cz97i2XpbmpqMzuun9dyrKsuREAAAAAAAAAAAAAAAAAp2J8CwAAAADgLHVNm5Dbrptbmb2wfX++88LOmhsBAAAAAAAAAAAAAAAAcCrGtwAAAAAAzsEdyxemKKqzP7t/XcqyrLcQAAAAAAAAAAAAAAAAACdlfAsAAAAA4Bwsmjk5P33V7MpszeaefH/DnpobAQAAAAAAAAAAAAAAAHAyxrcAAAAAAM7RHSsWnjC7a/W6GpsAAAAAAAAAAAAAAAAAcCrGtwAAAAAAztHizqm5+V0zKrPv/ujVPNXdU3MjAAAAAAAAAAAAAAAAAE7E+BYAAAAAQAPcuWLRCbO7Vq+rsQkAAAAAAAAAAAAAAAAAJ2N8CwAAAACgAd536bQsXdBRmX3ruR350Y79NTcCAAAAAAAAAAAAAAAAoIrxLQAAAACABiiKIneuWHjC/O7V62tsAwAAAAAAAAAAAAAAAMCJGN8CAAAAAGiQFZfPzBWzJ1dm9z61Ld17DtXcCAAAAAAAAAAAAAAAAIChjG8BAAAAADRIURS5c8Wiyqx/oMznHlxfcyMAAAAAAAAAAAAAAACA/8/evUbJnd51Yv8+VdXVV6k13dKMR5r7GIORDDGsAXOxxycJBjYJGDaLlyy7XMLF9kk2h+w5u2+ywKtlX+VsljVsYIP3BJYl3CGbEA7BBoMxGzB40djG9sxYmpHmptaoW32trqp/XlR1d3WrNRppuqtb0udzTp36P7/f83/+v9K05kVL/RU7Cd8CAAAAANhD3/KW+/PI7MSuvf/jz57LSwurQ54IAAAAAAAAAAAAAAAAgEHCtwAAAAAA9lC9VvK+Jx7ftddqd/Ov/+iZIU8EAAAAAAAAAAAAAAAAwCDhWwAAAAAAe+w9b30g90+P7dr7+Y+fy5Xl1pAnAgAAAAAAAAAAAAAAAGCD8C0AAAAAgD3WbNTyA9/w2K69pVYn/+Zj54Y8EQAAAAAAAAAAAAAAAAAbhG8BAAAAAOyD937Vg5mZbO7a+7mPPZOltfaQJwIAAAAAAAAAAAAAAAAgEb4FAAAAALAvJpqNfN/XPbJr78ryen7xP5wf7kAAAAAAAAAAAAAAAAAAJBG+BQAAAACwb7777Y9karSxa+9nPvp01tqdIU8EAAAAAAAAAAAAAAAAgPAtAAAAAIB9Mj0+ku9++8O79l5cWMuvfeLCkCcCAAAAAAAAAAAAAAAAQPgWAAAAAMA++r6vezSjjd2/FfvTf/BU2p3ukCcCAAAAAAAAAAAAAAAAuLsJ3wIAAAAA2EcnjozmvW97cNfeubnl/Pu/en7IEwEAAAAAAAAAAAAAAADc3YRvAQAAAADssx94x2Np1MquvQ9++Kl0u9WQJwIAAAAAAAAAAAAAAAC4ewnfAgAAAADYZw/cM5Fve+upXXt//eLV/P5nXhryRAAAAAAAAAAAAAAAAAB3L+FbAAAAAABD8MPvfDyl7N77yQ9/PlVVDXcgAAAAAAAAAAAAAAAAgLuU8C0AAAAAgCF4471T+eYzb9i195fPXsmfPD035IkAAAAAAAAAAAAAAAAA7k7CtwAAAAAAhuT9T7zxur0PfvipIU4CAAAAAAAAAAAAAAAAcPcSvgUAAAAAMCRnTk3nnW86sWvvjz5/KX/57JUhTwQAAAAAAAAAAAAAAABw9xG+BQAAAAAwRB941xuv2/vghz8/xEkAAAAAAAAAAAAAAAAA7k7CtwAAAAAAhuirHp3J33j4nl17v/upF/PZF68OeSIAAAAAAAAAAAAAAACAu4vwLQAAAACAIfvAu9543d5PfeSpIU4CAAAAAAAAAAAAAAAAcPcRvgUAAAAAMGRPfPGJvPn+o7v2fuuTF3N+bnnIEwEAAAAAAAAAAAAAAADcPYRvAQAAAAAMWSklH3jX47v2Ot0q/+oPnxryRAAAAAAAAAAAAAAAAAB3D+FbAAAAAAAH4JvP3J9Hj0/u2vvlP3suLy2sDnkiAAAAAAAAAAAAAAAAgLuD8C0AAAAAgANQr5W8752P79prdbr52T96ZsgTAQAAAAAAAAAAAAAAANwdhG8BAAAAAByQb3vrqdw/PbZr7+c/fi5XlltDnggAAAAAAAAAAAAAAADgzid8CwAAAADggDQbtfzgOx7btbfc6uRDH/vCcAcCAAAAAAAAAAAAAAAAuAsI3wIAAAAAOEDvfdtDmZls7tr7uT/+QhbX2kOeCAAAAAAAAAAAAAAAAODOJnwLAAAAAOAAjTfr+f6vf3TX3vzKen7xT88PeSIAAAAAAAAAAAAAAACAO9ttEb5VSnm0lPKeUsoHSin/qJTy90op7yyljBz0bAAAAAAAr9ff/ZqHc2S0sWvvZz76dFbXO0OeCAAAAAAAAAAAAAAAAODOdajDt0opf6uU8rEkTyf5tSQ/meQnkvybJB9J8kIp5YOllONDnKlRSvnyUsp/W0r56VLKn5dSWqWUauD1oddx/hM7zrrZ1xf27tMCAAAAAMMwPT6S7377w7v2Xrq6ll/9xHNDnggAAAAAAAAAAAAAAADgznUow7dKKVOllF9M8stJ3v4qW2eSvC/J2VLKu/d5pvf1g8AWkvxlkp9J8kNJviLJyH4+GwAAAAC4833f1z+a0cbu37L96T94Ku1Od8gTAQAAAAAAAAAAAAAAANyZDl34VimlnuSXkrx3R+vlJL+bXiDXJ5JUA737kvxmKeXr93G0d6cXBDa+j88AAAAAAO5Sx6dG83e+6qFde89eXsn/+R+fH/JEAAAAAAAAAAAAAAAAAHemQxe+leQnknzLwHo9yX+X5IGqqt5dVdXfrqrqK5OcSfInA/tGk/xGKeX+4Y2aJLmS5MI+nv/Pkzx6E6/9DCADAAAAAPbRD7zjsTRqZdfeBz/y+XS71a49AAAAAAAAAAAAAAAAAF67QxW+VUp5LMk/2FH+r6uq+smqqlqDxaqqPpXkP832AK7ZJD+6jyMuJvlokv85yXcleVOSmSQ/u4/PvFJV1Rdu4vXcPs4CAAAAAOyjU8fG8563ntq199kXF/N7n35xyBMBAAAAAAAAAAAAAAAA3HkOVfhWesFZIwPrD1VV9ZvX21xV1UqS70kyGMz1/f0Qr732Q0mmq6p6R1VVP1JV1S9WVfW5qqqqfXgWAAAAAHCX+uEnHk8pu/f+5Ueeim9JAgAAAAAAAAAAAAAAALw+hyZ8q5QynuRv7Sj/sxvdV1XVZ5P8xkCpkeS79nC0jee8WFVVd6/PBQAAAAAY9PiJqXzLmft37X3y2Sv52FNzQ54IAAAAAAAAAAAAAAAA4M5yaMK3krw7ycTA+k+qqvrMa7z353asv31vRgIAAAAAGL73PfH4dXv/8sOfH+IkAAAAAAAAAAAAAAAAAHeewxS+9U071h+5iXs/mqQ9sH5rKeW+1z0RAAAAAMABOHNqOk988Yldex97ai6fOP/KkCcCAAAAAAAAAAAAAAAAuHMcpvCtMzvWf/Jab6yqainJX+0on37dEwEAAAAAHJAPvOuN1+198MNPDXESAAAAAAAAAAAAAAAAgDvLYQrfevOO9edv8v6dP232pa9jlsPkXaWUXyulPF1KWSylrJRSLpRS/ryU8pOllO8opYwc9JAAAAAAwN562yMz+apHZnbt/d6nX8xnXlgY8kQAAAAAAAAAAAAAAAAAd4ZDEb5VSplJsvOnyM7f5DE793/RrU90qLwjyXuSPJpkMslYkpNJviLJB5L8SpKnSykfKKWUA5sSAAAAANhz73/X49ft/dRHdv57BAAAAAAAAAAAAAAAAAC8FocifCvJsR3r5aqqlm7yjJd2rKdfxzy3mweS/GSS3y6l7Py1BAAAAABuU+9804mcPnl0195vf/Jizs3d7LdRAQAAAAAAAAAAAAAAAGgc9AB9UzvWK7dwxs57jtziLIfFQpLfS/IHSZ5ML1xsJck9Sd6U5D9P8p1Jxgbu+ZtJfqOU8o1VVbX2apBSyr1JTtzkbY/v1fMBAAAA4G5VSskH3vXGvP8XPnFNr1slP/0HT+effvtbDmAyAAAAAAAAAAAAAAAAgNvXYQ3fWr2FM3aGb+0883bxQpLvTfLvqqq63q/D/5fkF0op/zjJ/5bkmwd670zyE0l+ZA9nen+SH93D8wAAAACA1+jdp9+Qx05M5umXl67p/eqfP5f/4T/7otx3dGyXOwEAAAAAAAAAAAAAAADYTe2gB7iOakj3HDpVVX2mqqoPvUrw1uDeF5L8zSS/vKP1gVLKo/syIAAAAAAwVPVayfve+fiuvVanm5/96NNDnggAAAAAAAAAAAAAAADg9nZYwrcWd6zHb+GMnffsPPOOVFVVleR7kjw/UG4m+f4DGQgAAAAA2HPf9tZTOXVs92+b/sKfns8rS60hTwQAAAAAAAAAAAAAAABw+xK+dQeoqmo5yf+yo/xNe/iIDyY5c5Ovb93D5wMAAADAXW2kXssPvuOxXXvLrU4+9LEvDHcgAAAAAAAAAAAAAAAAgNtY46AH6JvfsZ4opUxWVbV0E2fcu2N95XXOdLv5nST/dGD9lr06uKqql5K8dDP3lFL26vEAAAAAQJLvfNuD+Re//7lcWmxd0/vQx76QH3jHY5kaPSzf8gUAAAAAAAAAAAAAAAA4vGoHPUCSVFU1l+SVHeWHbvKYh3esP3frE92WvrBj3SylTB/EIAAAAADA3hsbqef7vv7RXXvzK+v5t396bsgTAQAAAAAAAAAAAAAAANyeDkX4Vt+nd6zfeJP3P3aD8+50K7vUxoc+BQAAAACwb/7u1zycI2ONXXs/89FnsrreGfJEAAAAAAAAAAAAAAAAALefwxS+dXbH+u2v9cZSymSSL7vBeXe647vU5oY+BQAAAACwb46OjeTvv/2RXXsvX13Lr/z5c8MdCAAAAAAAAAAAAAAAAOA21DjoAQb8TpIfHFg/cRP3fkO2f5a/qKrqxb0Y6jby1TvWL1dVtX4gkwAAAAAA++Z7v+6R/OwfPZ3V9e41vX/2O5/JL/cDuEq/VvoXW+uyS2/7prJLb3N9nfrG2dd79vV62Xn+a332Lp9r5/kTzUa+5A1HcubU0bz5/qOZaB6mb4kDAAAAAAAAAAAAAAAAB+Uw/aTR/5NkJcl4f/32UsqXVFX1mddw7/fsWP/6Xg52m/iuHeuPHMQQAAAAAMD+mp0azd/5qofyc3/8hWt6V1fb+eSzV4Y/1G2glOSx45M5c2o6Z05O5/Spozl9cjrT4yMHPRoAAAAAAAAAAAAAAAAwZIcmfKuqquVSyq8k+e6B8j9K8r2vdl8p5U1J3jNQaif5t2WkgWsAACAASURBVHs/4eFVSnkiybfvKP/mAYwCAAAAAAzBD3zDY/n5j5/Leqc66FFuG1WVPPXyUp56eSm/+ZcXN+sPzoznzMnpnDk1ndMne4FcJ46MHuCkAAAAAAAAAAAAAAAAwH47NOFbfT+W5L1JRvrr7yml/HpVVb+12+ZSyliSn0vSHCj/66qqnnq1h5RSdv5E2ruqqvrILU28h0op35jkxaqqPnkT93x1kl9NUgbKf53kl/Z4PAAAAADgkDh5bDzf/tYH8kt/9uxBj3Lbe/bySp69vJL/++wLm7X7jo7mzMnpnD41nTMnj+bMqencPz2WUsqrnAQAAAAAAAAAAAAAAADcLg5V+FZVVU+XUv55kn84UP6VUsqPJPlfq6pqbRRLKW9O8rNJvnZg71ySH9+P2UopjSQPXKd9bMd6qpTyyHX2XqqqavE6va9N8k9KKb+b5N8l+b+qqnrpOvM8mOS/T/IPshVWliTrSd5fVVX7Os8AAAAAAO4AP/zE4/m1v3gu652d/9YAr9eLC2t5ceGl/L+f2fr27MxkM6dPHs3pk9M5c+pozpyczkMzE6nVBHIBAAAAAAAAAAAAAADA7eZQhW/1/eMkp5N8c389kuRfJPmfSimfSHI1yWNJviLJ4E81tZK8p6qq5/dprgeSPPMa935H/7Wb703yoVe5tyR5d/+VUsqFJH+d5EqSlSTTSd7Uf+3USfJ9VVX9/mucEwAAAAC4TT16fDI/9l+dzj/5zSfT6Qrg2m+Xl1r56Ocu5aOfu7RZOzLayJcOBnKdms5jxyfTqNcOcFIAAAAAAAAAAAAAAADgRg5d+FZVVZ1Syt9O8rNJvnOgdW+Sb7rObS8l+ftVVX10v+c7AKf6rxt5Osnfq6rqj/d5HgAAAADgkPhvvvrhfM1js/mTp+ayuNZO1c/gqtK7qAYyuapqe63arG/ds7O3UXgte3c+O9Vg7/rP3m3Wbfe8yt6dz87AXN2qyrm55Xzq4kKurrWzH66utfOnz1zOnz5zebM2NlLLm+8/mtMnj+bMyemcOTWdL7pvKqON+r7MAAAAAAAAAAAAAAAAANy8Qxe+lSRVVS0meW8p5VeS/I9JvuY6Wy8n+aUkP1pV1cvDmm8f/VaSE0m+IcmXJrnRT2O1k/yHJP8qyS9VVbW2v+MBAAAAAIfN4yem8viJqYMe49Dqdqucv7ycsxfn8+TFhZy90Hu/vNTal+etrnfzF+ev5C/OX9msjdRL3nTfkZw5OZ3Tp47m9MnpvPn+I5loHspv0QMAAAAAAAAAAAAAAMAdr1RVddAz3FAp5dEkX5HkZJLJJC8kOZfkj6uq2p+fkDpgpZSx9AK4Hk5yf5IjSUaSLCZ5JckzSf6sqqrlAxvyVZRSTic5u7E+e/ZsTp8+fYATAQAAAAD0VFWV5+dXN4O4nrw4n7MXFvLCwurQZqiVXnDamVPTOX2yF8h1+tTRHB0bGdoMAAAAAAAAAAAAAAAAcJCefPLJnDlzZrB0pqqqJ4fx7NsifIvbj/AtAAAAAOB28/LVtTx5cXsg1/nLw/33Dx6enciZfhDXmZO9YK7ZqdGhzgAAAAAAAAAAAAAAAADDcJDhW41hPAQAAAAAAA67E0dG88QX35snvvjezdr88nqefH4+T15YyNl+MNdTLy9mv/5di3Nzyzk3t5x//1fPb9bunx7L6ZPTObMRyHXqaN5wdCyllP0ZAgAAAAAAAAAAAAAAAO5wwrcAAAAAAOA6pidG8rWPH8/XPn58s7a01s5nXljI2QsLOXuhF8j12Revpt3dn0Su5+dX8/z8an7v0y9u1mYnmzl9ajpnTh7NmVPTOXNyOg/OjAvkAgAAAAAAAAAAAAAAgNdA+BYAAAAAANyEydFGvvLhmXzlwzObtbV2J599YTFnL87n7IX5nL24kM88v5C1dndfZphbauUPP/ty/vCzL2/Wjow1cvrk0Zw5Od0L5Dp1NI8en0q9JpALAAAAAAAAAAAAAAAABgnfAgAAAACA12m0Uc9bHpjOWx6Y3qy1O9089fJSP4xrPk9eWMiTF+ez1OrsywxXV9v5+NOX8/GnL2/WxkfqefP9R3phXCen86Unj+bR45OZHPXHAwAAAAAAAAAAAAAAANy9/HQNAAAAAADsg0a9li9+w5F88RuO5Du+8oEkSbdb5QtzS3ny4sJmINfZi/O5sry+LzOsrHfyifNX8onzV7bVj02M5OT0eE4eG88D94zn5LGxnDzWW586Np4TU6Op1cq+zAQAAAAAAAAAAAAAAAAHTfgWAAAAAAAMSa1W8tiJqTx2Yir/5ZefTJJUVZULV1by5MWFPHlhPmcvLuTshfm8dHVt3+a4sryeK8vr+dTzC7v2R+ol909vhXKdGgjm6oV0jWWi6Y8YAAAAAAAAAAAAAAAAuD35yRgAAAAAADhApZQ8cM9EHrhnIu8+/YbN+ksLq71ArovzOXthIWcvzue5V1aGMtN6p8r5y8s5f3n5unvumRi5bjDXqWPjOT41mlqtDGVeAAAAAAAAAAAAAAAAuBnCtwAAAAAA4BC69+hY7j06lnd9yb2btSvLrTx5cSFnL8z33i/O55lLS6mq4c/3yvJ6Xllez5MXF3btN+u13H9sLCenN8K5xnLqno2ArvGcnB7PeLM+5KkBAAAAAAAAAAAAAABA+BYAAAAAANw2jk0083VvPJ6ve+PxzdriWjuffn4gkOvCfD730mI63QNI5BrQ6nRzbm455+aWr7tnZrKZU8fGc/LYWD+gayuc69Sx8cxONlOrlSFODQAAAAAAAAAAAAAAwN1A+BYAAAAAANzGpkYbedsjM3nbIzObtdX1Tv76has5e3E+Zy8s5FMX5/PpF66m1e4e4KTXurzUyuWlVv7qwvyu/WajlpPTY5uBXCePjeeBzetefWykPuSpAQAAAAAAAAAAAAAAuN0J3wIAAAAAgDvM2Eg9X/7gsXz5g8c2a+udbp6/spoLV1Zy8crKru+r64crnKvV7uYLc8v5wtzydffMTjZz6p7xnJzeCuU6dWy8Vzs2ntnJZkopQ5waAAAAAAAAAAAAAACAw074FgAAAAAA3AVG6rU8NDuRh2Yndu1XVZVXlte3BXL1Xqt5rn/98tW1IU99Y3NLrcwttfIfn5vftd9s1HKqH8p1cnorlKtXG8/902MZG6kPeWoAAAAAAAAAAAAAAAAOkvAtAAAAAAAgpZTMTDYzM9nMmVPTu+5Za3fywvxqP5xrNRde6Qd0zW8Fdq2ud4c8+atrtbt55tJSnrm0dN09YyO1NOu1jI7U++9b69H+erRRS7NRy2hj555amvX6jvX2s7bOqPfPGDirv27USkopQ/yVAQAAAAAAAAAAAAAAuHsJ3wIAAAAAAF6T0UY9D89O5uHZyV37VVXl8lKrF8zVD+PaDOd6ZSUXrqzm0uLakKe+sdX1bi80bLV9YDOUkl4o1w1CwJqDQV63GAK2sU8IGAAAAAAAAAAAAAAAcLcSvgUAAAAAAOyJUkpmp0YzOzWatzwwveue1fVOXphfzcUrK7nQf/VCurZqa+3ukCc/eFV1uELAxkfqOTI2kiNjjf6rd310l9pWb+t6fKQuxAsAAAAAAAAAAAAAADi0hG8BAAAAAABDMzZSzyPHJ/PI8cld+1VVZW6p1Q/kWsmFfijXxYGgrkuLrSFPffcYDAF7ZXn9ls9p1EqmNkK6RrdCuo5eJ7hre79Xm2gK8AIAAAAAAAAAAAAAAPaH8C0AAAAAAODQKKXk+NRojk+N5sseOLbrntX1Tp6f74VyXXhlK5Tr4vxKLl5ZzYUrK2m1u0OenEHtbpUry+u5sryeZOWWzqjXSqZGt4d1Hd0lpEuAFwAAAAAAAAAAAAAAcLOEbwEAAAAAALeVsZF6Hj0+mUePT+7ar6oqlxZbvUCuKxvhXKu5cGU5F6/0QrvmllpDnpqb1elWmV9Zz/zK/gR4DQZ2Xb8/kkkBXgAAAAAAAAAAAAAAcMcRvgUAAAAAANxRSik5cWQ0J46M5ssfPLbrntX1Ti5eWckL86tZbXfSanezNvDqrbfqu693r23sHTyLg7EXAV61kkyNNjI9MZJTx8bz8MxkHj4+0XufnchDsxM5Ojayt4MDAAAAAAAAAAAAAAD7SvgWAAAAAABw1xkbqeexE1N57MTUvj+rqqq0OjvDua4f8HUrIWDXDwfbXufmdatkYbWdhdV2nr28ko8/ffmaPfdMjOSh2ck8PDPRC+SamcjDs71wrnuPjKaUcgCTAwAAAAAAAAAAAAAA1yN8CwAAAAAAYB+VUjLaqGe0Uc+RA5zjtYaArbU7WVrr5OpqO1dX17e9LwzW1jZ67XS61QF+soP3yvJ6Xlm+kk8+e+Wa3thILQ/NTOShmV4Y12A416lj42k2agcwMQAAAAAAAAAAAAAA3N2EbwEAAAAAANwF9isErKqqrKxvhXUt9AO5rq6uZ2FFgNfqejeffXExn31x8ZperSQnj433A7n64VwzE3lothfONTXqj/IAAAAAAAAAAAAAAGA/+Bv7AAAAAAAA3LJSSiaajUw0G7nv6NgtnfFqAV7b39tZ2KW2cd2+zQK8ulXy3Csree6Vlfxx5q7pz042e0FcMxN5aHYyD89M9IK6ZidyYmo0pZQDmBoAAAAAAAAAAAAAAG5/wrcAAAAAAAA4UHsV4LW63h0I79oZ2HX7BXjNLbUyt9TKX5y/ck1volnPQ/0wrodnJ7euZyZz8thYGvXaAUwMAAAAAAAAAAAAAAC3B+FbAAAAAAAA3PZKKRlv1jPerOfeo7d2xqsFeL24sJrzl5dzbm4p5y4v57nLK2l1unv7IW7CcquTz7xwNZ954eo1vUat5NQ949sCuR6a7V0/NDORiaY/IgQAAAAAAAAAAAAA4O7mb9YDAAAAAABAbi7Aq9Ot8sLCas7NLeX83HLOXV7uvy/l3Nxyrq62hzP0LtrdKufmlnNubjkf/dy1/RNHRvPwzEQvkGtmshfKNTuRh2cmMjPZTCll+EMDAAAAAAAAAAAAAMAQCd8CAAAAAACAm1SvlZw6Np5Tx8bztY9v71VVlSvL6zl3eXnXcK4XF9YOZui+l6+u5eWra/mzc69c05sabeShmYmBQK5+ONfMRE4eG0+9JpgLAAAAAAAAAAAAAIDbn/AtAAAAAAAA2EOllNwz2cw9k838Jw8eu6a/0urk2VeWc26uH851uXd9/vJynntlOeud6gCm7llca+dTzy/kU88vXNMbqZc8cM/EVjjXzETunx7P8almjh8ZzfGp0Rwda6QUAV0AAAAAAAAAAAAAABxuwrcAAAAAAABgiMab9bzpviN5031Hrul1ulUuXlnZDOQ6d3kp5+e2wrkW19oHMHHPeqfKM5eW8sylpevuadZrmZ1q5vjUaI5PNTM7Nbp5feLIaGYnR3P8SK9/z0Qz9ZqgLgAAAAAAAAAAAAAAhk/4FgAAAAAAABwS9VrJgzMTeXBmIl/3xu29qqpyeamVc5eXNwO5NsO5Li/n5atrBzP0gFanm+fnV/P8/OoN99ZKMjO5EdR1bVjX8SOjOd4P65qdHE2zURvCJwAAAAAAAAAAAAAA4G4gfAsAAAAAAABuA6WUzE6NZnZqNF/x0D3X9Jdb7Zy/3AvlOt8P5jrXD+m6cGUlnW51AFNfX7dKLi22cmmxleTqDfdPj49kdqoX1nXiBmFdE01/DAoAAAAAAAAAAAAAwPX5W+cAAAAAAABwB5hoNvIlbziaL3nD0Wt6651uLl5Z6YVxXV7O+bleMNdGWNfKeucAJr458yvrmV9Zz9MvL91w70SzvhnUdXwwoGvgerYf4nV0vJFSyhA+AQAAAAAAAAAAAAAAh4XwLQAAAAAAALjDjdRreXh2Mg/PTl7Tq6oqLy+u5fzc8vZwrsvLOT+3nLml1gFM/PostzpZvrySZy+v3HBvs17L7FTz1cO6jjQzOzmamclm6jVBXQAAAAAAAAAAAAAAtzvhWwAAAAAAAHAXK6Xk3iNjuffIWP7GIzPX9K+urud8P4jr3OVeQNf5y0u58MpKLi22srjWPoCp906r083z86t5fn71hntrJZmZ7IVy7Qzrmp1q5sTUaKYnRjLRrGd8pJ7xZj0TzUbGR+pCuwAAAAAAAAAAAAAADhHhWwAAAAAAAMB1HRkbyemT0zl9cnrX/up6Jy9fXcvcUiuXrq7l0uLGq7Xtem5xLa8srw95+r3VrdL/XK2bvne0UcvERhjXtnCuej+sq7F5PTYyUG82+u/1TIxs3F/r1ftnjDZqKUW4FwAAAAAAAAAAAADAayV8CwAAAAAAALhlYyP1PDgzkQdnJm64d73TzeWl1q5hXXOLrbw8ENp1eamVTrcawicYjrV2N2vt7r4EkNVKtoV6bYZ1vYZQr4nNe/qhXgP7x/thYfWaYC8AAAAAAAAAAAAA4M4ifAsAAAAAAAAYipF6LfcdHct9R8duuLfbrXJlZb0XznV1LZd2Ceu61A/renlxLa12dwif4HDqVsniWjuLa+19Ob/ZqF0T6jUx0tgK+NoW+rV7qFezXs/oSC3Nem3gvb593ailFEFfAAAAAAAAAAAAAMD+E74FAAAAAAAAHDq1WsnMZDMzk8286b4jr7q3qqosrrVzaSOQ6wZhXfsVUnWnarW7abW7mV9Z3/dnbYRwNRuD7/Ud6+vVt9ajr3HfzvpooxcEVqsJAQMAAAAAAAAAAACAO5nwLQAAAAAAAOC2VkrJkbGRHBkbyaPHJ2+4f3W9sxnEtRHQNbfUysu7hHW9srz/gVNsaXW6aXW6ydrBzjFSL9eEdDXrtYyO9N/3IPhrpF5Ls1HSrNcz0ii9dX2jXstIfasmDAwAAAAAAAAAAAAA9pbwLQAAAAAAAOCuMjZSzwP3TOSBeyZuuHe9083lpdauYV2Xrq7l5X59rl/rdKshfAL223qnynqnfeAhYBvqtdIP5ir9YK7BkK5amv2grm21xvbaxv271vr3jA6cPVLvP7MxGAq2y3P6e+u1klKEhO2mqqpUVdKtqnT778n2ddXdWPdqVVWlSlJVSa2W1EtJrZTUaiW10vuaqPVr9X7Nrz8AAAAAAAAAAADAayd8CwAAAAAAAOA6Ruq13Hd0LPcdHbvh3qqqsrrezXKrneVWJyvrnay0Ov3rXm25NVBrtbOyvr22vN6rX3N/q5NWpzuET8xh1OlWWel2srJ+0JNcXylJcyOMq7EV9NUcCO/atdbYCvoqJel2t0KpqlwbWlVV1bY9yVZvI7SqO7Cn6p9zoz3dfthVd1tQVm9PNfj8bF93u7vMuGNPNaRMvlIGQ7rSC+baEdhV+rXe9WCI1+6BXrV+rT545uY5288c7G+cs3nuxgylv782MMNAsFi9bD13c4bBmWoljVrJPRPNHJ9q5vjUaGanmpkabQgfAwAAAAAAAAAAAG6K8C0AAAAAAACAPVBKyXiznvFmPbP7cH670+2Hc20Fcu0W6rXcamd1vbO9PhDqtdzq7Oi3s7zeGVpAEHemqkrW2t2stbvJ2kFPc3eqqqRdVUmqpHPQ0wxXs1HL8clmZvthXBuhXMcne++zU6OZnWzmxJHR3DPRTLNRO+iRAQAAAAAAAAAAgAMmfAsAAAAAAADgNtCo13K0XsvRsZE9P7uqqqy1u70wrh1BXdcN9Vrv1Qb3rGyGg7UHQr86aXW6wr2AfdNqd3NxfjUX51df0/7p8ZFt4VwbYV2zU6PXhHgdHWuklLLPnwAAAAAAAAAAAAAYNuFbAAAAAAAAAHe5UkrGRuoZG6nvy/lVVaXd7QV8tdrdrLU7aW1eb7w6m+vt7zvr1+7b/f5e6Nfaenfzfa3dSVcIGNz15lfWM7+ynqdfXrrh3pF6yezk9nCu40dGMzsY0rXZb2a0sT//HwUAAAAAAAAAAAD2lvAtAAAAAAAAAPZVKSUj9ZKRei0ZPdhZ2p3uNaFcrU4nq/31ngV/3SBIrC0FDG4L650qLyys5oWF1de0/8hYI8enNsK5+oFdU6M5PtXcDPHauJ4eH0mtVvb5EwAAAAAAAAAAAAC7Eb4FAAAAAAAAwF2jUa+lUa9lonmwc3S7Vda73ax3qqy3u1nv9EK51jv9WmcrDKxX66bV7tcHa53ttVanm/X2LrX+ua324Lq3d+dzBvcBN+fqajtXV9t55tLSDfc2aiUzkxsBXc1+YFcvrGswpKt3PZqxkfoQPgEAAAAAAAAAAADcHYRvAQAAAAAAAMCQ1Wolo7V6RhtJRg96mt1VVZV2t9oM6VrrdLaFhW2FdlUDwV8D4V79YK/tQWAb5w2e0dtXktRKUislpZSUsn29cV0r6a9vbs/Gemv/zr2vsqeWlAzes/2+17Sn1ntPBufc/oyNezb2JEm3qtKtkk636l13k061cV316xv7qmv2VVWVzrbr3quqbnROtnr99+4192zt37yn//yd92w9u/e11akG5uhWO+bPdc9cW+/m0uJa1tq3fzhcu1vlpatreenq2mvaPzXayOy2kK6tcK6xkXr/99DW19T13je+Ljf212o71htfh9d8LW//PbLt673/9X/t76etZ29b5wb9ne/Zvg8AAAAAAAAAAABeL+FbAAAAAAAAAMA1SikZqZeM1GtJM0lGDnokSNIL71pudTK32MqlpbVcurqWuaVW5hbXcmmxtXk9t9jKpcW1XF5upaoOeurXb3GtncW1ds7NLR/0KAeqlGwLCiv9YLBt6yQj9VpGG7WMjtR7741aRhv1jI4MXDdq/fXAno39N7hvbNt9vX6zXkutJhwMAAAAAAAAAADgdiB8CwAAAAAAAACA20YpJZOjjUyONvLQ7MQN93e6VV5ZbmVusR/QtRnUtRHQ1crc0tpmf6nVGcKn4FZVVVIl6VZVeleHS3Mz9GsrnKv5ekLAhH8BAAAAAAAAAADsC+FbAAAAAAAAAADcseq1kuNTozk+NZrkyA33r7Q6vWCufkjX3GIrl5Y2groG3pdaubzUSqd7+AKgODitTjetTjdX1w7m+a8W/jXWqOXIWCNHxkb674PXvfejO2qTzXpKEegFAAAAAAAAAADceYRvAQAAAAAAAABA33izngdnJvLgzMQN93a7VeZX1nNpcS2XFluZ64d0zS2u5VI/vOvS4laI19W19hA+AXezvQ7/qpVkanQwnOv6wV3b+1u1yWYjtZoALwAAAAAAAAAA4HARvgUAAAAAAAAAALegViu5Z7KZeyab+aL7brx/db2Ty0utzC22cmlpLZeurmVuaSuc69LA9dzSWtY71f5/CHgV3SpZWG1nYfXWg+NKP8Br9+Cu7SFeR68T7DUlwAsAAAAAAAAAANhjwrcAAAAAAAAAAGAIxkbqOXlsPCePjd9wb1VVWVhp59JSP4xrcW0znOvSRkDXYiuXl1vpdKt0q/6r27t/c10lVdU7b2tdpaqytc5WvduvV3K/2CNVlVxdbefq6w3war56cNeRsUY/vGtk932jArwAAAAAAAAAAIAtwrcAAAAAAAAAAOCQKaVkemIk0xMjefzEwcywM4xr23v6793t6409W+Fe1667VZJsnd3t9gLABgPBtu57lfXGDJtnbJxXpd2tstbuZq3dydp6d+u63e2v+9ftbtbWe9er6wO1Xe5rtbsH8x+CXoDXWjtX19rJ/OotnzM12rgmuGu0UUuzUc9IvaRZr2Vk49XorZv1WkYavVqzXjb7zY1aY6s20t/f65Vt+zbX9ZoQMAAAAAAAAAAAOASEbwEAAAAAAAAAANcopaReknoEBSVJt1ul1bl+ONf1Qr32ar/wr9dvca2dxbV2np8/2DkatbIZyLUV4jUY4DUQ6NXYCPTaGfJVtt8zEPi1sd4KEyub5wzubQ48Y6ReMlqvZ6RRMj5STyl+3wMAAAAAAAAAcGcTvgUAAAAAAAAAAHADtVrJWK2esZF6kpGhP//1hn8tr3VydXU9V1fbWVhtb15fXeu/r7bT6VZD/1x3o3a3Srvbycr6QU+yuwdnxvNffNnJvP+Jx3NkbPhf6wAAAAAAAAAAMAzCtwAAAAAAAAAAAA65/Q7/qqoqK+udfhDXehZW21nsh3JtBnX161evE951dXU96x0BXre7Zy+v5Kc+8lR+/RMX8uPfejrvPv2Ggx4JAAAAAAAAAAD2nPAtAAAAAAAAAACAu1wpJRPNRiaajdx3dOyWzqiqKmvtbhZWtwdyDb4v7FLbed3qdPf403ErXlhYzQ/973+eb/zS+/Lj33o690+PH/RIAAAAAAAAAACwZ4RvAQAAAAAAAAAA8LqVUjI2Us/YSD33Hrn1c1bXO9cN59o12Gtte21htZ1WW4DXXvndT72Yjz01l3/4jW/Kd7/9kdRr5aBHAgAAAAAAAACA1034FgAAAAAAAAAAAIfGRoDXiSOjt3zGWrtzbUhXP5hr43pxtZ31TjetTtV7b3ez3ulu1QbWa5vX1daedm/d6nTT6VZ7+Ctw+CyutfNjv/2p/PpfXsxPfPtb8ub7jx70SAAAAAAAAAAA8LoI3wIAAAAAAAAAAOCOMtqoZ3SqnuNTtx7gdTM63a1QrvWBMK/WRq1dbV0PBH3tDPlqdaodIWC9ewfva+3ynJ17t+1pd/P/s3f3UZKd9X3gv/dWdc9oJI2kHvSCJCRmMCBL4kUIMyIYlPCS2IAxfotjJ7GdxN5w1nuwl2yyjjfx2ifZtffsRsYLMXG8azv2xsaJYRELa2zIWcs2WCMwCKExGNCMjJAESNNoxMxIPV117/5Rt7pvVVd3z0g93dNTn885o/vc5/k9LzXV6r5nTve3F5q6p+vTDzyWN77jT/Ojr9ybn3zN83LebGcD/vYAAAAAAAAAAGDzCd8CAAAAAAAAAACAp6FTFumUneycOXvDqOq6bkLCJgeBLfSqvPeTD+bXP3o4Vb36Ov2qzq/ccSj/72cezr9+8wty6/Mu3bwXAQAAAAAAAAAAG0T4FgAAAAAAAAAAAJzjiqJIgDf0FgAAIABJREFUt1Ok20nOy+SQsBuuvChvfvFV+an33pODDz2+5noPzD+RH/61u/KdL74y//KN1+cZF+w4E8cGAAAAAAAAAIAzotzqAwAAAAAAAAAAAABnhxdcfVFu//FX5F+84Ztz3szkkK622+9+KK/5N3fkdz/+pdR1vQknBAAAAAAAAACAp0/4FgAAAAAAAAAAALCk2ynzo6/clw+/7VV59XWXrVt/9InF/Pfv+Uz+zr+/M/c9cmwTTggAAAAAAAAAAE+P8C0AAAAAAAAAAABghasv2ZX/84dfmnf+4E259MId69YfODyfb3/7n+TtH/l8Fnr9TTghAAAAAAAAAAA8NcK3AAAAAAAAAAAAgImKosgbX3hlPvK2W/OD+69Zt/5kv8rbP/KFvP6X/iQHDh3ZhBMCAAAAAAAAAMDpE74FAAAAAAAAAAAArOmi82byP3/XC/J7b3l5nnvZBevW3/fI8Xz/v78zP/Wee3L0xOImnBAAAAAAAAAAAE6d8C0AAAAAAAAAAADglLz02XP54Ftfmf/ubz4vs931vwXx3R9/IK+57Y9y+90Ppq7rTTghAAAAAAAAAACsT/gWAAAAAAAAAAAAcMpmu2X+m1c/Nx/6iVfm5fv2rFv/6LGT+Yl3350f/vWP54H5E5twQgAAAAAAAAAAWJvwLQAAAAAAAAAAAOC07bv0gvz2j+3P//Z9L8olu2bWrf/jzz+S1/3iHfmVO+7LYr/ahBMCAAAAAAAAAMBkwrcAAAAAAAAAAACAp6QoinzvzVfnI2+7Nd9901Xr1j+5WOXnf/9zedM7P5pPP/DYJpwQAAAAAAAAAABWEr4FAAAAAAAAAAAAPC17LtiR277/xfm//tH+XLtn17r1n3348bz5lz+an33/wRxb6G3CCQEAAAAAAAAAYJnwLQAAAAAAAAAAAGBDfOtzn5E/+MlX5cf/xnPSLYs1a+s6+Y2P3Z/X3XZH/vDgVzbphAAAAAAAAAAAIHwLAAAAAAAAAAAA2EA7Zzr5p3/runzwra/MS665eN36h48+mf/qt/48//i3PpGvHH1yE04IAAAAAAAAAMC0E74FAAAAAAAAAAAAbLjnX3Fhfu8tfy3/6s035sId3XXr/+DgV/Pa2+7Ib/7Z/elX9Zk/IAAAAAAAAAAAU0v4FgAAAAAAAAAAAHBGlGWRv3/LtfnIP7k1r3/BFevWH1vo5WduP5jvedfH8tmHH9+EEwIAAAAAAAAAMI2EbwEAAAAAAAAAAABn1OW7d+aX/+7N+T9+6KW58qKd69bf/cBj+Y53/Gl+4fc/lydO9jfhhAAAAAAAAAAATBPhWwAAAAAAAAAAAMCmeO31l+fDb7s1//AVe1MWa9f2qjr/7o778rfe/sf5488/sjkHBAAAAAAAAABgKgjfAgAAAAAAAAAAADbN+Tu6+ZnvuD7v+/FX5IYrd69b/6X5E/mhX7srP/nuT+XRYwubcEIAAAAAAAAAAM51wrcAAAAAAAAAAACATffCqy/O7T/+ivwPr//mnDfTWbf+fXc/lNfedkf+08cfSF3Xm3BCAAAAAAAAAADOVcK3AAAAAAAAAAAAgC3R7ZT5sVftyx/+t6/K33j+pevWP3ZiMf/sPffkB371ztz3yLFNOCEAAAAAAAAAAOci4VsAAAAAAAAAAADAlnrW3K782o98S975gzflGRfsWLf+zkPz+fa3/0l+6SNfyEKvvwknBAAAAAAAAADgXCJ8CwAAAAAAAAAAANhyRVHkjS+8Mv/lbbfmB152zbr1J/tVfvEjn8/rf+lPctfh+U04IQAAAAAAAAAA5wrhWwAAAAAAAAAAAMBZ46JdM/n5735B/vNbXp5vuuyCdevve+R4/vav/Fn++XvvydETi5twQgAAAAAAAAAAtjvhWwAAAAAAAAAAAMBZ51uePZcPvvVb87bXPS+z3fW/3fF37nogr7ntjrz/0w+lrutNOCEAAAAAAAAAANuV8C0AAAAAAAAAAADgrLSj28lbX/PcfOgnXplb9s2tW//osYW89Xc+lR/59Y/ngfkTm3BCAAAAAAAAAAC2I+FbAAAAAAAAAAAAwFlt36UX5Hd+7Jb8r9/7wly8a2bd+js+/0he94t35FfuuC+9frUJJwQAAAAAAAAAYDsRvgUAAAAAAAAAAACc9YqiyPe99Fn5L2+7Nd9901Xr1j+5WOXnf/9zedM7P5pPP/DYJpwQAAAAAAAAAIDtQvgWAAAAAAAAAAAAsG3suWBHbvv+F+e3/tHLcu2eXevW/8XDj+e7fvmj+dn3H8yxhd4mnBAAAAAAAAAAgLOd8C0AAAAAAAAAAABg23nlcy/NH/zkq/Jf//XnpFsWa9ZWdfIbH7s/r7vtjnz4L766SScEAAAAAAAAAOBsJXwLAAAAAAAAAAAA2JZ2znTyz77tunzgrd+am665eN36h48+mR/7zU/kLb/15/nK0Sc34YQAAAAAAAAAAJyNhG8BAAAAAAAAAAAA29p1V+zOe97y1/KvvvOGXLiju279hw5+Ja+97Y785p/dn35Vn/kDAgAAAAAAAABwVhG+BQAAAAAAAAAAAGx7ZVnk77/82fnIP7k1337jFevWH1vo5WduP5jvedfH8tmHH9+EEwIAAAAAAAAAcLYQvgUAAAAAAAAAAACcMy7fvTPv+ns351d/6KV55kU7162/+4HH8h3v+NP8Lx/6XJ5c7G/CCQEAAAAAAAAA2GrdrT4AAAAAAAAAAAAAwEZ73fWX5+XP2ZN/84d/mf/wsftT1avX9qo67/qj+/LBex7O//RdN+aVz7108w7KOaGu61R10q/qwZ+6Tr8/uPaqKlWV9Kpqebyp6fXrVHWdXrt/7E+vvWZVpV9l7DpW0x/WTlhjzZrWmvXy2sP51aR1xvaoxq4X7OzmhVddlP375rJ/757ccOXudDt+dzAAAAAAAAAAW6+o6zW+mwSeoqIobkhy7/D+3nvvzQ033LCFJwIAAAAAAAAAAGBa3fPlx/JT7/lM/uLhx0+p/rtuuir/4g3fnD0X7DjDJ2OSuq6z2B+EQS326pzsV6u3+1UW+3UWqyqLvWpp3slJ7X6VxX6Vk6u0F/t1c1273WvmLbb2P9mvtvqvbVu4YEc3N197SW7Ztyf7983lBVddlBlhXAAAAAAAAABT6+DBg7nxxhvbXTfWdX1wM/YWvsUZIXwLAAAAAAAAAACAs0mvX+XXPno4v/jhL+SJxf669RfvmslPv/6b8303X52iKDbhhKevruv0qjr95k9v6VoNrv1V+of3/VX6qzr9qpowv+mv6iz2mkCqJgirVzVhVqu0F3tNQNaE9nhY1mLf97ZOi12zndx87SXZv3cut+zbkxdefXFmu8K4AAAAAAAAAKaF8C3OOcK3AAAAAAAAAAAAOBs9MH8i//L2e/NHf/nIKdXfsm8ub3jBMyeEUI2GUfX7q/QP7/ur9E8Iu1o1MKs/2l/5FlDOMTtnyrzkmkuyf++e7N83lxc/6+LsnOls9bEAAAAAAAAAOEOEb3HOEb4FAAAAAAAAAADA2aqu63zgnofzc//PwTx67ORWHwdYxWy3zE3Pujj79+3JLXvn8pJrLxHGBQAAAAAAAHAO2crwre5mbAIAAAAAAAAAAABwtiiKIt/xoivzqudeml/40GfzO3c9sNVHAiY42aty4PB8Dhyez/+eZKZT5MXPujj79+7J/n1zufnaS7Jr1rdDAwAAAAAAAHD6irqut/oMnIOKorghyb3D+3vvvTc33HDDFp4IAAAAAAAAAAAAJrvr8Hx++v/+TL74tWNbfRRIknTLIp3Wn5H7okin01yX+st0yyLlsLYYnd9e5/RrynTKjFxPZZ0k+dzD38iBw0fy53/19Sz0qjPy9/SCqy/KLfv2ZP/eubz02XO5YIcwLgAAAAAAAIDt4uDBg7nxxhvbXTfWdX1wM/YWvsUZIXwLAAAAAAAAAACA7WSh18+/+6ND+bf/3xdzsr/xIUGcOUWRzHbKzHTKzHSKdDtlcz9oz3TKzC61i6Zu/Xa3mTepPdMpMttqz3TKFYFZo8FZ5SqhWWPhWsUg1Opcs9Dr554vH82BQ0dy4PB8PnH/1/PEYn/D9+mURW68cnf279uTW/YNwrh275zZ8H0AAAAAAAAA2BjCtzjnCN8CAAAAAAAAAABgO7rvkWP56fd+JgcOz2/1UTbFePjTMChq6b6zHAq1fL8ylKpblpnpNkFUZZmZ7sYFW63WHq7ZOQfDqs51i/0qn3nwaO48dCQHDs3nE/fP5/jJjQ/jKovk+it3Z//ePdm/dy4v2zuXi3fNbvg+AAAAAAAAADw1wrc45wjfAgAAAAAAAAAAYLuq6zq33/1QfvuuL+WRbyyMhFOtDKsqR+87Y+FV46FWnVMLu1rRXxYpJ+27Yr1J55nQ31yLQnAVW6/Xr3LwoccHYVyH5/Pxw/P5xkJvw/cpiuS6K3Zn/9653LJvLi/buydz5wvjAgAAAAAAANgqwrc45wjfAgAAAAAAAAAAAOCp6Fd1PvvwIIzrzkPz+fj98zn6xOIZ2ev5l1+Y/fvmsn/vnrxs71wuvXDHGdkHAAAAAAAAgJWEb3HOEb4FAAAAAAAAAAAAwEaoqjqf+8o3cuDwkRw4NJ8Dh4/k6yfOTBjXcy49P7fs25P9+/bklr1zuWz3zjOyDwAAAAAAAABbG77V3YxNAAAAAAAAAAAAAACeirIscv2Vu3P9lbvzD16xN1VV5wtfOzYSxvXosZMbstd9jxzPfY8cz3888KUkyd5nnJ/9e+eyf99c9u/dkysvPm9D9gEAAAAAAABgawnfAgAAAAAAAAAAAAC2jbIs8vwrLszzr7gwP/TyZ6eu69z3yLHceWg+Bw7P58ChI/naNxY2ZK/Djx7P4UeP590ffyBJcs3criaMa0/2753Ls+Z2bcg+AAAAAAAAAGwu4VsAAAAAAAAAAAAAwLZVFEW+6bIL802XXZi/d8u1qes69x85kTsPHcmBQ0dy4PB8Hj765Ibs9aX5E/nS/In85z//cpLkqovPy/59c7ll757s3zeXa+Z2pSiKDdkLAAAAAAAAgDNH+BYAAAAAAAAAAAAAcM4oiiJ7n3F+9j7j/PzAy65JXdd5YP6J3Hn4SBPINZ8HH3tiQ/Z68LEn8t5PPpj3fvLBJMkVu3fmln1z2b9vT/bvncveZ5wvjAsAAAAAAADgLCR8CwAAAAAAAAAAAAA4ZxVFkWv27Mo1e3blb7/0WUmSL3/9RA4cms+Bw0dy56H5fGn+xIbs9ZXHn8z77n4o77v7oSTJZRfuyMv2zuWWfXtyy765POfSC4RxnUPquk6/qtMfXqs6VZWR+35dp6pW1vWrOtXY/aA26VVV6jqpM7iv6jpVnSSD6/C+ruvUa9xXdZ26OWdVNTXD+6U5Td1q96096zpL/VVrr7XOUDdnaJ8pS+PLe46cqX3Gavl1DP7TUkxsphjpLyb3n0LNyFatgdX3Or36rHa2Vddf+7UUE/5CiqWaYux+5X7jNZmwd7Fi3TVqJiy02vxJr238XOPja52rUxaZ6ZTpdgbXmU6RbllmpltmpizSbfpmOmW6ZdH0r6yf7Q6u3U6R2U65Yl6nLHxOBwAAAACAbUz4FgAAAAAAAAAAAAAwVa6+ZFeuvnlXvufmq5MkDx99YimM68Ch+Rx69PiG7PO1byzkA/c8nA/c83CSZO782Vy8ayZFkrIoUhTD6yBCpiwHYTJlMQicWRpv1Q/7lu+H45nQVyytOewvW2sO74uxuWWxxjnK1eYWY69rrdcw3GOw3mgY1SBsqddvBVc1195YaNXKuqRfVc16WSXcap0QrCbwaWWoVr0iVKseD4MCpkpRZGJo10y3GOnvdgahX8NAsNnm2u2Ug3Y5bA+uSzXlpPqnECTW7F8Ug8Cwshh87i2LIp3W5+nhmECxzTMMcRx+fetVdXr9qtWu06tO7X6xP1yjGe8P161aYyvve/1qZP9Omcx2OpntlpntltnRHXycDu9nhu1OM9Zdvm+322PDj08fWwAAAADA2Ub4FgAAAAAAAAAAAAAw1Z550Xl5801X5c03XZUk+drjT+bOw/M5cOhIDhyezxe/dmxD9pk/fjLzx09uyFoAbK26Tk72q5zsJ0l/q4+zYZbCuMaCuYoiTUDX8M9qY8shkWUT6NVpgpfKIuksjbXnL48NQsLSmr/WWGuvUxhrB5AVKZZCFRdHAq+qFWFYS/enFH41CLhaDr8aXX/8floURZYCusYDvUbDuzpL4V0znaI1vnog2Km2x8PCup1yq/9aTlvdhH9WdZ2quS7fD/rq1lg1Vj95fqu+Or01+9XoeoP70dr2utXIWFJV9eR2E5jaPmN/7DUPA1TbZxmetd2u6kFA69JrrdKEqC6vN/o6Rs/bb60z8XW0zrmjW+b8Hd2cN9MZXGc7OX+2k12zy+3zZrvNtZPzZ7vZNdvJrh3NtandNdvJjm4psA4AAABgEwjfAgAAAAAAAAAAAABouWz3zrzpRVfmTS+6MknyyDcWctfh+Rw4fCQHDs3nL7/6jS0+IQCcGXU9CKbpZ3qCoTjz6jpZ6FVZ6FU5W56iyiKZ7ZaZ6awdCNYtyzWCqFYGM9UTQqrWCso6nTCt2v+WU6MsshTE1Q7l2rWjm10znezaMdY/0h5cz9/RyXkz3cG1Cfs6b6aTshTqBWeDYVhgrxXMudgEe/aaUM9+E/I3DAwdBggOAwgHffVYX0bH6+X+lbWj81b2tc7QHl/qa58xE/paZ2jOXq0Yz4S+sfGmL0m6ZZFup0i3LNNZahfplIPgzk5ZZGaNscH8cu11yiKdzvg6y3MG65StOUVmOmVzHazRba3ZXWWdYf9Mp9iywMXhc0Y7YLbfCpEd9veX7lcGzk7sb4XXTuxfsWY1Ye8Jc/qrrzXs71eDr6PdzuC97HYG78/wvZ/pLL9/M8PrauNNe6az/HEz0ymX3r9h/8zw42pkzfY6K9ffyvcdAIBRwrcAAAAAAAAAAAAAANZw6YU78oYXPjNveOEzkyTzx0/mrsPzufPQkRw4PJ/PfeVxYQgAANtIVSdPLlZ5cvHsCQSDoapOji30cmyht+Fr75wpB0Fcw0CusaCudoDXSM140NdY7Uyn3PCzbpVhMF49bGc0AG8Yktcer6ukzvK8ammNQUc1bDeKFCmKpBjcjNwXRZFhHEvRjKXIivGiNW9YO772SO02DXmpqjqLTchPr6rT6w9Cd4btxSbcZ3HYv3QdzOs3gVYr6oa1rbrFpm8kCKtV16smrDVStxykNSlUq98ffS0wVBZZCmpqh3l1x0K7xkO8yrJINRZAVbVCs1YEVPVX9rN1hu/vMLBrtSCvjQgS65ZFymLw9aF9LZuvD8vt0fthbdG6H84p0tyXy3OW+tpzymLpa9LyGsN1l++X1m3tM2n/9pzhfkXZ2jerr8FTMwzqWwrjawUl9sYCHcdDGHv90UDIXn84PgyUrEaCI4efx9r7jIdGtvdYql8KoKxGghvb40ufL1shksM9J+8xeA6p/MP/RJ3m80qnLNJpPhd0miDLTjE6XjY1nbJdX6RTZrndHm+vvdTOUl+3tWZ77sp1MnbG0TOM7rNcWxZN4GYxtk/rtXbLcunzFrD9Cd8CAAAAAAAAAAAAADgNc+fP5ttuvCLfduMVSZKjJxZz1/3zOXDoSO48fCR/8dDj8fOLAADA2WYQOncyOb6x6850iqUgrmEo10ynaEKnkjShDUsBVU2AVTOUOs14E2aVkYCrZrxKU1+PBVw1NWOBWFW1vNZ4INbwDMN2e69z3bohXxkN9hqpHQaGZTkgZTzkKyPzxgLCshxQMAxdKVKsGZ41De8JVHVysl/lZH+rT8JmGgYBPplqq48yNdrhX+2vaeNWy9IpVp2x1pzV6tdY6zQH1or+WW2f1bbvj4RVJb2q8u/csIayWBlE9qrnX5p/+4Mv2eqjAadB+BYAAAAAAAAAAAAAwNNw0a6ZvO76y/O66y9Pkjz+5GI+cf98Dhyaz52H53Pvg0fT91NKnIL2D+sM/mTpBwLb94MfVG/dp7kvl++HdaPXtdeYPGf0WhbLP1xfNnsWzXmLpLX+cM7k+7L5Kb+l11W2Xkdrrwz3bP0oYROjMGi3/tca+b+sNVBP7j6ldVarX6WZun7qa64WLFCv81omrVGP3Q97RvYemz8+d+IeE2vqiXuuVZNV9j7dc42/rjpZDm/o1znZr9KrWu3+INDhZL8e6/fDpADwdCz26xx9YjFHn1jc6qOwjmFg2cqHTw9DAJzbqqXwT1/zgI1R1UnVH/288qQ0Tdh2hG8BAAAAAAAAAAAAAGyg3Ttn8urrLs+rrxuEcR1b6OWTf/X1PPjYE+lXgziZuq5TNz/wVdXL93XqpR8Eq5v+wXjTl2HfcP5gTl0nVbP2irkZ3WNkbr08Z8U5qtE9R8674hzDPZfnTDxHNfwht+XxQYBTkU6ZdIa/Ib792+LL5d8aXzbXdv+gLqdYN2m94dwynSa86tT3He5dNnVZmjNe1y2X1ynLVl8xrBsETwFbYym0qxoGdA3CuRb7VRabsK7FXp3Fqt3fajfzhvVLY828XtXuH9Qv9gb7jay11N+EhI3sMXam4V5VtWp4G+euski6ZTn4etIZfE3plGVmOstfYzplkZlOOXLf7ZQTx8qySFXVOdkbfPwv9KpBu7lf0W6uAAAAAAyUpX/jh+1G+BYAAAAAAAAAAAAAwBl0wY5uXvW8S7f6GACsYRCY19nqYzxl/fEQr7GAr2EQYr+qmyDF5ft6rN1v7qtqZd2ksWG73wpyHOwzOjYMihxfY3ysHjnnyrrhWL3K61lrbPhaq7peDqMqy3RHAqvGA6xG79vhVcP71cKuJtWuNncQoDX5fnn/0bCsrVbXdRb79Wg4V6/KyX5/YnjXYjvUa8WcwXVhwthi/xQDwZo2Z1ZRDAJTy6IYtMuV7WGoaKfVLsvRecMA0qKpabfLZu6wrmwCUsvWeFmOt1v3I3VjfeO1rXXaZy7LYtXXuvT6mrlJstCrcnyhnydO9nLiZD/HTw7ag2s/x0/2Rq8L/Tyx2N/aNxMAAIAN1fELNmDbEb4FAAAAAAAAAAAAAAAA29gwPGznzPYNEGP7KYois90is90y2bHVpxmo67oV9lWvGwg2bC+Mh301/b2qHg2QGgtmaoc4FSPhT2P1E4KkTrl+ZLzZOxmbPymo6jTWHK5Rjr+mlfVsnKqq82SvCeJqgrlOnOznxPh1oT/WP6Fm2F7o58RiP/2q3uqXB2ySTtkEBw5DDpugz6X2MCywPb4UIDgcK9JphQwuhQ1O6l8xf+W8Tjk23tq3fb5hsOHIvKWajPQlSa8ahLz2qjq95uv08LrcX6dfDYJohwG1K+cM6wfBtb1Wu1/VzdzlsVNZZ5o/7Q6DYocfF+PhtO3Q204TMjuxv/n4Gp3XhNd2Vukf3jfjZVGkagJiB+9f1bQH79lSWPHS+7vaeNPXrm0Cjlf0T/ObD8AKnbMgLB04PcK3AAAAAAAAAAAAAAAAANj2iqLIjm4nO7qC6Dj7lWWRXbPd7Jrd2B/zrOs6C71qKdDriSaca9g+frKfJ072BqFfi/0cXxgN8npiRe1yMNjJXrWhZ4UzpSySbqfMTBPOM9Mp0+0MQn66TUjPzDAgaGLdcu2wbqazHBI002nPG9YN2sO6bhMyNGn9pfbIPmPhRKsEVg1DFof9nB2qqhXUVVXpN+FM/apuhXgNgp7GQ7za4V6LY8FgS+tVy2FS7eCvYfDUUvBVWaTTKVcPvloRZDUalLVizorQq5VhWdOurpeD3yYFdS32l9/PxbGQr+Xwr9EgsPXCv0bXXH/9flWnTp2qSqq6Tl0314zeD9vt+6oevMblvrTqRu+X1shgDVhN2QpfbH+dG4boDUMZx78eLgU2NuGQ3bIchEsO68cDHcf26JYr1xoNkyybsYzUMaquk349+NxSVXX69fK1N+xrPt/02+PV8n2/qpfH27XV2JzWtV8l/eZrYlVnqbZqrTf8uro8d/msm8XHDGw/wrcAAAAAAAAAAAAAAAAAAM4BRVFk50wnO2c6ueT82Q1du9evcmKxnxMLy2Fd7eCuYfv4Qj9Vk7pRFIOgoKJpFykG11ZfWTR9zYSyXZfBeJp2MRxfa62mnQxrR8dXrNXaqyhG28O90tRMfi3L7XJkfnvvwXUQSNIOJmkCTLIcVFJnOfxk2E5G79vrZJW12vvUGQxM3Ke9Vj2+/ynsM7ZO+6xL7TX2WRGO1YRPTQ69aoKqmlCgmSY8q10nEIitUJZFZpuPu/MiBHSaFMXg889Mx3vfNh7YNfycPxLgVQ36q3r18K+RNcauI0FiY4FhGa5btQLEMrr/5IOv1r16cM+qS53eFs2cyaOrzVk75GyVtdY413Jo1XIQVVkmnSYI61SCsdpBWOPhkd2yXHo2g81WtYK/VoSCtYO6+pMCwupVAsKyYp3Ld+/c6pcKnCbhWwAAAAAAAAAAAAAAAAAArKnbKbO7U2b3zpmtPgoAcBYbhlmWEbIEnB3KskiZQVgiQFu51QcAAAAAAAAAAAAAAAAAAAAAAACAzSJ8CwAAAAAAAAAAAAAAAAAAAAAAgKkhfAsAAAAAAAAAAAAAAAAAAAAAAICpIXwLAAAAAAAAAAAAAAAAAAAAAACAqSF8CwAAAAAAAAAAAAAAAAAAAAAAgKkhfAsAAAAAAAAAAAAAAAAAAAAAAICpIXwLAAAAAAAAAAAAAAAAAAAAAACAqSF8CwAAAAAAAAAAAAAAAAAAAAAAgKkhfAsAAAAAAAAAAAAAAAAAAAAAAICpIXwLAAAAAAAAAAAAAAAAAAAAAACAqSF8CwAAAAAAAAAAAAAAAAAAAAAAgKkhfAsAAAAAAAAAAAAAAAAAAAAAAICpIXwLAAAAAAAAAAAAAAAAAAAAAACAqSF8CwAAAAAAAAAAAAAAAAAAAAAAgKkhfAsAAACOANJ6AAAe9ElEQVQAAAAAAAAAAAAAAAAAAICpIXwLAAAAAAAAAAAAAAAAAAAAAACAqSF8CwAAAAAAAAAAAAAAAAAAAAAAgKkhfAsAAAAAAAAAAAAAAAAAAAAAAICpIXwLAAAAAAAAAAAAAAAAAAAAAACAqSF8CwAAAAAAAAAAAAAAAAAAAAAAgKkhfAsAAAAAAAAAAAAAAAAAAAAAAICpIXwLAAAAAAAAAAAAAAAAAAAAAACAqSF8CwAAAAAAAAAAAAAAAAAAAAAAgKkhfAsAAAAAAAAAAAAAAAAAAAAAAICpIXwLAAAAAAAAAAAAAAAAAAAAAACAqSF8CwAAAAAAAAAAAAAAAAAAAAAAgKkhfAsAAAAAAAAAAAAAAAAAAAAAAICpIXwLAAAAAAAAAAAAAAAAAAAAAACAqSF8CwAAAAAAAAAAAAAAAAAAAAAAgKkhfAsAAAAAAAAAAAAAAAAAAAAAAICpIXwLAAAAAAAAAAAAAAAAAAAAAACAqSF8CwAAAAAAAAAAAAAAAAAAAAAAgKkhfAsAAAAAAAAAAAAAAAAAAAAAAICpIXwLAAAAAAAAAAAAAAAAAAAAAACAqSF8CwAAAAAAAAAAAAAAAAAAAAAAgKkhfAsAAAAAAAAAAAAAAAAAAAAAAICpIXwLAAAAAAAAAAAAAAAAAAAAAACAqSF8CwAAAAAAAAAAAAAAAAAAAAAAgKkhfAsAAAAAAAAAAAAAAAAAAAAAAICpIXwLAAAAAAAAAAAAAAAAAAAAAACAqSF8CwAAAAAAAAAAAAAAAAAAAAAAgKkhfAsAAAAAAAAAAAAAAAAAAAAAAICpIXwLAAAAAAAAAAAAAAAAAAAAAACAqSF8CwAAAAAAAAAAAAAAAAAAAAAAgKkhfAsAAAAAAAAAAAAAAAAAAAAAAICp0d3qA3DOmm3ffPGLX9yqcwAAAAAAAAAAAAAAAAAAAAAAAGeZCblEs5PqzoSiruvN2ospUhTFm5LcvtXnAAAAAAAAAAAAAAAAAAAAAAAAtoXvrOv6/ZuxUbkZmwAAAAAAAAAAAAAAAAAAAAAAAMDZQPgWAAAAAAAAAAAAAAAAAAAAAAAAU6Oo63qrz8A5qCiKi5Lc2up6IMnJLToO28dzktzeuv/OJPdt0VkAAOCp8EwLAMB255kWAIDtzjMtAADbnWdaAAC2O8+0AABsd55pATbXbJJnte7vqOv66GZs3N2MTZg+zQfw+7f6HGwvRVGMd91X1/XBrTgLAAA8FZ5pAQDY7jzTAgCw3XmmBQBgu/NMCwDAdueZFgCA7c4zLcCW+NRWbFpuxaYAAAAAAAAAAAAAAAAAAAAAAACwFYRvAQAAAAAAAAAAAAAAAAAAAAAAMDWEbwEAAAAAAAAAAAAAAAAAAAAAADA1hG8BAAAAAAAAAAAAAAAAAAAAAAAwNYRvAQAAAAAAAAAAAAAAAAAAAAAAMDWEbwEAAAAAAAAAAAAAAAAAAAAAADA1hG8BAAAAAAAAAAAAAAAAAAAAAAAwNYRvAQAAAAAAAAAAAAAAAAAAAAAAMDWEbwEAAAAAAAAAAAAAAAAAAAAAADA1hG8BAAAAAAAAAAAAAAAAAAAAAAAwNYRvAQAAAAAAAAAAAAAAAAAAAAAAMDW6W30AgJZHkvzc2D0AAGwnnmkBANjuPNMCALDdeaYFAGC780wLAMB255kWAIDtzjMtwJQo6rre6jMAAAAAAAAAAAAAAAAAAAAAAADApii3+gAAAAAAAAAAAAAAAAAAAAAAAACwWYRvAQAAAAAAAAAAAAAAAAAAAAAAMDWEbwEAAAAAAAAAAAAAAAAAAAAAADA1hG8BAAAAAAAAAAAAAAAAAAAAAAAwNYRvAQAAAAAAAAAAAAAAAAAAAAAAMDWEbwEAAAAAAAAAAAAAAAAAAAAAADA1hG8BAAAAAAAAAAAAAAAAAAAAAAAwNYRvAQAAAAAAAAAAAAAAAAAAAAAAMDWEbwEAAAAAAAAAAAAAAAAAAAAAADA1hG8BAAAAAAAAAAAAAAAAAAAAAAAwNYRvAQAAAAAAAAAAAAAAAAAAAAAAMDWEbwEAAAAAAAAAAAAAAAAAAAAAADA1ult9AIAkKYpib5IXJ7kyyQVJHk7yV0k+Vtf14laeDQAAzoSiKGaSvCLJNUmemeRYkoeSfKqu6/u38GgAAGxDRVF0knxTkusz+HfWi5IsJPl6kvuSfKKu6+MbvKdnWgAANlRRFOcluS7JtRk8116YZCbJ40mOJLk3ycG6rnsbtJ9nWgAAtjXPtAAAbHeeaQEA2AxFUVyX5EVJrk5yXpInk3wtyReTfPrpfI+tZ1qA7a2o63qrzwBMsaIovjfJ25K8fJWS+SS/m+Rn6rp+dNMOBgDA1CmKYl+Sb0ny0ub6kgx+sGvor+q6fvYG7HNpkp9L8v1J5lYp+1iS2+q6fs/T3Q8AgHNXURTXJPnuJK9N8soku9co7yf5cJJ31nX9wae5r2daAAA2TFEU/yDJq5PsT/KcJOU6U44l+U9J3lHX9d1PcU/PtAAAbLqiKN6dwTNo21P6XgTPtAAAbKSiKH42yf/4NJb4D3Vd/8hp7umZFgCAM6ooiouT/ESSf5hBMNZq+knuTvJ7dV3/wmms75kW4BwgfAvYEkVRXJDkV5P8nVOc8tUkP1zX9R+cuVMBADBtiqL460n+eQaBW6v9I+fQ0w7fKori25P8RpLLTnHKf0zyj5/Ob08AAODcVBTFbyf5gac4/QNJfrSu668+hX090wIAsKGKovhykquewtR+knck+ad1XfdOYz/PtAAAbLqiKN6U5PYJQ6f9vQieaQEA2GibHb7lmRYAgDOtKIrvS/KuJHtOY9pX67q+4hTX90wLcI7obvUBgOlTFEUnye8mef3Y0CNJPpXkaAa/zfamJEUzdnmS24uieG1d13+6WWcFAOCc9+Ikf3MzNmqCvt6XZLbVXSf5ZJJDSS7O4Bn4Ga3xv5tkd1EUb67rutqMcwIAsG08b5X+B5N8IYNfaNBNsi/Ji5KUrZo3Jv9/e3caZOlZngf4fkYjhDYj9tUgFLGaJRDKZjEgITkgY7NF2IayywpEUWygTEwcCDFBdmHjBXBIIDgYEHIc7AIMmGAWgYzYBAGMCJso1hnCvkgsGu3Skx+nFb751Mvp6XO6e/pcV1XXnPfpd/t51zffeTrvraqHdvc3pz1QpgUAYJNcmuSLSb6S5IeZZNmbJLlnkuFLrockeXqSY6vq1O6+Zq2NZVoAALZCVR2TyZe8ZrHXCZFpAQA4iMm0AADMW1U9N8mZy/zqK0k+l0lPgxsmuXUm7yIcuc79T4hMC7BjaL4FbIU/yv6Nt65K8ttJXt7dV15XrKq7J3lFkgcslQ5L8qaqumd3f2OzLgsAwEK6IslXM2kKu2FVdbskb8j+D1U/kOT07r5wMO+wJGckeUGSQ5fKv5jkeUmePYu7AACwI12Q5FVJ3tbdXxz/sqpum+Q/JfnXg/Kdk7yuqh7S3b3WATItAABztC/Jm5O8Lcn5ST610oumVXX/TLLlSYPyYzJ55+BPVztEpgUAYAu9MMltlj7/KMnRB7KJTAsAwCZ6QpIPrWP+JdNMkmkBAJi3qnpGrt9466+TPL+7P7nM/F2Z9DL4F0kePsX+Mi3ADlNTfJ8CYGaq6rgkn82PQ2KSPKa7/26F+YcnOTc/bsCVJP+9u//N/G4JAMCiqKqnJ/mTJJ9O8tEkH1n695NJHpTk3YPpe7v72AM855VJnjQonZ/kpO6+fIX5j0nyxkHpiiR36e69B3I+AAA7T1V9JMm3kpzZ3R+dcs1vJnnpqPyE7v6bKdbKtAAAzEVVHdrdV61j/q4kZyf51UH5B0lu2d1XrLJOpgUAYNNV1clJ3rk0vDrJ7yT5s8GUqd9FkGkBAJiXqjozyXMHpRO7+7w5nCPTAgAwN1V170y+F7Z7qXRVkid29+unXL+7u69eY45MC7DD7NrqCwAL57nZv/HWq1dqvJUk3X1ZktOSXDkoP3mpiRcAAGzU2Ul+orvv092nd/fLu/tj6/mi11qq6k5Jfn1QujLJaSs9VE2S7n7T0t2uc1j2f6kBAAAe392/MG3jrSTp7v+W5G9H5V9ba51MCwDAPK33eWx3X5vkKUn2Dco3SnLiSmtkWgAAtkJVHZnkLwalFyX5+AHuJdMCAHBQk2kBAJinqtqd5FX5ceOtJDlj2sZbSTJF4y2ZFmAH0nwL2DRVdXiSU0flP15rXXd/LsmbBqXdSZ44w6sBALCguvvi1R5wzsgTkxwyGL+huz8/xbpxVv6lqrrh7K4FAMDBrLv3HODSl47GKzYoGJBpAQDYVrr7h0nePyofv8oSmRYAgK3w/CTHLn3+UpIzN7CXTAsAwMFOpgUAYJ4en+S+g/G53X3WjM+QaQF2IM23gM308CRHDMYf7O7PTrl2HG4fN5srAQDA3D12NJ7qwW13X5jkfw9KRyb557O6FAAAC+uC0fjwqjpmjTUyLQAA29FFo/HRq8yVaQEA2FRV9cAkTxmUzujuyzawpUwLAMDBTqYFAGCezhiN/3AOZ8i0ADuQ5lvAZnrEaHzeOta+L8nVg/F9quqWG74RAADMUVXdKsm9B6Wrk3xgHVucNxqfstE7AQCw8K5epnaDlSbLtAAAbGN3GI2/vtwkmRYAgM1WVYcleVV+/K7+2d39rg3sJ9MCAHBQk2kBAJinqjo+yUMHpT1J3j3jM2RagB1K8y1gM91jNP7gtAu7e1+ST47KP7XhGwEAwHyNM/AnlrLttM4fjWVgAAA26vjR+Ook311lvkwLAMC2U1V3TvIzg1Inec8K02VaAAA225lJ7rL0+TtJnrHB/WRaAAAOdjItAADzdOJofG5394zPkGkBdijNt4DNdLfR+AvrXP/F0fjuG7gLAABshnFmlYEBANhqp47GH+3ua1eZL9MCALCtVNWtk7wuySGD8uu7e88KS2RaAAA2TVXdN8m/G5Se3t3f2+C2Mi0AAJvtjKp6V1V9raour6ofVdWeqnpPVf1BVT14nfvJtAAAzNNPj8YfTJKaOLmqzqqqz1TVD6pqX1XtXcq7z6qqY6c8Q6YF2KE03wI2RVXdJMlNRuWvrHOb8fw7HfiNAABgUxw/Gq83A+8djW9aVTfewH0AAFhgVXVUkiePym9cY5lMCwDAlqqq3VV186p6SFX9SZLPJrnXYMqXkjx1lS1kWgAANkVV7U7yqiS7l0pv7+7XzGBrmRYAgM32K0lOSnKbJIclOSrJHZI8JMmzk7y3qj5SVSdPuZ9MCwDAPN1vNL5wqanWu5K8M8lpSe6W5CeSHJHk9pnk3ecn+VxVvbSqjljjDJkWYIfSfAvYLMeMxpd297517vHt0fhGG7gPAABshnEOHmfaVXX3JUkuH5XlYAAADtTzk9xqMP5+klessUamBQBgU1XVf66qvu4nyVWZ5ND3JPmdTF6Gvc67kzyku1fLqTItAACb5VlJ7r30eV+S35jRvjItAADb0f2SnFNVf1BVtcZcmRYAgHm69Wh8RJKPJHnYFGsPTfKbSd5fVeN9hmRagB1q99pTAGbiqNH4sgPYY7zm6AO8CwAAbJZZ5eAbDsZyMAAA61ZVj03y1FH5P3b3RWsslWkBANiO3pzkpd19zhRzZVoAAOauqu6e5HcHped0954ZbS/TAgCwWb6W5K1JPpzkwiQXJbk2yU2T3DfJLyR5+GB+JXl2kl1J/sMq+8q0AADM07gx1llJbrb0eV+SP0/ytiRfTXJkJn9E4UlJfnaw5j5J/raqHtrdVy1zhkwLsENpvgVslnGgHHdmncY4hI73BACA7WZWOfjGq+wJAACrqqp7J/nLUfmcJC+bYrlMCwDAdnRKkkOq6vLufu8ac2VaAADmqqp2JXllksOWSv+Y5L/M8AiZFgCAeftwJk213tndvcKc85O8pKrul+Q1Se40+N2zqupD3f13K6yVaQEAmIuqOiw/fjZ7ndst/fuZJI/o7v87+v3HkpxVVc9I8oJB/QFJnpnkecscJdMC7FC7tvoCwMJa6UHsrNcAAMB2IgcDALCpqur2Sf4++/8H/d4kv7rKC7OrkWkBAJi3309yx8HP3ZM8OMnTkvzD0pxDkzwyyXuq6iVVdcg69pdpAQCYtd9Kcv+lz1cn+Vfdfc0cz5NpAQCYqe5+a3efM817BN390Uzy7+dGv/qjdTyrlWkBAJiVlTLoD7J8463/r7tfmOTPRuV/W1XTNMWSaQF2CM23gM1yyWh8+AHsMV4z3hMAALYbORgAgC1TVbdI8s4ktx2Uv5nk57r7O1NuI9MCALCpuvui7t4z+Lmwu9/f3S/p7pMyacS1d7DkKUlevsqWMi0AAHNTVccled6g9KLu/viMj5FpAQDYVrr7oiRPyP7NA+6a5MQVlsi0AADMRXdfmuTaZX71otUabw08J5NGXde5SZJTlpkn0wLsUJpvAZtFoAQAYBHJwQAAbImqukmSdyW586D83SQnd/fn17GVTAsAwLbS3e/P5Atc3xuUn1RVj15hiUwLAMBcVFUl+YskRyyVvpTkzDkcJdMCALDtdPfHkpwzKj9ihekyLQAA87RvmdpfTrOwu/clecOofMIyU2VagB1K8y1gs/xgND6iqo5c5x63GI2/v4H7AADAZhjn4JuvZ3FVHZXrP1iVgwEAWFVV3SiTF1zvOShfnOTnuvvT69xOpgUAYNvp7i8n+f1R+d+vMF2mBQBgXk5P8rDB+IzuvmwO58i0AABsV28fje+1wjyZFgCAeRpnw2919551rP/QaHy3ZebItAA71O6tvgCwGLr7e1V1cZIbD8q3T3LhOra5w2j8+Q1fDAAA5mucWceZdi3j+Rd198UbuA8AADtcVR2dycut/2xQ/mGSR3T3xw9gS5kWAIDt6m+SvHgwvn9VHdPd45dTZVoAAObl9waf35rkC1V17BprbjUa715mzde7+8rBWKYFAGC72jMar9SAQKYFAGCePpfkJwfjb6xz/ddH45suM0emBdihNN8CNtOFSR44GB+f9TXfOm6Z/QAAYDsbZ9bj17l+nIE/s4G7AACww1XVkZl8wev+g/IlSU7p7g8f4LYyLQAA21J3f3v0R8B2JbljkgtGU2VaAADm5fDB559P8uUD2OO2y6y7T5LhH1OQaQEA2K4uG40PX3aWTAsAwHx9OslJg/EV61w/nn/DZebItAA71K6tvgCwUD41Gj9g2oVLXxq71xr7AQDAdjPOrPeqqiPWsf5Ba+wHAABJkqo6PMlbkvzsoHxpkkd29/kb2FqmBQBgO7tqND5smTkyLQAABzuZFgCA7epmo/F3V5gn0wIAME+fGI2PWef68fzvLTNHpgXYoTTfAjbT20fjE9ax9sFJdg/GF3T3tzZ8IwAAmKPu/kb2f4C7O/s3Q1jLCaPx2zZ6JwAAdp6qumGSN2f//Hh5kkd193s3srdMCwDAdrWUg8df7LreewQyLQAABzuZFgCAbexnRuOvLzdJpgUAYM7elqQH4+OW3imY1j1G46+OJ8i0ADuX5lvAZnpHkssG4wdU1V2nXHvaaPzGmdwIAADmb5xd/+U0i5ay8vClhH1JzpnVpQAA2Bmq6gZJ3pDk5EH5iiSP6e5zZ3SMTAsAwHZ0UvZ/9+nSJF9bYa5MCwDAzHX3Md1d6/lJcuJom73LzPv4MsfJtAAAbCtLzQweNyqft8oSmRYAgLno7q8n+eCgdGgm7xRM6xGj8ftWmCfTAuxAmm8Bm6a7L03y+lH5mWutq6o7J3nsoHR1ktfM8GoAADBP/zPJNYPx46rqTlOsG2fl13b35bO7FgAAB7uq2p3ktUlOGZSvSnJqd79jhkfJtAAAbCtVtSvJc0blt3f3lSsskWkBADjYybQAAGw3z0xy28H4miR/v8p8mRYAgHk6azT+7WkWVdWDk/z0oHRtkreuMF2mBdiBNN8CNtuZmXz56zqnVdWjVpq89FcQzkpyg0H5ld39xflcDwAAZqu7P5/k7EHpBklevZR1l1VVj05y2qB0ZZLfm8sFAQA4KFXVIZn8J/6jB+Wrk/xyd79llmfJtAAAzEtVPa2qbr3ONYcmeWX2/6uwSfLSldbItAAAHOxkWgAA5qWqfq2qbrnONacnee6o/Oru3rvSGpkWAIA5OyvJhYPxw6pq1QZcVXWLXL9p12tX6mMg0wLsTJpvAZuqu7+U5MWj8uur6qlVNWywlaq6W5JzkzxwUP5eBEoAAGaoqm5XVceOf5LcajR193Lzln5utsYxz01y8WD8wCTvqqq7ju5yWFU9LcnrRutfuNoLCQAALKRXJfmlUe3ZSS5YJbeu9LPif/oPyLQAAMzDk5N8sar+qqp+saqOXmliVR1eVU9IckH2fzE1Sf5Hd//DGmfJtAAAHOxkWgAA5uHJSb5cVWdX1SOr6siVJlbV/arqDUlenqQGv/pakt+d4iyZFgCAuejua5L8VpJrB+UXVtWLq+rG4/lVdXKSDyT5J4PyxZm8i7samRZgh6nu3uo7AAumqg5J8r+SnDL61beTfCzJj5Icl+S+2f9B7JVJTu7u923GPQEAWAxVtSfJHTa4zdndfdoa55yQ5B2Z/FWD63SSf0zypSQ3yiQD33y09C1JHrP0EBgAAJIkVTXL/+A5sbvPm+LMEyLTAgAwQ1X18ST3HpQ6yReS7Eny/UzeEzg6k2e4d09y6DLbvCXJqd19xRTnnRCZFgCALbSUSd89KO3t7mPXuV6mBQBgZqrqvCQPHZSuTfL5TJ7T/iDJNUlumsmz3Fsus8VFSR7a3Z+a8rwTItMCADAnVfXUJP91VL4qyYcyaRp7eJJ/mut/l+zKJI/q7ndMccYJkWkBdgzNt4AtUVVHJXlFkl+ecsm3k/x6d799frcCAGARbVbzraWzfj7Jq3P9h6cr+eskp3f3vgO/GgAAO9FWNN9aOlemBQBgZpZpvrUelyV5XpI/7e6r1nGmTAsAwJbZaPOtpT1kWgAAZmaZ5lvrcW6S07r7q+s8U6YFAGBuquo3krwgyRFTLvlWksd19/nrOEOmBdghdm31BYDF1N2XdPevJHl8Jp1iV3JRkpcluYfGWwAAHOy6+61J7pHkz5NcvMrUDyU5tbuf6KEqAADbiUwLAMCMnZ5JA60PJrliyjWfTfKcJHfu7j9cT+OtRKYFAODgJ9MCADBjL07ymiR7p5y/L8kbk5zc3Sevt/FWItMCADBf3f2yJPdK8ldJfrTK1G8mOTPJXdbTeGvpDJkWYIeo7ln+YXSAA1NVd0xy3yS3SXJkJmF1b5IPdPeVW3k3AACYh6q6QZIHJblDkltl8jLC15Jc0N1f3sq7AQDANGRaAABmqaoOTXK3JMcluW2So5IcmuSSJD9MsieTrLnaS6vrPVOmBQDgoCbTAgAwS1V1TJKfSvKTSW6Z5Igku5J8P5OGAhcm+UR3XzPDM2VaAADmpqoOzyRv3i6TvHllku8k+T/d/YkZnSHTAhzENN8CAAAAAAAAAAAAAAAAAAAAAABgYeza6gsAAAAAAAAAAAAAAAAAAAAAAADAZtF8CwAAAAAAAAAAAAAAAAAAAAAAgIWh+RYAAAAAAAAAAAAAAAAAAAAAAAALQ/MtAAAAAAAAAAAAAAAAAAAAAAAAFobmWwAAAAAAAAAAAAAAAAAAAAAAACwMzbcAAAAAAAAAAAAAAAAAAAAAAABYGJpvAQAAAAAAAAAAAAAAAAAAAAAAsDA03wIAAAAAAAAAAAAAAAAAAAAAAGBhaL4FAAAAAAAAAAAAAAAAAAAAAADAwtB8CwAAAAAAAAAAAAAAAAAAAAAAgIWh+RYAAAAAAAAAAAAAAAAAAAAAAAALQ/MtAAAAAAAAAAAAAAAAAAAAAAAAFobmWwAAAAAAAAAAAAAAAAAAAAAAACwMzbcAAAAAAAAAAAAAAAAAAAAAAABYGJpvAQAAAAAAAAAAAAAAAAAAAAAAsDA03wIAAAAAAAAAAAAAAAAAAAAAAGBhaL4FAAAAAAAAAAAAAAAAAAAAAADAwtB8CwAAAAAAAAAAAAAAAAAAAAAAgIWh+RYAAAAAAAAAAAAAAAAAAAAAAAALQ/MtAAAAAAAAAAAAAAAAAAAAAAAAFobmWwAAAAAAAAAAAAAAAAAAAAAAACwMzbcAAAAAAAAAAAAAAAAAAAAAAABYGJpvAQAAAAAAAAAAAAAAAAAAAAAAsDA03wIAAAAAAAAAAAAAAAAAAAAAAGBhaL4FAAAAAAAAAAAAAAAAAAAAAADAwtB8CwAAAAAAAAAAAAAAAAAAAAAAgIWh+RYAAAAAAAAAAAAAAAAAAAAAAAALQ/MtAAAAAAAAAAAAAAAAAAAAAAAAFobmWwAAAAAAAAAAAAAAAAAAAAAAACwMzbcAAAAAAAAAAAAAAAAAAAAAAABYGJpvAQAAAAAAAAAAAAAAAAAAAAAAsDA03wIAAAAAAAAAAAAAAAAAAAAAAGBhaL4FAAAAAAAAAAAAAAAAAAAAAADAwtB8CwAAAAAAAAAAAAAAAAAAAAAAgIWh+RYAAAAAAAAAAAAAAAAAAAAAAAALQ/MtAAAAAAAAAAAAAAAAAAAAAAAAFobmWwAAAAAAAAAAAAAAAAAAAAAAACyM/wfxRXtBBqjJZAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 6000x1500 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20, 5), dpi=300)\n",
    "plt.plot(range(1, indices.size + 1), scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmin(scores) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimum is achieved at 50 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection of regressor\n",
    "\n",
    "We train our regressor with the top 50 features on the whole train set and evaluate its quality on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = TransformedTargetRegressor(make_pipeline(\n",
    "    xgb_regressor.regressor['column_transformer'],\n",
    "    ColumnTransformer([('selector', 'passthrough', indices[:np.argmin(scores) + 1])]),\n",
    "    xgb_regressor.regressor['xgb_regressor']\n",
    "), func=np.log, inverse_func=np.exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses_test = pd.read_pickle('data/king_county_test.pickle')\n",
    "\n",
    "preprocessor_clean = joblib.load('objects/preprocessor_clean.joblib')\n",
    "preprocessor_engineer = joblib.load('objects/preprocessor_engineer.joblib')\n",
    "preprocessor = make_pipeline(preprocessor_clean, preprocessor_engineer)\n",
    "\n",
    "houses_test = preprocessor.transform(houses_test)\n",
    "y_test = houses_test['price'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2:   0.970, 0.888\n",
      "RMSE:  63893, 123061\n",
      "MAE:   38271, 63512\n",
      "MedAE: 23947, 36568\n",
      "MAPE:  0.073, 0.117\n"
     ]
    }
   ],
   "source": [
    "regressor.fit(houses_train, y_train)\n",
    "print_evaluation(regressor, houses_train, houses_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the test set, the mean absolute percentage error is about 11.7% and the median absolute error is about $ 36,600."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "245px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
